{
    "docs": [
        {
            "location": "/", 
            "text": "POMDPs\n\n\nA Julia interface for defining, solving and simulating partially observable Markov decision processes and their fully observable counterparts.\n\n\n\n\nPackage Features\n\n\n\n\nGeneral interface that can handle problems with discrete and continuous state/action/observation spaces\n\n\nA number of popular state-of-the-art solvers availiable to use out of the box\n\n\nTools that make it easy to define problems and simulate solutions\n\n\nSimple integration of custom solvers into the existing interface\n\n\n\n\n\n\nAvailible Packages\n\n\nThe POMDPs.jl package contains the interface used for expressing and solving Markov decision processes (MDPs) and partially observable Markov decision processes (POMDPs) in the Julia programming language. The \nJuliaPOMDP\n community maintains these packages. The packages currently maintained by JuliaPOMDP are as follows: \n\n\n\n\nMDP solvers:\n\n\n\n\nValue Iteration\n\n\nMonte Carlo Tree Search\n\n\n\n\n\n\nPOMDP solvers:\n\n\n\n\nQMDP\n\n\nSARSOP\n\n\nPOMCP\n\n\nDESPOT\n\n\nMCVI\n\n\nPOMDPSolve\n\n\n\n\n\n\nSupport Tools:\n\n\n\n\nPOMDPToolbox\n\n\nPOMDPModels\n\n\n\n\n\n\nInterface Extensions:\n\n\n\n\nGenerativeModels\n\n\nPOMDPBounds\n\n\n\n\n\n\nManual Outline\n\n\n\n\nPOMDPs\n\n\nPackage Features\n\n\nAvailible Packages\n\n\nManual Outline\n\n\n\n\n\n\nDefining a POMDP\n\n\nFunctional Form POMDP\n\n\nTabular Form POMDP\n\n\nContinous POMDP\n\n\n\n\n\n\nFrequently Asked Questions (FAQ)\n\n\nHow do I save my policies?\n\n\n\n\n\n\nInstallation\n\n\nGetting Started\n\n\nAPI Documentation\n\n\nContents\n\n\nIndex\n\n\nTypes\n\n\nModel Functions\n\n\nDistribution/Space Functions\n\n\nBelief Functions\n\n\nPolicy and Solver Functions\n\n\nSimulator\n\n\nUtility Tools\n\n\nConstants\n\n\n\n\n\n\nConcepts and Architecture\n\n\nBelief\n\n\n\n\n\n\nDefining a Solver", 
            "title": "About"
        }, 
        {
            "location": "/#pomdps", 
            "text": "A Julia interface for defining, solving and simulating partially observable Markov decision processes and their fully observable counterparts.", 
            "title": "POMDPs"
        }, 
        {
            "location": "/#package-features", 
            "text": "General interface that can handle problems with discrete and continuous state/action/observation spaces  A number of popular state-of-the-art solvers availiable to use out of the box  Tools that make it easy to define problems and simulate solutions  Simple integration of custom solvers into the existing interface", 
            "title": "Package Features"
        }, 
        {
            "location": "/#availible-packages", 
            "text": "The POMDPs.jl package contains the interface used for expressing and solving Markov decision processes (MDPs) and partially observable Markov decision processes (POMDPs) in the Julia programming language. The  JuliaPOMDP  community maintains these packages. The packages currently maintained by JuliaPOMDP are as follows:", 
            "title": "Availible Packages"
        }, 
        {
            "location": "/#mdp-solvers", 
            "text": "Value Iteration  Monte Carlo Tree Search", 
            "title": "MDP solvers:"
        }, 
        {
            "location": "/#pomdp-solvers", 
            "text": "QMDP  SARSOP  POMCP  DESPOT  MCVI  POMDPSolve", 
            "title": "POMDP solvers:"
        }, 
        {
            "location": "/#support-tools", 
            "text": "POMDPToolbox  POMDPModels", 
            "title": "Support Tools:"
        }, 
        {
            "location": "/#interface-extensions", 
            "text": "GenerativeModels  POMDPBounds", 
            "title": "Interface Extensions:"
        }, 
        {
            "location": "/#manual-outline", 
            "text": "POMDPs  Package Features  Availible Packages  Manual Outline    Defining a POMDP  Functional Form POMDP  Tabular Form POMDP  Continous POMDP    Frequently Asked Questions (FAQ)  How do I save my policies?    Installation  Getting Started  API Documentation  Contents  Index  Types  Model Functions  Distribution/Space Functions  Belief Functions  Policy and Solver Functions  Simulator  Utility Tools  Constants    Concepts and Architecture  Belief    Defining a Solver", 
            "title": "Manual Outline"
        }, 
        {
            "location": "/install/", 
            "text": "Installation\n\n\nIf you have a running Julia distriubtion (Julia 0.4 or greaer), you have everything you need to install POMDPs.jl. To install the package, simply run the following from the Julia REPL:\n\n\nPkg\n.\nadd\n(\nPOMDPs\n)\n \n# intalls the POMDPs.jl package\n\n\n\n\n\n\nOnce you have POMDPs.jl installed, you can install any package that is part of the JuliaPOMDP community by running:\n\n\nusing\n \nPOMDPs\n\n\nPOMDPs\n.\nadd\n(\nSARSOP\n)\n \n# installs the SARSOP solver\n\n\n\n\n\n\nThe code above will download and install all the dependencies automatically. All JuliaPOMDP packages have been tested on Linux and OS X, and most have been tested on Windows. \n\n\nTo get a list of all the availible packages run:\n\n\nPOMDPs\n.\navailable\n()\n \n# prints a list of all the availible packages that can be installed with POMDPs.add\n\n\n\n\n\n\nDue to the modular nature of the framework, you can install only the solvers/support tools you plan on using. However, if you want to install all of the supported JuliaPOMDP packages you can run the following code:\n\n\nPOMDPs\n.\nadd_all\n()\n \n# installs all the JuliaPOMDP packages (may take a few minutes)", 
            "title": "Installation"
        }, 
        {
            "location": "/install/#installation", 
            "text": "If you have a running Julia distriubtion (Julia 0.4 or greaer), you have everything you need to install POMDPs.jl. To install the package, simply run the following from the Julia REPL:  Pkg . add ( POMDPs )   # intalls the POMDPs.jl package   Once you have POMDPs.jl installed, you can install any package that is part of the JuliaPOMDP community by running:  using   POMDPs  POMDPs . add ( SARSOP )   # installs the SARSOP solver   The code above will download and install all the dependencies automatically. All JuliaPOMDP packages have been tested on Linux and OS X, and most have been tested on Windows.   To get a list of all the availible packages run:  POMDPs . available ()   # prints a list of all the availible packages that can be installed with POMDPs.add   Due to the modular nature of the framework, you can install only the solvers/support tools you plan on using. However, if you want to install all of the supported JuliaPOMDP packages you can run the following code:  POMDPs . add_all ()   # installs all the JuliaPOMDP packages (may take a few minutes)", 
            "title": "Installation"
        }, 
        {
            "location": "/get_started/", 
            "text": "Getting Started\n\n\nBefore writing our own POMDP problems or solvers, let's try out some of the availiable solvers and problem models availible in JuliaPOMDP.\n\n\nHere is a short piece of code that solves the Tiger POMDP using SARSOP, and evaluates the results. Note that you must have the SARSOP, POMDPModels, and POMDPToolbox modules installed. \n\n\nusing\n \nSARSOP\n,\n \nPOMDPModels\n,\n \nPOMDPToolbox\n\n\n\n# initialize problem and solver\n\n\npomdp\n \n=\n \nTigerPOMDP\n()\n \n# from POMDPModels\n\n\nsolver\n \n=\n \nSARSOPSolver\n()\n \n# from SARSOP\n\n\n\n# compute a policy\n\n\npolicy\n \n=\n \nsolve\n(\nsolver\n,\n \npomdp\n)\n\n\n\n#evaluate the policy\n\n\nbelief_updater\n \n=\n \nupdater\n(\npolicy\n)\n \n# the default QMPD belief updater (discrete Bayesian filter)\n\n\ninit_dist\n \n=\n \ninitial_state_distribution\n(\npomdp\n)\n \n# from POMDPModels\n\n\nhist\n \n=\n \nHistoryRecorder\n(\nmax_steps\n=\n100\n)\n \n# from POMDPToolbox\n\n\nr\n \n=\n \nsimulate\n(\nhist\n,\n \npomdp\n,\n \npolicy\n,\n \nbelief_updater\n,\n \ninit_dist\n)\n \n# run 100 step simulation\n\n\n\n\n\n\nThe first part of the code loads the desired packages and initializes the problem and the solver. Next, we compute a POMDP policy. Lastly, we evaluate the results. \n\n\nThere are a few things to mention here. First, the TigerPOMDP type implements all the functions required by SARSOPSolver to compute a policy. Second, each policy has a default updater (essentially a filter used to update the belief of the POMDP). To learn more about Updaters check out the Concepts section. We can also play around with the history.\n\n\nhist\n.\nstate_hist\n\n\nhist\n.\nobservation_hist", 
            "title": "Getting Started"
        }, 
        {
            "location": "/get_started/#getting-started", 
            "text": "Before writing our own POMDP problems or solvers, let's try out some of the availiable solvers and problem models availible in JuliaPOMDP.  Here is a short piece of code that solves the Tiger POMDP using SARSOP, and evaluates the results. Note that you must have the SARSOP, POMDPModels, and POMDPToolbox modules installed.   using   SARSOP ,   POMDPModels ,   POMDPToolbox  # initialize problem and solver  pomdp   =   TigerPOMDP ()   # from POMDPModels  solver   =   SARSOPSolver ()   # from SARSOP  # compute a policy  policy   =   solve ( solver ,   pomdp )  #evaluate the policy  belief_updater   =   updater ( policy )   # the default QMPD belief updater (discrete Bayesian filter)  init_dist   =   initial_state_distribution ( pomdp )   # from POMDPModels  hist   =   HistoryRecorder ( max_steps = 100 )   # from POMDPToolbox  r   =   simulate ( hist ,   pomdp ,   policy ,   belief_updater ,   init_dist )   # run 100 step simulation   The first part of the code loads the desired packages and initializes the problem and the solver. Next, we compute a POMDP policy. Lastly, we evaluate the results.   There are a few things to mention here. First, the TigerPOMDP type implements all the functions required by SARSOPSolver to compute a policy. Second, each policy has a default updater (essentially a filter used to update the belief of the POMDP). To learn more about Updaters check out the Concepts section. We can also play around with the history.  hist . state_hist  hist . observation_hist", 
            "title": "Getting Started"
        }, 
        {
            "location": "/def_pomdp/", 
            "text": "Defining a POMDP\n\n\nThe expressive nature of POMDPs.jl gives problem writers the flexiblity to write their problem in many forms. In this section we will take a look at two ways to write a discrete problem, and a way of writing a continuous problem. \n\n\n\n\nFunctional Form POMDP\n\n\nThe first, and most straighforward way to define a POMDP problem is to implement the model functions that you may need. For example, all POMDPs will need \ntransition\n, \nreward\n, and \nobservation\n functions. In this example we'll start with the simple Tiger POMDP problem. We want to use the SARSOP solver to compute a policy. To use a solver from JuliaPOMDP, a problem writer must define a set of functions required by the solver. To see what functions are required by SARSOP, check out its documentation \nhere\n. \n\n\nLet's first define the Tiger POMDP type.\n\n\nusing\n \nPOMDPs\n \n# load the interface\n\n\ntype\n TigerPOMDP\n \n:\n \nPOMDP\n{\nBool\n,\n \nInt64\n,\n \nBool\n}\n \n# parametarized inheritance POMDP{state, action, observation}\n\n    \nr_listen\n::\nFloat64\n \n# reward for listening (negative)\n\n    \nr_findtiger\n::\nFloat64\n \n# reward for finding the tiger (negative)\n\n    \nr_escapetiger\n::\nFloat64\n \n# reward for escaping\n\n    \np_listen_correctly\n::\nFloat64\n \n# probbility that we hear the tiger correctly\n\n    \ndiscount_factor\n::\nFloat64\n \n# discount factor\n\n\nend\n\n\nTigerPOMDP\n()\n \n=\n \nTigerPOMDP\n(\n-\n1.0\n,\n \n-\n100.0\n,\n \n10.0\n,\n \n0.85\n,\n \n0.95\n)\n \n# default contructor\n\n\n\n\n\n\nNotice that the \nTigerPOMDP\n is inheriting from the abstract \nPOMDP\n type that comes from POMDPs.jl. The abstract \nPOMDP\n is parametarized by a \nBool\n, \nInt64\n, \nBool\n combination with the syntax \nTigerPOMDP \n: POMDP{Bool, Int64, Bool}\n. The parametarization defines how we choose to represent the state, actions, and observations in our problem. In the \nTigerPOMDP\n we use a boolean to represent our states and observations (because there are two of each) and an integer to represent our actions (because there are 3). If you wanted to create a custom concrete type to represent your states, actions, or observations you could do that as well. Let's say we made a type to represent our states called \nAwesomeTigerState\n. That type could contain integers, floats, arrays, or other complex data structures (depending on what's convenient). We would then parametrize the Tiger POMDP in the following way: \ntype TigerPOMDP \n: POMDP{AwesomeTigerState, Int64, Bool}\n.\n\n\nNow, let's consider another important component of POMDPs, probability distributions. In the POMDPs.jl interface, we think in terms of distribution types. We want to be able to sample from these distriubtions and compute their probability masses or densities. In the Tiger POMDP, our distriubtions are over binary variables (boolean state or observation), so we can implement a simple version of a Bernoulli distribution.\n\n\ntype\n TigerDistribution\n \n:\n \nAbstractDistribution\n \n# inherits from a POMDPs.jl abstract type\n\n    \np\n::\nFloat64\n \n# probability of 1\n\n    \nit\n::\nVector\n{\nBool\n}\n \n# pre-allocate the domain of the distriubtion\n\n\nend\n\n\nTigerDistribution\n()\n \n=\n \nTigerDistribution\n(\n0.5\n,\n \n[\ntrue\n,\n \nfalse\n])\n \n# default constructo\n\n\n\niterator\n(\nd\n::\nTigerDistribution\n)\n \n=\n \nd\n.\nit\n \n# convenience function used by discrete solvers (iterator over the discrete distriubtion)\n\n\n\n\n\n\nLet's implement the pdf and rand function that returns the probability mass and samples from the distribution.\n\n\n# returns the probability mass \n\n\nfunction\n pdf\n(\nd\n::\nTigerDistribution\n,\n \nso\n::\nBool\n)\n\n    \nso\n \n?\n \n(\nreturn\n \nd\n.\np\n)\n \n:\n \n(\nreturn\n \n1.0\n-\nd\n.\np\n)\n\n\nend\n\n\n\n# samples the dsitribution\n\n\nrand\n(\nrng\n::\nAbstractRNG\n,\n \nd\n::\nTigerDistribution\n,\n \ns\n::\nBool\n)\n \n=\n \nrand\n(\nrng\n)\n \n=\n \nd\n.\np\n\n\n\n\n\n\nWe also want some convenience functions for initializing the distriubtions.\n\n\ncreate_transition_distribution\n(::\nTigerPOMDP\n)\n \n=\n \nTigerDistribution\n()\n\n\ncreate_observation_distribution\n(::\nTigerPOMDP\n)\n \n=\n \nTigerDistribution\n()\n\n\n\n\n\n\nLet's define our transition, observation, and reward functions.\n\n\nfunction\n transition\n(\npomdp\n::\nTigerPOMDP\n,\n \ns\n::\nBool\n,\n \na\n::\nInt64\n,\n \nd\n::\nTigerDistribution\n=\ncreate_transition_distribution\n(\npomdp\n))\n\n    \n# Resets the problem after opening door; does nothing after listening        \n\n    \nif\n \na\n \n==\n \n1\n \n||\n \na\n \n==\n \n2\n\n        \nd\n.\np\n \n=\n \n0.5\n\n    \nelseif\n \ns\n\n        \nd\n.\np\n \n=\n \n1.0\n\n    \nelse\n\n        \nd\n.\np\n \n=\n \n0.0\n\n    \nend\n\n    \nd\n\n\nend\n\n\n\nfunction\n observation\n(\npomdp\n::\nTigerPOMDP\n,\n \ns\n::\nBool\n,\n \na\n::\nInt64\n,\n \nd\n::\nTigerDistribution\n=\ncreate_observation_distribution\n(\npomdp\n))\n\n    \n# correct observation wiht prob pc        \n\n    \npc\n \n=\n \npomdp\n.\np_listen_correctly\n\n    \nif\n \na\n \n==\n \n0\n\n        \ns\n \n?\n \n(\nd\n.\np\n \n=\n \npc\n)\n \n:\n \n(\nd\n.\np\n \n=\n \n1.0\n-\npc\n)\n\n    \nelse\n\n        \nd\n.\np\n \n=\n \n0.5\n\n    \nend\n\n    \nd\n\n\nend\n\n\n# convenience function\n\n\nfunction\n observation\n(\npomdp\n::\nTigerPOMDP\n,\n \ns\n::\nBool\n,\n \na\n::\nInt64\n,\n \nsp\n::\nBool\n,\n \nd\n::\nTigerDistribution\n=\ncreate_observation_distribution\n(\npomdp\n))\n\n    \nreturn\n \nobservation\n(\npomdp\n,\n \ns\n,\n \na\n,\n \nd\n)\n\n\nend\n\n\n\nfunction\n reward\n(\npomdp\n::\nTigerPOMDP\n,\n \ns\n::\nBool\n,\n \na\n::\nInt64\n)\n\n    \n# rewarded for escaping, penalized for listening and getting caught\n\n    \nr\n \n=\n \n0.0\n\n    \na\n \n==\n \n0\n \n?\n \n(\nr\n+=\npomdp\n.\nr_listen\n)\n \n:\n \n(\nnothing\n)\n\n    \nif\n \na\n \n==\n \n1\n\n        \ns\n \n?\n \n(\nr\n \n+=\n \npomdp\n.\nr_findtiger\n)\n \n:\n \n(\nr\n \n+=\n \npomdp\n.\nr_escapetiger\n)\n\n    \nend\n\n    \nif\n \na\n \n==\n \n2\n\n        \ns\n \n?\n \n(\nr\n \n+=\n \npomdp\n.\nr_escapetiger\n)\n \n:\n \n(\nr\n \n+=\n \npomdp\n.\nr_findtiger\n)\n\n    \nend\n\n    \nreturn\n \nr\n\n\nend\n\n\n# convenience function\n\n\nreward\n(\npomdp\n::\nTigerPOMDP\n,\n \ns\n::\nBool\n,\n \na\n::\nInt64\n,\n \nsp\n::\nBool\n)\n \n=\n \nreward\n(\npomdp\n,\n \ns\n,\n \na\n)\n\n\n\n\n\n\nThe last important component of a POMDP are the spaces. There is a special \nAbstractSpace\n type in POMDPs.jl which all spaces inherit from. We define the state, action, and observation spaces below as well as functions for intializing them and sampling from them.\n\n\n# STATE SPACE\n\n\ntype\n TigerStateSpace\n \n:\n \nAbstractSpace\n\n    \nstates\n::\nVector\n{\nBool\n}\n \n# states are boolean\n\n\nend\n\n\n# initialize the state space\n\n\nstates\n(::\nTigerPOMDP\n)\n \n=\n \nTigerStateSpace\n([\ntrue\n,\n \nfalse\n])\n\n\n# for iterating over discrete spaces\n\n\niterator\n(\nspace\n::\nTigerStateSpace\n)\n \n=\n \nspace\n.\nstates\n\n\ndimensions\n(::\nTigerStateSpace\n)\n \n=\n \n1\n\n\n# sample from the state sapce\n\n\nrand\n(\nrng\n::\nAbstractRNG\n,\n \nspace\n::\nTigerStateSpace\n,\n \ns\n::\nBool\n)\n \n=\n \nrand\n(\nrng\n)\n \n \n0.5\n \n?\n \n(\nreturn\n \ntrue\n)\n \n:\n \n(\nreturn\n \nfalse\n)\n\n\n\n# ACTION SPACE\n\n\ntype\n TigerActionSpace\n \n:\n \nAbstractSpace\n\n    \nactions\n::\nVector\n{\nInt64\n}\n \n# three possible actions\n\n\nend\n\n\n# initialize the action space\n\n\nactions\n(::\nTigerPOMDP\n)\n \n=\n \nTigerActionSpace\n([\n0\n,\n1\n,\n2\n])\n\n\n# iterate of the action space\n\n\niterator\n(\nspace\n::\nTigerActionSpace\n)\n \n=\n \nspace\n.\nactions\n\n\ndimensions\n(::\nTigerActionSpace\n)\n \n=\n \n1\n\n\n# sample from the aciton space\n\n\nrand\n(\nrng\n::\nAbstractRNG\n,\n \nspace\n::\nTigerActionSpace\n,\n \na\n::\nInt64\n)\n \n=\n \nrand\n(\nrng\n,\n \n0\n:\n2\n)\n\n\n\n# OBSERVATION SPACE\n\n\ntype\n TigerObservationSpace\n \n:\n \nAbstractSpace\n\n    \nobs\n::\nVector\n{\nBool\n}\n\n\nend\n\n\n# initialize\n\n\nobservations\n(::\nTigerPOMDP\n)\n \n=\n \nTigerObservationSpace\n([\ntrue\n,\n \nfalse\n])\n\n\n# iterate over obs space\n\n\niterator\n(\nspace\n::\nTigerObservationSpace\n)\n \n=\n \nspace\n.\nobs\n\n\ndimensions\n(::\nTigerObservationSpace\n)\n \n=\n \n1\n\n\n# sample from the obs sapce\n\n\nrand\n(\nrng\n::\nAbstractRNG\n,\n \nspace\n::\nTigerObservationSpace\n,\n \ns\n::\nBool\n)\n \n=\n \nrand\n(\nrng\n)\n \n \n0.5\n \n?\n \n(\nreturn\n \ntrue\n)\n \n:\n \n(\nreturn\n \nfalse\n)\n\n\n\n\n\n\nThe last important component of a POMDP is the initial distribution over the state space. In POMDPs.jl we make a strong distinction between this distribution and a belief. In most literature these two concepts are considered the same. However, in most general terms, a belief is something that is mapped to an action using a POMDP policy. If the policy is represented as something other than alpha-vectors (a policy graph, tree, or a reccurent neural netowrk to give a few examples), it may not make sense to think of a belief as a probability distribution over the state space. Thus, in POMDPs.jl we abstract the concept of a belief beyond a probability distribution (of course it can be a probability distriubtion if it makes sense). \n\n\nIn order to reconcile this difference, each policy has a function called \ninitialize_belief\n which takes in an initial state distirubtion (this is a probability distribution over the state space) and a policy, and converts the distribution into what we call a belief in POMDPs.jl - a representation of a POMDP that is mapped to an action using the policy. \n\n\nLet's define the initial state distribution function for our POMDP.\n\n\ninitial_state_distribution\n(\npomdp\n::\nTigerPOMDP\n)\n \n=\n \nTigerDistribution\n(\n0.5\n,\n \n[\ntrue\n,\n \nfalse\n])\n\n\n\n\n\n\nNow that've defined all the main components, we need to wrap up our model by creating some convenience functions below.\n\n\n# initialization functions\n\n\ncreate_state\n(::\nTigerPOMDP\n)\n \n=\n \nzero\n(\nBool\n)\n\n\ncreate_observation\n(::\nTigerPOMDP\n)\n \n=\n \nzero\n(\nBool\n)\n\n\ncreate_action\n(::\nTigerPOMDP\n)\n \n=\n \nzero\n(\nInt64\n)\n\n\n\n# for discrete problems\n\n\nn_states\n(::\nTigerPOMDP\n)\n \n=\n \n2\n\n\nn_actions\n(::\nTigerPOMDP\n)\n \n=\n \n3\n\n\nn_observations\n(::\nTigerPOMDP\n)\n \n=\n \n2\n\n\n\n# for indexing discrete states\n\n\nstate_index\n(::\nTigerPOMDP\n,\n \ns\n::\nBool\n)\n \n=\n \nInt64\n(\ns\n)\n \n+\n \n1\n\n\n\ndiscount\n(\npomdp\n::\nTigerPOMDP\n)\n \n=\n \npomdp\n.\ndiscount_factor\n\n\n\n\n\n\nNow that we've defined all these functions, we can use one of the JuliaPOMDP solvers to compute and evaluate a policy. \n\n\nusing\n \nQMDP\n,\n \nPOMDPToolbox\n\n\n\npomdp\n \n=\n \nTigerPOMDP\n()\n\n\nsolver\n \n=\n \nQMDPSolver\n()\n\n\npolicy\n \n=\n \nsolve\n(\nsolver\n,\n \npomdp\n)\n\n\n\ninit_dist\n \n=\n \ninitial_state_distribution\n(\npomdp\n)\n\n\nhist\n \n=\n \nHistoryRecorder\n(\nmax_steps\n=\n100\n)\n \n# from POMDPToolbox\n\n\nr\n \n=\n \nsimulate\n(\nhist\n,\n \npomdp\n,\n \npolicy\n,\n \nbelief_updater\n,\n \ninit_dist\n)\n \n# run 100 step simulation\n\n\n\n\n\n\nPlease note that you do not need to define all the functions for most solvers. If you want to use an individual solver, you usually need only a subset of what's above. \n\n\n\n\nTabular Form POMDP\n\n\nAnother way to define discrete POMDP problems is by writing them in tabular form. Specifically, if you can write the transition and observation probabilities as well as the rewards in matrix form, you can use the \nDiscreteMDP\n or \nDiscretePOMDP\n types form \nPOMDPModels\n which automatically implements all the functions you'll need for you. Let's do this with the Tiger POMDP.\n\n\nusing\n \nPOMDPModels\n\n\n\n# write out the matrix forms\n\n\n\n# REWARDS\n\n\nR\n \n=\n \n[\n-\n1.\n \n-\n100\n \n10\n;\n \n-\n1\n \n10\n \n-\n100\n]\n \n# |S|x|A| state-action pair rewards\n\n\n\n# TRANSITIONS\n\n\nT\n \n=\n \nzeros\n(\n2\n,\n3\n,\n2\n)\n \n# |S|x|A|x|S|, T[s\n, a, s] = p(s\n|a,s)\n\n\nT\n[:,:,\n1\n]\n \n=\n \n[\n1.\n \n0.5\n \n0.5\n;\n \n0\n \n0.5\n \n0.5\n]\n\n\nT\n[:,:,\n2\n]\n \n=\n \n[\n0.\n \n0.5\n \n0.5\n;\n \n1\n \n0.5\n \n0.5\n]\n\n\n\n# OBSERVATIONS\n\n\nO\n \n=\n \nzeros\n(\n2\n,\n3\n,\n2\n)\n \n# |O|x|A|x|S|, O[o, a, s] = p(o|a,s)\n\n\nO\n[:,:,\n1\n]\n \n=\n \n[\n0.85\n \n0.5\n \n0.5\n;\n \n0.15\n \n0.5\n \n0.5\n]\n\n\nO\n[:,:,\n2\n]\n \n=\n \n[\n0.15\n \n0.5\n \n0.5\n;\n \n0.85\n \n0.5\n \n0.5\n]\n\n\n\ndiscount\n \n=\n \n0.95\n\n\npomdp\n \n=\n \nDiscretePOMDP\n(\nT\n,\n \nR\n,\n \nO\n,\n \ndiscount\n)\n\n\n\n# solve the POMDP the same way\n\n\nsolver\n \n=\n \nSARSOPSolver\n()\n\n\npolicy\n \n=\n \nsolve\n(\nsolver\n,\n \npomdp\n)\n\n\n\n\n\n\nIt is usually fairly simple to define smaller problems in the tabular form. However, for larger problems it can be tedious and the functional form may be preffered. You can usually use any supported POMDP solver to sovle these types of problems (the performance of the policy may vary however - SARSOP will usually outperform QMDP). \n\n\n\n\nContinous POMDP\n\n\nWithin the POMDPs.jl interface, we can also define problems with continuous spaces. There are a few solvers that can handle these types of problems, namely, MCVI and POMCP (with some tunning). Light-Dark problem here. What should we say about bounds? This is a good place to discuss GenerativeModels. Would also be good if we could include RL stuff somewhere (depending on Chris).", 
            "title": "Defining a POMDP"
        }, 
        {
            "location": "/def_pomdp/#defining-a-pomdp", 
            "text": "The expressive nature of POMDPs.jl gives problem writers the flexiblity to write their problem in many forms. In this section we will take a look at two ways to write a discrete problem, and a way of writing a continuous problem.", 
            "title": "Defining a POMDP"
        }, 
        {
            "location": "/def_pomdp/#functional-form-pomdp", 
            "text": "The first, and most straighforward way to define a POMDP problem is to implement the model functions that you may need. For example, all POMDPs will need  transition ,  reward , and  observation  functions. In this example we'll start with the simple Tiger POMDP problem. We want to use the SARSOP solver to compute a policy. To use a solver from JuliaPOMDP, a problem writer must define a set of functions required by the solver. To see what functions are required by SARSOP, check out its documentation  here .   Let's first define the Tiger POMDP type.  using   POMDPs   # load the interface  type  TigerPOMDP   :   POMDP { Bool ,   Int64 ,   Bool }   # parametarized inheritance POMDP{state, action, observation} \n     r_listen :: Float64   # reward for listening (negative) \n     r_findtiger :: Float64   # reward for finding the tiger (negative) \n     r_escapetiger :: Float64   # reward for escaping \n     p_listen_correctly :: Float64   # probbility that we hear the tiger correctly \n     discount_factor :: Float64   # discount factor  end  TigerPOMDP ()   =   TigerPOMDP ( - 1.0 ,   - 100.0 ,   10.0 ,   0.85 ,   0.95 )   # default contructor   Notice that the  TigerPOMDP  is inheriting from the abstract  POMDP  type that comes from POMDPs.jl. The abstract  POMDP  is parametarized by a  Bool ,  Int64 ,  Bool  combination with the syntax  TigerPOMDP  : POMDP{Bool, Int64, Bool} . The parametarization defines how we choose to represent the state, actions, and observations in our problem. In the  TigerPOMDP  we use a boolean to represent our states and observations (because there are two of each) and an integer to represent our actions (because there are 3). If you wanted to create a custom concrete type to represent your states, actions, or observations you could do that as well. Let's say we made a type to represent our states called  AwesomeTigerState . That type could contain integers, floats, arrays, or other complex data structures (depending on what's convenient). We would then parametrize the Tiger POMDP in the following way:  type TigerPOMDP  : POMDP{AwesomeTigerState, Int64, Bool} .  Now, let's consider another important component of POMDPs, probability distributions. In the POMDPs.jl interface, we think in terms of distribution types. We want to be able to sample from these distriubtions and compute their probability masses or densities. In the Tiger POMDP, our distriubtions are over binary variables (boolean state or observation), so we can implement a simple version of a Bernoulli distribution.  type  TigerDistribution   :   AbstractDistribution   # inherits from a POMDPs.jl abstract type \n     p :: Float64   # probability of 1 \n     it :: Vector { Bool }   # pre-allocate the domain of the distriubtion  end  TigerDistribution ()   =   TigerDistribution ( 0.5 ,   [ true ,   false ])   # default constructo  iterator ( d :: TigerDistribution )   =   d . it   # convenience function used by discrete solvers (iterator over the discrete distriubtion)   Let's implement the pdf and rand function that returns the probability mass and samples from the distribution.  # returns the probability mass   function  pdf ( d :: TigerDistribution ,   so :: Bool ) \n     so   ?   ( return   d . p )   :   ( return   1.0 - d . p )  end  # samples the dsitribution  rand ( rng :: AbstractRNG ,   d :: TigerDistribution ,   s :: Bool )   =   rand ( rng )   =   d . p   We also want some convenience functions for initializing the distriubtions.  create_transition_distribution (:: TigerPOMDP )   =   TigerDistribution ()  create_observation_distribution (:: TigerPOMDP )   =   TigerDistribution ()   Let's define our transition, observation, and reward functions.  function  transition ( pomdp :: TigerPOMDP ,   s :: Bool ,   a :: Int64 ,   d :: TigerDistribution = create_transition_distribution ( pomdp )) \n     # Resets the problem after opening door; does nothing after listening         \n     if   a   ==   1   ||   a   ==   2 \n         d . p   =   0.5 \n     elseif   s \n         d . p   =   1.0 \n     else \n         d . p   =   0.0 \n     end \n     d  end  function  observation ( pomdp :: TigerPOMDP ,   s :: Bool ,   a :: Int64 ,   d :: TigerDistribution = create_observation_distribution ( pomdp )) \n     # correct observation wiht prob pc         \n     pc   =   pomdp . p_listen_correctly \n     if   a   ==   0 \n         s   ?   ( d . p   =   pc )   :   ( d . p   =   1.0 - pc ) \n     else \n         d . p   =   0.5 \n     end \n     d  end  # convenience function  function  observation ( pomdp :: TigerPOMDP ,   s :: Bool ,   a :: Int64 ,   sp :: Bool ,   d :: TigerDistribution = create_observation_distribution ( pomdp )) \n     return   observation ( pomdp ,   s ,   a ,   d )  end  function  reward ( pomdp :: TigerPOMDP ,   s :: Bool ,   a :: Int64 ) \n     # rewarded for escaping, penalized for listening and getting caught \n     r   =   0.0 \n     a   ==   0   ?   ( r += pomdp . r_listen )   :   ( nothing ) \n     if   a   ==   1 \n         s   ?   ( r   +=   pomdp . r_findtiger )   :   ( r   +=   pomdp . r_escapetiger ) \n     end \n     if   a   ==   2 \n         s   ?   ( r   +=   pomdp . r_escapetiger )   :   ( r   +=   pomdp . r_findtiger ) \n     end \n     return   r  end  # convenience function  reward ( pomdp :: TigerPOMDP ,   s :: Bool ,   a :: Int64 ,   sp :: Bool )   =   reward ( pomdp ,   s ,   a )   The last important component of a POMDP are the spaces. There is a special  AbstractSpace  type in POMDPs.jl which all spaces inherit from. We define the state, action, and observation spaces below as well as functions for intializing them and sampling from them.  # STATE SPACE  type  TigerStateSpace   :   AbstractSpace \n     states :: Vector { Bool }   # states are boolean  end  # initialize the state space  states (:: TigerPOMDP )   =   TigerStateSpace ([ true ,   false ])  # for iterating over discrete spaces  iterator ( space :: TigerStateSpace )   =   space . states  dimensions (:: TigerStateSpace )   =   1  # sample from the state sapce  rand ( rng :: AbstractRNG ,   space :: TigerStateSpace ,   s :: Bool )   =   rand ( rng )     0.5   ?   ( return   true )   :   ( return   false )  # ACTION SPACE  type  TigerActionSpace   :   AbstractSpace \n     actions :: Vector { Int64 }   # three possible actions  end  # initialize the action space  actions (:: TigerPOMDP )   =   TigerActionSpace ([ 0 , 1 , 2 ])  # iterate of the action space  iterator ( space :: TigerActionSpace )   =   space . actions  dimensions (:: TigerActionSpace )   =   1  # sample from the aciton space  rand ( rng :: AbstractRNG ,   space :: TigerActionSpace ,   a :: Int64 )   =   rand ( rng ,   0 : 2 )  # OBSERVATION SPACE  type  TigerObservationSpace   :   AbstractSpace \n     obs :: Vector { Bool }  end  # initialize  observations (:: TigerPOMDP )   =   TigerObservationSpace ([ true ,   false ])  # iterate over obs space  iterator ( space :: TigerObservationSpace )   =   space . obs  dimensions (:: TigerObservationSpace )   =   1  # sample from the obs sapce  rand ( rng :: AbstractRNG ,   space :: TigerObservationSpace ,   s :: Bool )   =   rand ( rng )     0.5   ?   ( return   true )   :   ( return   false )   The last important component of a POMDP is the initial distribution over the state space. In POMDPs.jl we make a strong distinction between this distribution and a belief. In most literature these two concepts are considered the same. However, in most general terms, a belief is something that is mapped to an action using a POMDP policy. If the policy is represented as something other than alpha-vectors (a policy graph, tree, or a reccurent neural netowrk to give a few examples), it may not make sense to think of a belief as a probability distribution over the state space. Thus, in POMDPs.jl we abstract the concept of a belief beyond a probability distribution (of course it can be a probability distriubtion if it makes sense).   In order to reconcile this difference, each policy has a function called  initialize_belief  which takes in an initial state distirubtion (this is a probability distribution over the state space) and a policy, and converts the distribution into what we call a belief in POMDPs.jl - a representation of a POMDP that is mapped to an action using the policy.   Let's define the initial state distribution function for our POMDP.  initial_state_distribution ( pomdp :: TigerPOMDP )   =   TigerDistribution ( 0.5 ,   [ true ,   false ])   Now that've defined all the main components, we need to wrap up our model by creating some convenience functions below.  # initialization functions  create_state (:: TigerPOMDP )   =   zero ( Bool )  create_observation (:: TigerPOMDP )   =   zero ( Bool )  create_action (:: TigerPOMDP )   =   zero ( Int64 )  # for discrete problems  n_states (:: TigerPOMDP )   =   2  n_actions (:: TigerPOMDP )   =   3  n_observations (:: TigerPOMDP )   =   2  # for indexing discrete states  state_index (:: TigerPOMDP ,   s :: Bool )   =   Int64 ( s )   +   1  discount ( pomdp :: TigerPOMDP )   =   pomdp . discount_factor   Now that we've defined all these functions, we can use one of the JuliaPOMDP solvers to compute and evaluate a policy.   using   QMDP ,   POMDPToolbox  pomdp   =   TigerPOMDP ()  solver   =   QMDPSolver ()  policy   =   solve ( solver ,   pomdp )  init_dist   =   initial_state_distribution ( pomdp )  hist   =   HistoryRecorder ( max_steps = 100 )   # from POMDPToolbox  r   =   simulate ( hist ,   pomdp ,   policy ,   belief_updater ,   init_dist )   # run 100 step simulation   Please note that you do not need to define all the functions for most solvers. If you want to use an individual solver, you usually need only a subset of what's above.", 
            "title": "Functional Form POMDP"
        }, 
        {
            "location": "/def_pomdp/#tabular-form-pomdp", 
            "text": "Another way to define discrete POMDP problems is by writing them in tabular form. Specifically, if you can write the transition and observation probabilities as well as the rewards in matrix form, you can use the  DiscreteMDP  or  DiscretePOMDP  types form  POMDPModels  which automatically implements all the functions you'll need for you. Let's do this with the Tiger POMDP.  using   POMDPModels  # write out the matrix forms  # REWARDS  R   =   [ - 1.   - 100   10 ;   - 1   10   - 100 ]   # |S|x|A| state-action pair rewards  # TRANSITIONS  T   =   zeros ( 2 , 3 , 2 )   # |S|x|A|x|S|, T[s , a, s] = p(s |a,s)  T [:,:, 1 ]   =   [ 1.   0.5   0.5 ;   0   0.5   0.5 ]  T [:,:, 2 ]   =   [ 0.   0.5   0.5 ;   1   0.5   0.5 ]  # OBSERVATIONS  O   =   zeros ( 2 , 3 , 2 )   # |O|x|A|x|S|, O[o, a, s] = p(o|a,s)  O [:,:, 1 ]   =   [ 0.85   0.5   0.5 ;   0.15   0.5   0.5 ]  O [:,:, 2 ]   =   [ 0.15   0.5   0.5 ;   0.85   0.5   0.5 ]  discount   =   0.95  pomdp   =   DiscretePOMDP ( T ,   R ,   O ,   discount )  # solve the POMDP the same way  solver   =   SARSOPSolver ()  policy   =   solve ( solver ,   pomdp )   It is usually fairly simple to define smaller problems in the tabular form. However, for larger problems it can be tedious and the functional form may be preffered. You can usually use any supported POMDP solver to sovle these types of problems (the performance of the policy may vary however - SARSOP will usually outperform QMDP).", 
            "title": "Tabular Form POMDP"
        }, 
        {
            "location": "/def_pomdp/#continous-pomdp", 
            "text": "Within the POMDPs.jl interface, we can also define problems with continuous spaces. There are a few solvers that can handle these types of problems, namely, MCVI and POMCP (with some tunning). Light-Dark problem here. What should we say about bounds? This is a good place to discuss GenerativeModels. Would also be good if we could include RL stuff somewhere (depending on Chris).", 
            "title": "Continous POMDP"
        }, 
        {
            "location": "/def_solver/", 
            "text": "Defining a Solver", 
            "title": "Defining a Solver"
        }, 
        {
            "location": "/def_solver/#defining-a-solver", 
            "text": "", 
            "title": "Defining a Solver"
        }, 
        {
            "location": "/concepts/", 
            "text": "Concepts and Architecture\n\n\n\n\nBelief\n\n\nThe last important component of a POMDP is the initial distribution over the state of the agent. In POMDPs.jl we make a strong distinction between this distribution and a belief. In most literature these two concepts are considered the same. However, in most general terms, a belief is something that is mapped to an action using a POMDP policy. If the policy is represented as something other than alpha-vectors (a policy graph, tree, or a reccurent neural netowrk to give a few examples), it doesn't make sense to think of a belief as a probability distribution over the state space. Thus, in POMDPs.jl we abstract the concept of a belief beyond a probability distribution (of course it can be a probability distriubtion if it makes sense). \n\n\nIn order to reconcile this difference, each policy has a function called \ninitialize_belief\n which takes in an initial state distirubtion (this is a probability distribution over the state space of a POMDP) and a policy, and converts the distribution into what we call a belief in POMDPs.jl - a representation of a POMDP that is mapped to an action using the policy.", 
            "title": "Concepts and Architecture"
        }, 
        {
            "location": "/concepts/#concepts-and-architecture", 
            "text": "", 
            "title": "Concepts and Architecture"
        }, 
        {
            "location": "/concepts/#belief", 
            "text": "The last important component of a POMDP is the initial distribution over the state of the agent. In POMDPs.jl we make a strong distinction between this distribution and a belief. In most literature these two concepts are considered the same. However, in most general terms, a belief is something that is mapped to an action using a POMDP policy. If the policy is represented as something other than alpha-vectors (a policy graph, tree, or a reccurent neural netowrk to give a few examples), it doesn't make sense to think of a belief as a probability distribution over the state space. Thus, in POMDPs.jl we abstract the concept of a belief beyond a probability distribution (of course it can be a probability distriubtion if it makes sense).   In order to reconcile this difference, each policy has a function called  initialize_belief  which takes in an initial state distirubtion (this is a probability distribution over the state space of a POMDP) and a policy, and converts the distribution into what we call a belief in POMDPs.jl - a representation of a POMDP that is mapped to an action using the policy.", 
            "title": "Belief"
        }, 
        {
            "location": "/api/", 
            "text": "API Documentation\n\n\nDocumentation for the \nPOMDPs.jl\n user interface. You can get help for any type or function in the module by typing \n?\n in the Julia REPL followed by the name of type or function. For example:\n\n\njulia\nusing\n \nPOMDPs\n\n\njulia\n?\n\n\nhelp\n?\nreward\n\n\nsearch\n:\n \nreward\n\n\n  \nreward\n{\nS\n,\nA\n,\nO\n}(\npomdp\n::\nPOMDP\n{\nS\n,\nA\n,\nO\n},\n \nstate\n::\nS\n,\n \naction\n::\nA\n,\n \nstatep\n::\nS\n)\n\n\n  \nReturns\n \nthe\n \nimmediate\n \nreward\n \nfor\n \nthe\n \ns\n-\na\n-\ns\n \ntriple\n\n\n  \nreward\n{\nS\n,\nA\n,\nO\n}(\npomdp\n::\nPOMDP\n{\nS\n,\nA\n,\nO\n},\n \nstate\n::\nS\n,\n \naction\n::\nA\n)\n\n\n  \nReturns\n \nthe\n \nimmediate\n \nreward\n \nfor\n \nthe\n \ns\n-\na\n \npair\n\n\n\n\n\n\n\n\nContents\n\n\n\n\nAPI Documentation\n\n\nContents\n\n\nIndex\n\n\nTypes\n\n\nModel Functions\n\n\nDistribution/Space Functions\n\n\nBelief Functions\n\n\nPolicy and Solver Functions\n\n\nSimulator\n\n\nUtility Tools\n\n\nConstants\n\n\n\n\n\n\n\n\n\n\nIndex\n\n\n\n\nPOMDPs.REMOTE_URL\n\n\nPOMDPs.SUPPORTED_PACKAGES\n\n\nPOMDPs.AbstractDistribution\n\n\nPOMDPs.AbstractSpace\n\n\nPOMDPs.MDP\n\n\nPOMDPs.POMDP\n\n\nPOMDPs.Policy\n\n\nPOMDPs.Simulator\n\n\nPOMDPs.Solver\n\n\nPOMDPs.Updater\n\n\nBase.Random.rand\n\n\nPOMDPs.action\n\n\nPOMDPs.action_index\n\n\nPOMDPs.actions\n\n\nPOMDPs.add\n\n\nPOMDPs.add_all\n\n\nPOMDPs.available\n\n\nPOMDPs.create_action\n\n\nPOMDPs.create_belief\n\n\nPOMDPs.create_observation\n\n\nPOMDPs.create_observation_distribution\n\n\nPOMDPs.create_policy\n\n\nPOMDPs.create_state\n\n\nPOMDPs.create_transition_distribution\n\n\nPOMDPs.dimensions\n\n\nPOMDPs.discount\n\n\nPOMDPs.initial_state_distribution\n\n\nPOMDPs.initialize_belief\n\n\nPOMDPs.isterminal\n\n\nPOMDPs.isterminal_obs\n\n\nPOMDPs.iterator\n\n\nPOMDPs.n_actions\n\n\nPOMDPs.n_observations\n\n\nPOMDPs.n_states\n\n\nPOMDPs.obs_index\n\n\nPOMDPs.observation\n\n\nPOMDPs.observations\n\n\nPOMDPs.pdf\n\n\nPOMDPs.reward\n\n\nPOMDPs.simulate\n\n\nPOMDPs.solve\n\n\nPOMDPs.state_index\n\n\nPOMDPs.states\n\n\nPOMDPs.strip_arg\n\n\nPOMDPs.test_all\n\n\nPOMDPs.transition\n\n\nPOMDPs.update\n\n\nPOMDPs.updater\n\n\nPOMDPs.value\n\n\nPOMDPs.@pomdp_func\n\n\n\n\n\n\nTypes\n\n\n#\n\n\nPOMDPs.POMDP\n \n \nType\n.\n\n\nAbstract base type for a partially observable Markov decision process.\n\n\nS\n:\n \nstate\n \ntype\n\n\nA\n:\n \naction\n \ntype\n\n\nO\n:\n \nobservation\n \ntype\n\n\n\n\n\n\n#\n\n\nPOMDPs.MDP\n \n \nType\n.\n\n\nAbstract base type for a fully observable Markov decision process.\n\n\nS\n:\n \nstate\n \ntype\n\n\nA\n:\n \naction\n \ntype\n\n\n\n\n\n\n#\n\n\nPOMDPs.AbstractSpace\n \n \nType\n.\n\n\nBase type for state, action and observation spaces.\n\n\nT\n:\n \ntype\n \nthat\n \nparametarizes\n \nthe\n \nspace\n \n(\nstate\n,\n \naction\n,\n \nor\n \nobservation\n)\n\n\n\n\n\n\n#\n\n\nPOMDPs.AbstractDistribution\n \n \nType\n.\n\n\nAbstract type for a probability distribution.\n\n\nT\n:\n \ntype\n \nover\n \nwhich\n \ndistribution\n \nis\n \nover\n \n(\nstate\n,\n \naction\n,\n \nor\n \nobservation\n)\n\n\n\n\n\n\n#\n\n\nPOMDPs.Solver\n \n \nType\n.\n\n\nBase type for an MDP/POMDP solver\n\n\n#\n\n\nPOMDPs.Policy\n \n \nType\n.\n\n\nBase type for a policy (a map from every possible belief, or more abstract policy state, to an optimal or suboptimal action)\n\n\nB\n:\n \na\n \nbelief\n \n(\nor\n \npolicy\n \nstate\n)\n \nthat\n \nrepresents\n \nthe\n \nknowledge\n \nan\n \nagent\n \nhas\n \nabout\n \nthe\n \nstate\n \nof\n \nthe\n \nsystem\n\n\n\n\n\n\n#\n\n\nPOMDPs.Updater\n \n \nType\n.\n\n\nAbstract type for an object that defines how the belief should be updated\n\n\nB\n:\n \nbelief\n \ntype\n \nthat\n \nparametarizes\n \nthe\n \nupdater\n\n\n\n\n\n\nA belief is a general construct that represents the knowledge an agent has about the state of the system. This can be a probability distribution, an action observation history or a more general representation. \n\n\n\n\nModel Functions\n\n\n#\n\n\nPOMDPs.states\n \n \nFunction\n.\n\n\nstates{S,A,O}(problem::POMDP{S,A,O}, state::S)\nstates{S,A}(problem::MDP{S,A}, state::S)\n\n\n\n\n\nReturns a subset of the state space reachable from \nstate\n. \n\n\nstates(problem::POMDP)\nstates(problem::MDP)\n\n\n\n\n\nReturns the complete state space of a POMDP. \n\n\n#\n\n\nPOMDPs.actions\n \n \nFunction\n.\n\n\nactions{S,A,O}(problem::POMDP{S,A,O}, state::S, aspace::AbstractSpace{A})\nactions{S,A}(problem::MDP{S,A}, state::S, aspace::AbstractSpace{A})\n\n\n\n\n\nModifies aspace to the action space accessible from the given state and returns it.\n\n\nactions(problem::POMDP)\nactions(problem::MDP)\n\n\n\n\n\nReturns the entire action space of a POMDP.\n\n\nactions{S,A,O,B}(problem::POMDP{S,A,O}, belief::B, aspace::AbstractSpace{A})\n\n\n\n\n\nModifies aspace to the action space accessible from the states with nonzero belief and returns it.\n\n\n#\n\n\nPOMDPs.observations\n \n \nFunction\n.\n\n\nobservations{S,A,O}(problem::POMDP{S,A,O}, state::S, obs::AbstractSpace{O}=observations(problem))\n\n\n\n\n\nModifies ospace to the observation space accessible from the given state and returns it.\n\n\nobservations(problem::POMDP)\n\n\n\n\n\nReturns the entire observation space.\n\n\n#\n\n\nPOMDPs.reward\n \n \nFunction\n.\n\n\nreward{S,A,O}(problem::POMDP{S,A,O}, state::S, action::A, statep::S)\nreward{S,A}(problem::MDP{S,A}, state::S, action::A, statep::S)\n\n\n\n\n\nReturns the immediate reward for the s-a-s' triple\n\n\nreward{S,A,O}(problem::POMDP{S,A,O}, state::S, action::A)\nreward{S,A}(problem::MDP{S,A}, state::S, action::A)\n\n\n\n\n\nReturns the immediate reward for the s-a pair\n\n\n#\n\n\nPOMDPs.transition\n \n \nFunction\n.\n\n\ntransition{S,A,O}(problem::POMDP{S,A,O}, state::S, action::A,\n\n\n\n\n\ndistribution::AbstractDistribution{S}=create_transition_distribution(problem))     transition{S,A}(problem::MDP{S,A}, state::S, action::A, distribution::AbstractDistribution{S}=create_transition_distribution(problem))\n\n\nReturns the transition distribution from the current state-action pair\n\n\n#\n\n\nPOMDPs.observation\n \n \nFunction\n.\n\n\nobservation{S,A,O}(problem::POMDP{S,A,O}, state::S, action::A, statep::S, distribution::AbstractDistribution{O}=create_observation_distribution(problem))\n\n\n\n\n\nReturns the observation distribution for the s-a-s' tuple (state, action, and next state)\n\n\nobservation{S,A,O}(problem::POMDP{S,A,O}, action::A, statep::S, distribution::AbstractDistribution{O}=create_observation_distribution(problem))\n\n\n\n\n\nModifies distribution to the observation distribution for the a-s' tuple (action and next state) and returns it\n\n\n#\n\n\nPOMDPs.isterminal\n \n \nFunction\n.\n\n\nisterminal{S,A,O}(problem::POMDP{S,A,O}, state::S)\nisterminal{S,A}(problem::MDP{S,A}, state::S)\n\n\n\n\n\nChecks if state s is terminal\n\n\n#\n\n\nPOMDPs.isterminal_obs\n \n \nFunction\n.\n\n\nisterminal_obs{S,A,O}(problem::POMDP{S,A,O}, observation::O)\n\n\n\n\n\nChecks if an observation is terminal.\n\n\n#\n\n\nPOMDPs.discount\n \n \nFunction\n.\n\n\ndiscount(problem::POMDP)\ndiscount(problem::MDP)\n\n\n\n\n\nReturn the discount factor for the problem.\n\n\n#\n\n\nPOMDPs.n_states\n \n \nFunction\n.\n\n\nn_states(problem::POMDP)\nn_states(problem::MDP)\n\n\n\n\n\nReturns the number of states in \nproblem\n. Used for discrete models only.\n\n\n#\n\n\nPOMDPs.n_actions\n \n \nFunction\n.\n\n\nn_actions(problem::POMDP)\nn_actions(problem::MDP)\n\n\n\n\n\nReturns the number of actions in \nproblem\n. Used for discrete models only.\n\n\n#\n\n\nPOMDPs.n_observations\n \n \nFunction\n.\n\n\nn_observations(problem::POMDP)\n\n\n\n\n\nReturns the number of actions in \nproblem\n. Used for discrete models only.\n\n\n#\n\n\nPOMDPs.state_index\n \n \nFunction\n.\n\n\nstate_index{S,A,O}(problem::POMDP{S,A,O}, s::S)\nstate_index{S,A}(problem::MDP{S,A}, s::S)\n\n\n\n\n\nReturns the integer index of state \ns\n. Used for discrete models only.\n\n\n#\n\n\nPOMDPs.action_index\n \n \nFunction\n.\n\n\naction_index{S,A,O}(problem::POMDP{S,A,O}, a::A)\naction_index{S,A}(problem::MDP{S,A}, a::A)\n\n\n\n\n\nReturns the integer index of action \na\n. Used for discrete models only.\n\n\n#\n\n\nPOMDPs.obs_index\n \n \nFunction\n.\n\n\nobs_index{S,A,O}(problem::POMDP{S,A,O}, o::O)\n\n\n\n\n\nReturns the integer index of observation \no\n. Used for discrete models only.\n\n\n#\n\n\nPOMDPs.create_state\n \n \nFunction\n.\n\n\ncreate_state(problem::POMDP)\ncreate_state(problem::MDP)\n\n\n\n\n\nCreate a state object (for preallocation purposes).\n\n\n#\n\n\nPOMDPs.create_action\n \n \nFunction\n.\n\n\ncreate_action(problem::POMDP)\ncreate_action(problem::MDP)\n\n\n\n\n\nCreates an action object (for preallocation purposes)\n\n\n#\n\n\nPOMDPs.create_observation\n \n \nFunction\n.\n\n\ncreate_observation(problem::POMDP)\n\n\n\n\n\nCreate an observation object (for preallocation purposes).\n\n\n\n\nDistribution/Space Functions\n\n\n#\n\n\nBase.Random.rand\n \n \nFunction\n.\n\n\nrand{T}(rng::AbstractRNG, d::AbstractSpace{T}, sample::T)\n\n\n\n\n\nReturns a random \nsample\n from space \ns\n.\n\n\nrand{T}(rng::AbstractRNG, d::AbstractDistribution{T}, sample::T)\n\n\n\n\n\nFill \nsample\n with a random element from distribution \nd\n. The sample can be a state, action or observation.\n\n\n#\n\n\nPOMDPs.pdf\n \n \nFunction\n.\n\n\npdf{T}(d::AbstractDistribution{T}, x::T)\n\n\n\n\n\nValue of probability distribution \nd\n function at sample \nx\n.\n\n\n#\n\n\nPOMDPs.dimensions\n \n \nFunction\n.\n\n\ndimensions{T}(s::AbstractSpace{T})\n\n\n\n\n\nReturns the number of dimensions in space \ns\n.\n\n\n#\n\n\nPOMDPs.iterator\n \n \nFunction\n.\n\n\niterator{T}(s::AbstractSpace{T})\n\n\n\n\n\nReturns an iterable type (array or custom iterator) corresponding to space \ns\n. \n\n\niterator{T}(d::AbstractDistribution{T})\n\n\n\n\n\nReturns an iterable type (array or custom iterator) corresponding to distribution \nd\n. \n\n\n#\n\n\nPOMDPs.initial_state_distribution\n \n \nFunction\n.\n\n\ninitial_state_distribution(pomdp::POMDP)\n\n\n\n\n\nReturns an initial belief for the pomdp.\n\n\n#\n\n\nPOMDPs.create_transition_distribution\n \n \nFunction\n.\n\n\ncreate_transition_distribution(problem::POMDP)\ncreate_transition_distribution(problem::MDP)\n\n\n\n\n\nReturns a transition distribution (for memory preallocation).\n\n\n#\n\n\nPOMDPs.create_observation_distribution\n \n \nFunction\n.\n\n\ncreate_observation_distribution(problem::POMDP)\ncreate_observation_distribution(problem::MDP)\n\n\n\n\n\nReturns an observation distribution (for memory preallocation).\n\n\n\n\nBelief Functions\n\n\n#\n\n\nPOMDPs.update\n \n \nFunction\n.\n\n\nupdate{B,A,O}(updater::Updater, belief_old::B, action::A, obs::O,\nbelief_new::B=create_belief(updater))\n\n\n\n\n\nReturns a new instance of an updated belief given \nbelief_old\n and the latest action and observation.\n\n\n#\n\n\nPOMDPs.create_belief\n \n \nFunction\n.\n\n\ncreate_belief(updater::Updater)\n\n\n\n\n\nCreates a belief object of the type used by \nupdater\n (preallocates memory)\n\n\ncreate_belief(pomdp::POMDP)\n\n\n\n\n\nCreates a belief either to be used by updater or pomdp\n\n\n#\n\n\nPOMDPs.initialize_belief\n \n \nFunction\n.\n\n\ninitialize_belief{B}(updater::Updater{B}, \n                     state_distribution::AbstractDistribution,\n                     new_belief::B=create_belief(updater))\ninitialize_belief{B}(updater::Updater{B},\n                     belief::Any,\n                     new_belief::B=create_belief(updater))\n\n\n\n\n\nReturns a belief that can be updated using \nupdater\n that has similar distribution to \nstate_distribution\n or \nbelief\n.\n\n\nThe conversion may be lossy. This function is also idempotent, i.e. there is a default implementation that passes the belief through when it is already the correct type: \ninitialize_belief{B}(updater::Updater{B}, belief::B) = belief\n\n\n\n\nPolicy and Solver Functions\n\n\n#\n\n\nPOMDPs.create_policy\n \n \nFunction\n.\n\n\ncreate_policy(solver::Solver, problem::POMDP)\ncreate_policy(solver::Solver, problem::MDP)\n\n\n\n\n\nCreates a policy object (for preallocation purposes)\n\n\n#\n\n\nPOMDPs.solve\n \n \nFunction\n.\n\n\nsolve(solver::Solver, problem::POMDP, policy=create_policy(solver, problem))\n\n\n\n\n\nSolves the POMDP using method associated with solver, and returns a policy. \n\n\n#\n\n\nPOMDPs.updater\n \n \nFunction\n.\n\n\nupdater(policy::Policy)\n\n\n\n\n\nReturns a default Updater appropriate for a belief type that policy \np\n can use\n\n\n#\n\n\nPOMDPs.action\n \n \nFunction\n.\n\n\naction{B}(p::Policy, x::B, action)\n\n\n\n\n\nFills and returns action based on the current state or belief, given the policy. B is a generalized information state - can be a state in an MDP, a distribution in POMDP, or any other representation needed to make a decision using the given policy. \n\n\naction{B}(policy::Policy, x::B)\n\n\n\n\n\nReturns an action for the current state or belief, given the policy\n\n\nIf an MDP is being simulated, x will be a state; if a POMDP is being simulated, x will be a belief\n\n\n#\n\n\nPOMDPs.value\n \n \nFunction\n.\n\n\nvalue{B}(p::Policy, x::B)\n\n\n\n\n\nReturns the utility value from policy p given the state\n\n\n\n\nSimulator\n\n\n#\n\n\nPOMDPs.Simulator\n \n \nType\n.\n\n\nBase type for an object defining how a simulation should be carried out\n\n\n#\n\n\nPOMDPs.simulate\n \n \nFunction\n.\n\n\nsimulate{S,A}(simulator::Simulator, problem::MDP{S,A}, policy::Policy, initial_state::S)\n\n\n\n\n\nRun a simulation using the specified policy and returns the accumulated reward\n\n\nsimulate{S,A,O,B}(simulator::Simulator, problem::POMDP{S,A,O}, policy::Policy{B}, updater::Updater{B}, initial_belief::Union{B,AbstractDistribution{S}})\n\n\n\n\n\nRun a simulation using the specified policy and returns the accumulated reward\n\n\n\n\nUtility Tools\n\n\n#\n\n\nPOMDPs.add\n \n \nFunction\n.\n\n\nadd(solver_name::AbstractString, v::Bool=true)\n\n\n\n\n\nDownloads and installs a registered solver with name \nsolver_name\n.  \nv\n is a verbose flag, when set to true, function will notify the user if solver is already installed. This function is not exported, and must be called:\n\n\njulia\n \nusing\n \nPOMDPs\n\n\njulia\n \nPOMDPs\n.\nadd\n(\nMCTS\n)\n\n\n\n\n\n\n#\n\n\nPOMDPs.add_all\n \n \nFunction\n.\n\n\nadd_all()\n\n\n\n\n\nDownloads and installs all the packages supported by JuliaPOMDP\n\n\n#\n\n\nPOMDPs.test_all\n \n \nFunction\n.\n\n\ntest_all()\n\n\n\n\n\nTests all the JuliaPOMDP packages installed on your current machine.\n\n\n#\n\n\nPOMDPs.available\n \n \nFunction\n.\n\n\navailable()\n\n\n\n\n\nPrints all the availiable packages in JuliaPOMDP\n\n\n#\n\n\nPOMDPs.@pomdp_func\n \n \nMacro\n.\n\n\nProvide a default function implementation that throws an error when called.\n\n\n#\n\n\nPOMDPs.strip_arg\n \n \nFunction\n.\n\n\nStrip anything extra (type annotations, default values, etc) from an argument.\n\n\nFor now this cannot handle keyword arguments (it will throw an error).\n\n\n\n\nConstants\n\n\n#\n\n\nPOMDPs.REMOTE_URL\n \n \nConstant\n.\n\n\nurl to remote JuliaPOMDP organization repo\n\n\n#\n\n\nPOMDPs.SUPPORTED_PACKAGES\n \n \nConstant\n.\n\n\nSet containing string names of officially supported solvers and utility packages (e.g. \nMCTS\n, \nSARSOP\n, \nPOMDPToolbox\n, etc).  If you have a validated solver that supports the POMDPs.jl API, contact the developers to add your solver to this list.", 
            "title": "API"
        }, 
        {
            "location": "/api/#api-documentation", 
            "text": "Documentation for the  POMDPs.jl  user interface. You can get help for any type or function in the module by typing  ?  in the Julia REPL followed by the name of type or function. For example:  julia using   POMDPs  julia ?  help ? reward  search :   reward \n\n   reward { S , A , O }( pomdp :: POMDP { S , A , O },   state :: S ,   action :: A ,   statep :: S ) \n\n   Returns   the   immediate   reward   for   the   s - a - s   triple \n\n   reward { S , A , O }( pomdp :: POMDP { S , A , O },   state :: S ,   action :: A ) \n\n   Returns   the   immediate   reward   for   the   s - a   pair", 
            "title": "API Documentation"
        }, 
        {
            "location": "/api/#contents", 
            "text": "API Documentation  Contents  Index  Types  Model Functions  Distribution/Space Functions  Belief Functions  Policy and Solver Functions  Simulator  Utility Tools  Constants", 
            "title": "Contents"
        }, 
        {
            "location": "/api/#index", 
            "text": "POMDPs.REMOTE_URL  POMDPs.SUPPORTED_PACKAGES  POMDPs.AbstractDistribution  POMDPs.AbstractSpace  POMDPs.MDP  POMDPs.POMDP  POMDPs.Policy  POMDPs.Simulator  POMDPs.Solver  POMDPs.Updater  Base.Random.rand  POMDPs.action  POMDPs.action_index  POMDPs.actions  POMDPs.add  POMDPs.add_all  POMDPs.available  POMDPs.create_action  POMDPs.create_belief  POMDPs.create_observation  POMDPs.create_observation_distribution  POMDPs.create_policy  POMDPs.create_state  POMDPs.create_transition_distribution  POMDPs.dimensions  POMDPs.discount  POMDPs.initial_state_distribution  POMDPs.initialize_belief  POMDPs.isterminal  POMDPs.isterminal_obs  POMDPs.iterator  POMDPs.n_actions  POMDPs.n_observations  POMDPs.n_states  POMDPs.obs_index  POMDPs.observation  POMDPs.observations  POMDPs.pdf  POMDPs.reward  POMDPs.simulate  POMDPs.solve  POMDPs.state_index  POMDPs.states  POMDPs.strip_arg  POMDPs.test_all  POMDPs.transition  POMDPs.update  POMDPs.updater  POMDPs.value  POMDPs.@pomdp_func", 
            "title": "Index"
        }, 
        {
            "location": "/api/#types", 
            "text": "#  POMDPs.POMDP     Type .  Abstract base type for a partially observable Markov decision process.  S :   state   type  A :   action   type  O :   observation   type   #  POMDPs.MDP     Type .  Abstract base type for a fully observable Markov decision process.  S :   state   type  A :   action   type   #  POMDPs.AbstractSpace     Type .  Base type for state, action and observation spaces.  T :   type   that   parametarizes   the   space   ( state ,   action ,   or   observation )   #  POMDPs.AbstractDistribution     Type .  Abstract type for a probability distribution.  T :   type   over   which   distribution   is   over   ( state ,   action ,   or   observation )   #  POMDPs.Solver     Type .  Base type for an MDP/POMDP solver  #  POMDPs.Policy     Type .  Base type for a policy (a map from every possible belief, or more abstract policy state, to an optimal or suboptimal action)  B :   a   belief   ( or   policy   state )   that   represents   the   knowledge   an   agent   has   about   the   state   of   the   system   #  POMDPs.Updater     Type .  Abstract type for an object that defines how the belief should be updated  B :   belief   type   that   parametarizes   the   updater   A belief is a general construct that represents the knowledge an agent has about the state of the system. This can be a probability distribution, an action observation history or a more general representation.", 
            "title": "Types"
        }, 
        {
            "location": "/api/#model-functions", 
            "text": "#  POMDPs.states     Function .  states{S,A,O}(problem::POMDP{S,A,O}, state::S)\nstates{S,A}(problem::MDP{S,A}, state::S)  Returns a subset of the state space reachable from  state .   states(problem::POMDP)\nstates(problem::MDP)  Returns the complete state space of a POMDP.   #  POMDPs.actions     Function .  actions{S,A,O}(problem::POMDP{S,A,O}, state::S, aspace::AbstractSpace{A})\nactions{S,A}(problem::MDP{S,A}, state::S, aspace::AbstractSpace{A})  Modifies aspace to the action space accessible from the given state and returns it.  actions(problem::POMDP)\nactions(problem::MDP)  Returns the entire action space of a POMDP.  actions{S,A,O,B}(problem::POMDP{S,A,O}, belief::B, aspace::AbstractSpace{A})  Modifies aspace to the action space accessible from the states with nonzero belief and returns it.  #  POMDPs.observations     Function .  observations{S,A,O}(problem::POMDP{S,A,O}, state::S, obs::AbstractSpace{O}=observations(problem))  Modifies ospace to the observation space accessible from the given state and returns it.  observations(problem::POMDP)  Returns the entire observation space.  #  POMDPs.reward     Function .  reward{S,A,O}(problem::POMDP{S,A,O}, state::S, action::A, statep::S)\nreward{S,A}(problem::MDP{S,A}, state::S, action::A, statep::S)  Returns the immediate reward for the s-a-s' triple  reward{S,A,O}(problem::POMDP{S,A,O}, state::S, action::A)\nreward{S,A}(problem::MDP{S,A}, state::S, action::A)  Returns the immediate reward for the s-a pair  #  POMDPs.transition     Function .  transition{S,A,O}(problem::POMDP{S,A,O}, state::S, action::A,  distribution::AbstractDistribution{S}=create_transition_distribution(problem))     transition{S,A}(problem::MDP{S,A}, state::S, action::A, distribution::AbstractDistribution{S}=create_transition_distribution(problem))  Returns the transition distribution from the current state-action pair  #  POMDPs.observation     Function .  observation{S,A,O}(problem::POMDP{S,A,O}, state::S, action::A, statep::S, distribution::AbstractDistribution{O}=create_observation_distribution(problem))  Returns the observation distribution for the s-a-s' tuple (state, action, and next state)  observation{S,A,O}(problem::POMDP{S,A,O}, action::A, statep::S, distribution::AbstractDistribution{O}=create_observation_distribution(problem))  Modifies distribution to the observation distribution for the a-s' tuple (action and next state) and returns it  #  POMDPs.isterminal     Function .  isterminal{S,A,O}(problem::POMDP{S,A,O}, state::S)\nisterminal{S,A}(problem::MDP{S,A}, state::S)  Checks if state s is terminal  #  POMDPs.isterminal_obs     Function .  isterminal_obs{S,A,O}(problem::POMDP{S,A,O}, observation::O)  Checks if an observation is terminal.  #  POMDPs.discount     Function .  discount(problem::POMDP)\ndiscount(problem::MDP)  Return the discount factor for the problem.  #  POMDPs.n_states     Function .  n_states(problem::POMDP)\nn_states(problem::MDP)  Returns the number of states in  problem . Used for discrete models only.  #  POMDPs.n_actions     Function .  n_actions(problem::POMDP)\nn_actions(problem::MDP)  Returns the number of actions in  problem . Used for discrete models only.  #  POMDPs.n_observations     Function .  n_observations(problem::POMDP)  Returns the number of actions in  problem . Used for discrete models only.  #  POMDPs.state_index     Function .  state_index{S,A,O}(problem::POMDP{S,A,O}, s::S)\nstate_index{S,A}(problem::MDP{S,A}, s::S)  Returns the integer index of state  s . Used for discrete models only.  #  POMDPs.action_index     Function .  action_index{S,A,O}(problem::POMDP{S,A,O}, a::A)\naction_index{S,A}(problem::MDP{S,A}, a::A)  Returns the integer index of action  a . Used for discrete models only.  #  POMDPs.obs_index     Function .  obs_index{S,A,O}(problem::POMDP{S,A,O}, o::O)  Returns the integer index of observation  o . Used for discrete models only.  #  POMDPs.create_state     Function .  create_state(problem::POMDP)\ncreate_state(problem::MDP)  Create a state object (for preallocation purposes).  #  POMDPs.create_action     Function .  create_action(problem::POMDP)\ncreate_action(problem::MDP)  Creates an action object (for preallocation purposes)  #  POMDPs.create_observation     Function .  create_observation(problem::POMDP)  Create an observation object (for preallocation purposes).", 
            "title": "Model Functions"
        }, 
        {
            "location": "/api/#distributionspace-functions", 
            "text": "#  Base.Random.rand     Function .  rand{T}(rng::AbstractRNG, d::AbstractSpace{T}, sample::T)  Returns a random  sample  from space  s .  rand{T}(rng::AbstractRNG, d::AbstractDistribution{T}, sample::T)  Fill  sample  with a random element from distribution  d . The sample can be a state, action or observation.  #  POMDPs.pdf     Function .  pdf{T}(d::AbstractDistribution{T}, x::T)  Value of probability distribution  d  function at sample  x .  #  POMDPs.dimensions     Function .  dimensions{T}(s::AbstractSpace{T})  Returns the number of dimensions in space  s .  #  POMDPs.iterator     Function .  iterator{T}(s::AbstractSpace{T})  Returns an iterable type (array or custom iterator) corresponding to space  s .   iterator{T}(d::AbstractDistribution{T})  Returns an iterable type (array or custom iterator) corresponding to distribution  d .   #  POMDPs.initial_state_distribution     Function .  initial_state_distribution(pomdp::POMDP)  Returns an initial belief for the pomdp.  #  POMDPs.create_transition_distribution     Function .  create_transition_distribution(problem::POMDP)\ncreate_transition_distribution(problem::MDP)  Returns a transition distribution (for memory preallocation).  #  POMDPs.create_observation_distribution     Function .  create_observation_distribution(problem::POMDP)\ncreate_observation_distribution(problem::MDP)  Returns an observation distribution (for memory preallocation).", 
            "title": "Distribution/Space Functions"
        }, 
        {
            "location": "/api/#belief-functions", 
            "text": "#  POMDPs.update     Function .  update{B,A,O}(updater::Updater, belief_old::B, action::A, obs::O,\nbelief_new::B=create_belief(updater))  Returns a new instance of an updated belief given  belief_old  and the latest action and observation.  #  POMDPs.create_belief     Function .  create_belief(updater::Updater)  Creates a belief object of the type used by  updater  (preallocates memory)  create_belief(pomdp::POMDP)  Creates a belief either to be used by updater or pomdp  #  POMDPs.initialize_belief     Function .  initialize_belief{B}(updater::Updater{B}, \n                     state_distribution::AbstractDistribution,\n                     new_belief::B=create_belief(updater))\ninitialize_belief{B}(updater::Updater{B},\n                     belief::Any,\n                     new_belief::B=create_belief(updater))  Returns a belief that can be updated using  updater  that has similar distribution to  state_distribution  or  belief .  The conversion may be lossy. This function is also idempotent, i.e. there is a default implementation that passes the belief through when it is already the correct type:  initialize_belief{B}(updater::Updater{B}, belief::B) = belief", 
            "title": "Belief Functions"
        }, 
        {
            "location": "/api/#policy-and-solver-functions", 
            "text": "#  POMDPs.create_policy     Function .  create_policy(solver::Solver, problem::POMDP)\ncreate_policy(solver::Solver, problem::MDP)  Creates a policy object (for preallocation purposes)  #  POMDPs.solve     Function .  solve(solver::Solver, problem::POMDP, policy=create_policy(solver, problem))  Solves the POMDP using method associated with solver, and returns a policy.   #  POMDPs.updater     Function .  updater(policy::Policy)  Returns a default Updater appropriate for a belief type that policy  p  can use  #  POMDPs.action     Function .  action{B}(p::Policy, x::B, action)  Fills and returns action based on the current state or belief, given the policy. B is a generalized information state - can be a state in an MDP, a distribution in POMDP, or any other representation needed to make a decision using the given policy.   action{B}(policy::Policy, x::B)  Returns an action for the current state or belief, given the policy  If an MDP is being simulated, x will be a state; if a POMDP is being simulated, x will be a belief  #  POMDPs.value     Function .  value{B}(p::Policy, x::B)  Returns the utility value from policy p given the state", 
            "title": "Policy and Solver Functions"
        }, 
        {
            "location": "/api/#simulator", 
            "text": "#  POMDPs.Simulator     Type .  Base type for an object defining how a simulation should be carried out  #  POMDPs.simulate     Function .  simulate{S,A}(simulator::Simulator, problem::MDP{S,A}, policy::Policy, initial_state::S)  Run a simulation using the specified policy and returns the accumulated reward  simulate{S,A,O,B}(simulator::Simulator, problem::POMDP{S,A,O}, policy::Policy{B}, updater::Updater{B}, initial_belief::Union{B,AbstractDistribution{S}})  Run a simulation using the specified policy and returns the accumulated reward", 
            "title": "Simulator"
        }, 
        {
            "location": "/api/#utility-tools", 
            "text": "#  POMDPs.add     Function .  add(solver_name::AbstractString, v::Bool=true)  Downloads and installs a registered solver with name  solver_name .   v  is a verbose flag, when set to true, function will notify the user if solver is already installed. This function is not exported, and must be called:  julia   using   POMDPs  julia   POMDPs . add ( MCTS )   #  POMDPs.add_all     Function .  add_all()  Downloads and installs all the packages supported by JuliaPOMDP  #  POMDPs.test_all     Function .  test_all()  Tests all the JuliaPOMDP packages installed on your current machine.  #  POMDPs.available     Function .  available()  Prints all the availiable packages in JuliaPOMDP  #  POMDPs.@pomdp_func     Macro .  Provide a default function implementation that throws an error when called.  #  POMDPs.strip_arg     Function .  Strip anything extra (type annotations, default values, etc) from an argument.  For now this cannot handle keyword arguments (it will throw an error).", 
            "title": "Utility Tools"
        }, 
        {
            "location": "/api/#constants", 
            "text": "#  POMDPs.REMOTE_URL     Constant .  url to remote JuliaPOMDP organization repo  #  POMDPs.SUPPORTED_PACKAGES     Constant .  Set containing string names of officially supported solvers and utility packages (e.g.  MCTS ,  SARSOP ,  POMDPToolbox , etc).  If you have a validated solver that supports the POMDPs.jl API, contact the developers to add your solver to this list.", 
            "title": "Constants"
        }, 
        {
            "location": "/faq/", 
            "text": "Frequently Asked Questions (FAQ)\n\n\n\n\nHow do I save my policies?\n\n\nWe reccomend using \nJLD\n to save the whole policy object. This is the simplest, and failry efficient way to save Julia objects. JLD uses HDF5 format underneath. If you've already computed a policy, you can simply run:\n\n\nusing\n \nJLD\n\n\nsave\n(\nmy_policy.jld\n,\n \npolicy\n,\n \npolicy\n)", 
            "title": "Frequently Asked Questions"
        }, 
        {
            "location": "/faq/#frequently-asked-questions-faq", 
            "text": "", 
            "title": "Frequently Asked Questions (FAQ)"
        }, 
        {
            "location": "/faq/#how-do-i-save-my-policies", 
            "text": "We reccomend using  JLD  to save the whole policy object. This is the simplest, and failry efficient way to save Julia objects. JLD uses HDF5 format underneath. If you've already computed a policy, you can simply run:  using   JLD  save ( my_policy.jld ,   policy ,   policy )", 
            "title": "How do I save my policies?"
        }
    ]
}