{
    "docs": [
        {
            "location": "/", 
            "text": "POMDPs\n\n\nA Julia interface for defining, solving and simulating partially observable Markov decision processes and their fully observable counterparts.\n\n\n\n\nPackage Features\n\n\n\n\nGeneral interface that can handle problems with discrete and continuous state/action/observation spaces\n\n\nA number of popular state-of-the-art solvers availiable to use out of the box\n\n\nTools that make it easy to define problems and simulate solutions\n\n\nSimple integration of custom solvers into the existing interface\n\n\n\n\n\n\nAvailible Packages\n\n\nThe POMDPs.jl package contains the interface used for expressing and solving Markov decision processes (MDPs) and partially observable Markov decision processes (POMDPs) in the Julia programming language. The \nJuliaPOMDP\n community maintains these packages. The packages currently maintained by JuliaPOMDP are as follows: \n\n\n\n\nMDP solvers:\n\n\n\n\nValue Iteration\n\n\nMonte Carlo Tree Search\n\n\n\n\n\n\nPOMDP solvers:\n\n\n\n\nQMDP\n\n\nSARSOP\n\n\nPOMCP\n\n\nDESPOT\n\n\nMCVI\n\n\nPOMDPSolve\n\n\n\n\n\n\nSupport Tools:\n\n\n\n\nPOMDPToolbox\n\n\nPOMDPModels\n\n\n\n\n\n\nInterface Extensions:\n\n\n\n\nGenerativeModels\n\n\nPOMDPBounds\n\n\n\n\n\n\nManual Outline\n\n\n\n\nPOMDPs\n\n\nPackage Features\n\n\nAvailible Packages\n\n\nManual Outline\n\n\n\n\n\n\nDefining a POMDP\n\n\nFrequently Asked Questions (FAQ)\n\n\n\n\n\n\nInstallation\n\n\nGetting Started\n\n\n\n\n\n\nAPI Documentation\n\n\nContents\n\n\nIndex\n\n\nTypes\n\n\nModel Functions\n\n\nDistribution/Space Functions\n\n\nBelief Functions\n\n\nPolicy and Solver Functions\n\n\nSimulator\n\n\nUtility Tools\n\n\nConstants\n\n\n\n\n\n\nConcepts\n\n\nArchitecture\n\n\nDefining a Solver", 
            "title": "About"
        }, 
        {
            "location": "/#pomdps", 
            "text": "A Julia interface for defining, solving and simulating partially observable Markov decision processes and their fully observable counterparts.", 
            "title": "POMDPs"
        }, 
        {
            "location": "/#package-features", 
            "text": "General interface that can handle problems with discrete and continuous state/action/observation spaces  A number of popular state-of-the-art solvers availiable to use out of the box  Tools that make it easy to define problems and simulate solutions  Simple integration of custom solvers into the existing interface", 
            "title": "Package Features"
        }, 
        {
            "location": "/#availible-packages", 
            "text": "The POMDPs.jl package contains the interface used for expressing and solving Markov decision processes (MDPs) and partially observable Markov decision processes (POMDPs) in the Julia programming language. The  JuliaPOMDP  community maintains these packages. The packages currently maintained by JuliaPOMDP are as follows:", 
            "title": "Availible Packages"
        }, 
        {
            "location": "/#mdp-solvers", 
            "text": "Value Iteration  Monte Carlo Tree Search", 
            "title": "MDP solvers:"
        }, 
        {
            "location": "/#pomdp-solvers", 
            "text": "QMDP  SARSOP  POMCP  DESPOT  MCVI  POMDPSolve", 
            "title": "POMDP solvers:"
        }, 
        {
            "location": "/#support-tools", 
            "text": "POMDPToolbox  POMDPModels", 
            "title": "Support Tools:"
        }, 
        {
            "location": "/#interface-extensions", 
            "text": "GenerativeModels  POMDPBounds", 
            "title": "Interface Extensions:"
        }, 
        {
            "location": "/#manual-outline", 
            "text": "POMDPs  Package Features  Availible Packages  Manual Outline    Defining a POMDP  Frequently Asked Questions (FAQ)    Installation  Getting Started    API Documentation  Contents  Index  Types  Model Functions  Distribution/Space Functions  Belief Functions  Policy and Solver Functions  Simulator  Utility Tools  Constants    Concepts  Architecture  Defining a Solver", 
            "title": "Manual Outline"
        }, 
        {
            "location": "/install/", 
            "text": "Installation\n\n\nIf you have a running Julia distriubtion (Julia 0.4 or greaer), you have everything you need to install POMDPs.jl. To install the package, simply run the following from the Julia REPL:\n\n\nPkg\n.\nadd\n(\nPOMDPs\n)\n \n# intalls the POMDPs.jl package\n\n\n\n\n\n\nOnce you have POMDPs.jl installed, you can install any package that is part of the JuliaPOMDP community by running:\n\n\nusing\n \nPOMDPs\n\n\nPOMDPs\n.\nadd\n(\nSARSOP\n)\n \n# installs the SARSOP solver\n\n\n\n\n\n\nThe code above will download and install all the dependencies automatically. All JuliaPOMDP packages have been tested on Linux and OS X, and most have been tested on Windows. \n\n\nTo get a list of all the availible packages run:\n\n\nPOMDPs\n.\navailable\n()\n \n# prints a list of all the availible packages that can be installed with POMDPs.add\n\n\n\n\n\n\nDue to the modular nature of the framework, you can install only the solvers/support tools you plan on using. However, if you want to install all of the supported JuliaPOMDP packages you can run the following code:\n\n\nPOMDPs\n.\nadd_all\n()\n \n# installs all the JuliaPOMDP packages (may take a few minutes)", 
            "title": "Installation"
        }, 
        {
            "location": "/install/#installation", 
            "text": "If you have a running Julia distriubtion (Julia 0.4 or greaer), you have everything you need to install POMDPs.jl. To install the package, simply run the following from the Julia REPL:  Pkg . add ( POMDPs )   # intalls the POMDPs.jl package   Once you have POMDPs.jl installed, you can install any package that is part of the JuliaPOMDP community by running:  using   POMDPs  POMDPs . add ( SARSOP )   # installs the SARSOP solver   The code above will download and install all the dependencies automatically. All JuliaPOMDP packages have been tested on Linux and OS X, and most have been tested on Windows.   To get a list of all the availible packages run:  POMDPs . available ()   # prints a list of all the availible packages that can be installed with POMDPs.add   Due to the modular nature of the framework, you can install only the solvers/support tools you plan on using. However, if you want to install all of the supported JuliaPOMDP packages you can run the following code:  POMDPs . add_all ()   # installs all the JuliaPOMDP packages (may take a few minutes)", 
            "title": "Installation"
        }, 
        {
            "location": "/get_started/", 
            "text": "Getting Started\n\n\nPOMDPs serves as the interface used by a number of packages under the \nJuliaPOMDP\n framework. It is essentially the agreed upon API used by all the other packages in JuliaPOMDP. If you are using this framework, you may be trying to accomplish one or more of the following three goals:\n\n\n\n\nSolve a decision or planning problem with stochastic dynamics (MDP) or partial observability (POMDP)\n\n\nEvaluate a solution in simulation\n\n\nTest your custom algorithm for solving MDPs or POMDPs against other state-of-the-art algorithms\n\n\n\n\nIf you are attempting to complete the first two goals, take a look at these Jupyer Notebook tutorials:\n\n\n\n\nMDP Tutorial\n for beginners gives an overview of using Value Iteration and Monte-Carlo Tree Search with the classic grid world problem\n\n\nPOMDP Tutorial\n gives an overview of using SARSOP and QMDP to solve the tiger problem\n\n\n\n\nIf you are trying to write your own algorithm for solving MDPs or POMDPs with this interface take a look at the API section of this guide.\n\n\nThe following snippet shows how a solver should be used to solve a problem and run a simulation.\n\n\nusing\n \nSARSOP\n\n\nusing\n \nPOMDPModels\n\n\n\nsolver\n \n=\n \nSARSOP\n()\n\n\n\nproblem\n \n=\n \nBabyPOMDP\n()\n\n\n\npolicy\n \n=\n \nsolve\n(\nsolver\n,\n \nproblem\n)\n\n\nup\n \n=\n \nupdater\n(\npolicy\n)\n\n\nsim\n \n=\n \nReferenceSimulator\n(\nMersenneTwister\n(\n1\n),\n \n10\n)\n\n\n\nr\n \n=\n \nsimulate\n(\nsim\n,\n \nproblem\n,\n \npolicy\n,\n \nup\n,\n \ninitial_state_distribution\n(\nproblem\n))\n\n\n\nprintln\n(\nReward: \n$r\n)", 
            "title": "Getting Started"
        }, 
        {
            "location": "/get_started/#getting-started", 
            "text": "POMDPs serves as the interface used by a number of packages under the  JuliaPOMDP  framework. It is essentially the agreed upon API used by all the other packages in JuliaPOMDP. If you are using this framework, you may be trying to accomplish one or more of the following three goals:   Solve a decision or planning problem with stochastic dynamics (MDP) or partial observability (POMDP)  Evaluate a solution in simulation  Test your custom algorithm for solving MDPs or POMDPs against other state-of-the-art algorithms   If you are attempting to complete the first two goals, take a look at these Jupyer Notebook tutorials:   MDP Tutorial  for beginners gives an overview of using Value Iteration and Monte-Carlo Tree Search with the classic grid world problem  POMDP Tutorial  gives an overview of using SARSOP and QMDP to solve the tiger problem   If you are trying to write your own algorithm for solving MDPs or POMDPs with this interface take a look at the API section of this guide.  The following snippet shows how a solver should be used to solve a problem and run a simulation.  using   SARSOP  using   POMDPModels  solver   =   SARSOP ()  problem   =   BabyPOMDP ()  policy   =   solve ( solver ,   problem )  up   =   updater ( policy )  sim   =   ReferenceSimulator ( MersenneTwister ( 1 ),   10 )  r   =   simulate ( sim ,   problem ,   policy ,   up ,   initial_state_distribution ( problem ))  println ( Reward:  $r )", 
            "title": "Getting Started"
        }, 
        {
            "location": "/def_pomdp/", 
            "text": "Defining a POMDP", 
            "title": "Defining a POMDP"
        }, 
        {
            "location": "/def_pomdp/#defining-a-pomdp", 
            "text": "", 
            "title": "Defining a POMDP"
        }, 
        {
            "location": "/def_solver/", 
            "text": "Defining a Solver", 
            "title": "Defining a Solver"
        }, 
        {
            "location": "/def_solver/#defining-a-solver", 
            "text": "", 
            "title": "Defining a Solver"
        }, 
        {
            "location": "/concepts/", 
            "text": "Concepts\n\n\n\n\nArchitecture", 
            "title": "Concepts and Architecture"
        }, 
        {
            "location": "/concepts/#concepts", 
            "text": "", 
            "title": "Concepts"
        }, 
        {
            "location": "/concepts/#architecture", 
            "text": "", 
            "title": "Architecture"
        }, 
        {
            "location": "/api/", 
            "text": "API Documentation\n\n\nDocumentation for the \nPOMDPs.jl\n user interface. You can get help for any type or function in the module by typing \n?\n in the Julia REPL followed by the name of type or function. For example:\n\n\njulia\nusing\n \nPOMDPs\n\n\njulia\n?\n\n\nhelp\n?\nreward\n\n\nsearch\n:\n \nreward\n\n\n  \nreward\n{\nS\n,\nA\n,\nO\n}(\npomdp\n::\nPOMDP\n{\nS\n,\nA\n,\nO\n},\n \nstate\n::\nS\n,\n \naction\n::\nA\n,\n \nstatep\n::\nS\n)\n\n\n  \nReturns\n \nthe\n \nimmediate\n \nreward\n \nfor\n \nthe\n \ns\n-\na\n-\ns\n \ntriple\n\n\n  \nreward\n{\nS\n,\nA\n,\nO\n}(\npomdp\n::\nPOMDP\n{\nS\n,\nA\n,\nO\n},\n \nstate\n::\nS\n,\n \naction\n::\nA\n)\n\n\n  \nReturns\n \nthe\n \nimmediate\n \nreward\n \nfor\n \nthe\n \ns\n-\na\n \npair\n\n\n\n\n\n\n\n\nContents\n\n\n\n\nAPI Documentation\n\n\nContents\n\n\nIndex\n\n\nTypes\n\n\nModel Functions\n\n\nDistribution/Space Functions\n\n\nBelief Functions\n\n\nPolicy and Solver Functions\n\n\nSimulator\n\n\nUtility Tools\n\n\nConstants\n\n\n\n\n\n\n\n\n\n\nIndex\n\n\n\n\nPOMDPs.REMOTE_URL\n\n\nPOMDPs.SUPPORTED_PACKAGES\n\n\nPOMDPs.AbstractDistribution\n\n\nPOMDPs.AbstractSpace\n\n\nPOMDPs.MDP\n\n\nPOMDPs.POMDP\n\n\nPOMDPs.Policy\n\n\nPOMDPs.Simulator\n\n\nPOMDPs.Solver\n\n\nPOMDPs.Updater\n\n\nBase.Random.rand\n\n\nPOMDPs.action\n\n\nPOMDPs.action_index\n\n\nPOMDPs.actions\n\n\nPOMDPs.add\n\n\nPOMDPs.add_all\n\n\nPOMDPs.available\n\n\nPOMDPs.create_action\n\n\nPOMDPs.create_belief\n\n\nPOMDPs.create_observation\n\n\nPOMDPs.create_observation_distribution\n\n\nPOMDPs.create_policy\n\n\nPOMDPs.create_state\n\n\nPOMDPs.create_transition_distribution\n\n\nPOMDPs.dimensions\n\n\nPOMDPs.discount\n\n\nPOMDPs.initial_state_distribution\n\n\nPOMDPs.initialize_belief\n\n\nPOMDPs.isterminal\n\n\nPOMDPs.isterminal_obs\n\n\nPOMDPs.iterator\n\n\nPOMDPs.n_actions\n\n\nPOMDPs.n_observations\n\n\nPOMDPs.n_states\n\n\nPOMDPs.obs_index\n\n\nPOMDPs.observation\n\n\nPOMDPs.observations\n\n\nPOMDPs.pdf\n\n\nPOMDPs.reward\n\n\nPOMDPs.simulate\n\n\nPOMDPs.solve\n\n\nPOMDPs.state_index\n\n\nPOMDPs.states\n\n\nPOMDPs.strip_arg\n\n\nPOMDPs.test_all\n\n\nPOMDPs.transition\n\n\nPOMDPs.update\n\n\nPOMDPs.updater\n\n\nPOMDPs.value\n\n\nPOMDPs.@pomdp_func\n\n\n\n\n\n\nTypes\n\n\n#\n\n\nPOMDPs.POMDP\n \n \nType\n.\n\n\nAbstract base type for a partially observable Markov decision process.\n\n\nS\n:\n \nstate\n \ntype\n\n\nA\n:\n \naction\n \ntype\n\n\nO\n:\n \nobservation\n \ntype\n\n\n\n\n\n\n#\n\n\nPOMDPs.MDP\n \n \nType\n.\n\n\nAbstract base type for a fully observable Markov decision process.\n\n\nS\n:\n \nstate\n \ntype\n\n\nA\n:\n \naction\n \ntype\n\n\n\n\n\n\n#\n\n\nPOMDPs.AbstractSpace\n \n \nType\n.\n\n\nBase type for state, action and observation spaces.\n\n\nT\n:\n \ntype\n \nthat\n \nparametarizes\n \nthe\n \nspace\n \n(\nstate\n,\n \naction\n,\n \nor\n \nobservation\n)\n\n\n\n\n\n\n#\n\n\nPOMDPs.AbstractDistribution\n \n \nType\n.\n\n\nAbstract type for a probability distribution.\n\n\nT\n:\n \ntype\n \nover\n \nwhich\n \ndistribution\n \nis\n \nover\n \n(\nstate\n,\n \naction\n,\n \nor\n \nobservation\n)\n\n\n\n\n\n\n#\n\n\nPOMDPs.Solver\n \n \nType\n.\n\n\nBase type for an MDP/POMDP solver\n\n\n#\n\n\nPOMDPs.Policy\n \n \nType\n.\n\n\nBase type for a policy (a map from every possible belief, or more abstract policy state, to an optimal or suboptimal action)\n\n\nB\n:\n \na\n \nbelief\n \n(\nor\n \npolicy\n \nstate\n)\n \nthat\n \nrepresents\n \nthe\n \nknowledge\n \nan\n \nagent\n \nhas\n \nabout\n \nthe\n \nstate\n \nof\n \nthe\n \nsystem\n\n\n\n\n\n\n#\n\n\nPOMDPs.Updater\n \n \nType\n.\n\n\nAbstract type for an object that defines how the belief should be updated\n\n\nB\n:\n \nbelief\n \ntype\n \nthat\n \nparametarizes\n \nthe\n \nupdater\n\n\n\n\n\n\nA belief is a general construct that represents the knowledge an agent has about the state of the system. This can be a probability distribution, an action observation history or a more general representation. \n\n\n\n\nModel Functions\n\n\n#\n\n\nPOMDPs.states\n \n \nFunction\n.\n\n\nstates{S,A,O}(problem::POMDP{S,A,O}, state::S)\nstates{S,A}(problem::MDP{S,A}, state::S)\n\n\n\n\n\nReturns a subset of the state space reachable from \nstate\n. \n\n\nstates(problem::POMDP)\nstates(problem::MDP)\n\n\n\n\n\nReturns the complete state space of a POMDP. \n\n\n#\n\n\nPOMDPs.actions\n \n \nFunction\n.\n\n\nactions{S,A,O}(problem::POMDP{S,A,O}, state::S, aspace::AbstractSpace{A})\nactions{S,A}(problem::MDP{S,A}, state::S, aspace::AbstractSpace{A})\n\n\n\n\n\nModifies aspace to the action space accessible from the given state and returns it.\n\n\nactions(problem::POMDP)\nactions(problem::MDP)\n\n\n\n\n\nReturns the entire action space of a POMDP.\n\n\nactions{S,A,O,B}(problem::POMDP{S,A,O}, belief::B, aspace::AbstractSpace{A})\n\n\n\n\n\nModifies aspace to the action space accessible from the states with nonzero belief and returns it.\n\n\n#\n\n\nPOMDPs.observations\n \n \nFunction\n.\n\n\nobservations{S,A,O}(problem::POMDP{S,A,O}, state::S, obs::AbstractSpace{O}=observations(problem))\n\n\n\n\n\nModifies ospace to the observation space accessible from the given state and returns it.\n\n\nobservations(problem::POMDP)\n\n\n\n\n\nReturns the entire observation space.\n\n\n#\n\n\nPOMDPs.reward\n \n \nFunction\n.\n\n\nreward{S,A,O}(problem::POMDP{S,A,O}, state::S, action::A, statep::S)\nreward{S,A}(problem::MDP{S,A}, state::S, action::A, statep::S)\n\n\n\n\n\nReturns the immediate reward for the s-a-s' triple\n\n\nreward{S,A,O}(problem::POMDP{S,A,O}, state::S, action::A)\nreward{S,A}(problem::MDP{S,A}, state::S, action::A)\n\n\n\n\n\nReturns the immediate reward for the s-a pair\n\n\n#\n\n\nPOMDPs.transition\n \n \nFunction\n.\n\n\ntransition{S,A,O}(problem::POMDP{S,A,O}, state::S, action::A,\n\n\n\n\n\ndistribution::AbstractDistribution{S}=create_transition_distribution(problem))     transition{S,A}(problem::MDP{S,A}, state::S, action::A, distribution::AbstractDistribution{S}=create_transition_distribution(problem))\n\n\nReturns the transition distribution from the current state-action pair\n\n\n#\n\n\nPOMDPs.observation\n \n \nFunction\n.\n\n\nobservation{S,A,O}(problem::POMDP{S,A,O}, state::S, action::A, statep::S, distribution::AbstractDistribution{O}=create_observation_distribution(problem))\n\n\n\n\n\nReturns the observation distribution for the s-a-s' tuple (state, action, and next state)\n\n\nobservation{S,A,O}(problem::POMDP{S,A,O}, action::A, statep::S, distribution::AbstractDistribution{O}=create_observation_distribution(problem))\n\n\n\n\n\nModifies distribution to the observation distribution for the a-s' tuple (action and next state) and returns it\n\n\n#\n\n\nPOMDPs.isterminal\n \n \nFunction\n.\n\n\nisterminal{S,A,O}(problem::POMDP{S,A,O}, state::S)\nisterminal{S,A}(problem::MDP{S,A}, state::S)\n\n\n\n\n\nChecks if state s is terminal\n\n\n#\n\n\nPOMDPs.isterminal_obs\n \n \nFunction\n.\n\n\nisterminal_obs{S,A,O}(problem::POMDP{S,A,O}, observation::O)\n\n\n\n\n\nChecks if an observation is terminal.\n\n\n#\n\n\nPOMDPs.discount\n \n \nFunction\n.\n\n\ndiscount(problem::POMDP)\ndiscount(problem::MDP)\n\n\n\n\n\nReturn the discount factor for the problem.\n\n\n#\n\n\nPOMDPs.n_states\n \n \nFunction\n.\n\n\nn_states(problem::POMDP)\nn_states(problem::MDP)\n\n\n\n\n\nReturns the number of states in \nproblem\n. Used for discrete models only.\n\n\n#\n\n\nPOMDPs.n_actions\n \n \nFunction\n.\n\n\nn_actions(problem::POMDP)\nn_actions(problem::MDP)\n\n\n\n\n\nReturns the number of actions in \nproblem\n. Used for discrete models only.\n\n\n#\n\n\nPOMDPs.n_observations\n \n \nFunction\n.\n\n\nn_observations(problem::POMDP)\n\n\n\n\n\nReturns the number of actions in \nproblem\n. Used for discrete models only.\n\n\n#\n\n\nPOMDPs.state_index\n \n \nFunction\n.\n\n\nstate_index{S,A,O}(problem::POMDP{S,A,O}, s::S)\nstate_index{S,A}(problem::MDP{S,A}, s::S)\n\n\n\n\n\nReturns the integer index of state \ns\n. Used for discrete models only.\n\n\n#\n\n\nPOMDPs.action_index\n \n \nFunction\n.\n\n\naction_index{S,A,O}(problem::POMDP{S,A,O}, a::A)\naction_index{S,A}(problem::MDP{S,A}, a::A)\n\n\n\n\n\nReturns the integer index of action \na\n. Used for discrete models only.\n\n\n#\n\n\nPOMDPs.obs_index\n \n \nFunction\n.\n\n\nobs_index{S,A,O}(problem::POMDP{S,A,O}, o::O)\n\n\n\n\n\nReturns the integer index of observation \no\n. Used for discrete models only.\n\n\n#\n\n\nPOMDPs.create_state\n \n \nFunction\n.\n\n\ncreate_state(problem::POMDP)\ncreate_state(problem::MDP)\n\n\n\n\n\nCreate a state object (for preallocation purposes).\n\n\n#\n\n\nPOMDPs.create_action\n \n \nFunction\n.\n\n\ncreate_action(problem::POMDP)\ncreate_action(problem::MDP)\n\n\n\n\n\nCreates an action object (for preallocation purposes)\n\n\n#\n\n\nPOMDPs.create_observation\n \n \nFunction\n.\n\n\ncreate_observation(problem::POMDP)\n\n\n\n\n\nCreate an observation object (for preallocation purposes).\n\n\n\n\nDistribution/Space Functions\n\n\n#\n\n\nBase.Random.rand\n \n \nFunction\n.\n\n\nrand{T}(rng::AbstractRNG, d::AbstractSpace{T}, sample::T)\n\n\n\n\n\nReturns a random \nsample\n from space \ns\n.\n\n\nrand{T}(rng::AbstractRNG, d::AbstractDistribution{T}, sample::T)\n\n\n\n\n\nFill \nsample\n with a random element from distribution \nd\n. The sample can be a state, action or observation.\n\n\n#\n\n\nPOMDPs.pdf\n \n \nFunction\n.\n\n\npdf{T}(d::AbstractDistribution{T}, x::T)\n\n\n\n\n\nValue of probability distribution \nd\n function at sample \nx\n.\n\n\n#\n\n\nPOMDPs.dimensions\n \n \nFunction\n.\n\n\ndimensions{T}(s::AbstractSpace{T})\n\n\n\n\n\nReturns the number of dimensions in space \ns\n.\n\n\n#\n\n\nPOMDPs.iterator\n \n \nFunction\n.\n\n\niterator{T}(s::AbstractSpace{T})\n\n\n\n\n\nReturns an iterable type (array or custom iterator) corresponding to space \ns\n. \n\n\niterator{T}(d::AbstractDistribution{T})\n\n\n\n\n\nReturns an iterable type (array or custom iterator) corresponding to distribution \nd\n. \n\n\n#\n\n\nPOMDPs.initial_state_distribution\n \n \nFunction\n.\n\n\ninitial_state_distribution(pomdp::POMDP)\n\n\n\n\n\nReturns an initial belief for the pomdp.\n\n\n#\n\n\nPOMDPs.create_transition_distribution\n \n \nFunction\n.\n\n\ncreate_transition_distribution(problem::POMDP)\ncreate_transition_distribution(problem::MDP)\n\n\n\n\n\nReturns a transition distribution (for memory preallocation).\n\n\n#\n\n\nPOMDPs.create_observation_distribution\n \n \nFunction\n.\n\n\ncreate_observation_distribution(problem::POMDP)\ncreate_observation_distribution(problem::MDP)\n\n\n\n\n\nReturns an observation distribution (for memory preallocation).\n\n\n\n\nBelief Functions\n\n\n#\n\n\nPOMDPs.update\n \n \nFunction\n.\n\n\nupdate{B,A,O}(updater::Updater, belief_old::B, action::A, obs::O,\nbelief_new::B=create_belief(updater))\n\n\n\n\n\nReturns a new instance of an updated belief given \nbelief_old\n and the latest action and observation.\n\n\n#\n\n\nPOMDPs.create_belief\n \n \nFunction\n.\n\n\ncreate_belief(updater::Updater)\n\n\n\n\n\nCreates a belief object of the type used by \nupdater\n (preallocates memory)\n\n\ncreate_belief(pomdp::POMDP)\n\n\n\n\n\nCreates a belief either to be used by updater or pomdp\n\n\n#\n\n\nPOMDPs.initialize_belief\n \n \nFunction\n.\n\n\ninitialize_belief{B}(updater::Updater{B}, \n                     state_distribution::AbstractDistribution,\n                     new_belief::B=create_belief(updater))\ninitialize_belief{B}(updater::Updater{B},\n                     belief::Any,\n                     new_belief::B=create_belief(updater))\n\n\n\n\n\nReturns a belief that can be updated using \nupdater\n that has similar distribution to \nstate_distribution\n or \nbelief\n.\n\n\nThe conversion may be lossy. This function is also idempotent, i.e. there is a default implementation that passes the belief through when it is already the correct type: \ninitialize_belief{B}(updater::Updater{B}, belief::B) = belief\n\n\n\n\nPolicy and Solver Functions\n\n\n#\n\n\nPOMDPs.create_policy\n \n \nFunction\n.\n\n\ncreate_policy(solver::Solver, problem::POMDP)\ncreate_policy(solver::Solver, problem::MDP)\n\n\n\n\n\nCreates a policy object (for preallocation purposes)\n\n\n#\n\n\nPOMDPs.solve\n \n \nFunction\n.\n\n\nsolve(solver::Solver, problem::POMDP, policy=create_policy(solver, problem))\n\n\n\n\n\nSolves the POMDP using method associated with solver, and returns a policy. \n\n\n#\n\n\nPOMDPs.updater\n \n \nFunction\n.\n\n\nupdater(policy::Policy)\n\n\n\n\n\nReturns a default Updater appropriate for a belief type that policy \np\n can use\n\n\n#\n\n\nPOMDPs.action\n \n \nFunction\n.\n\n\naction{B}(p::Policy, x::B, action)\n\n\n\n\n\nFills and returns action based on the current state or belief, given the policy. B is a generalized information state - can be a state in an MDP, a distribution in POMDP, or any other representation needed to make a decision using the given policy. \n\n\naction{B}(policy::Policy, x::B)\n\n\n\n\n\nReturns an action for the current state or belief, given the policy\n\n\nIf an MDP is being simulated, x will be a state; if a POMDP is being simulated, x will be a belief\n\n\n#\n\n\nPOMDPs.value\n \n \nFunction\n.\n\n\nvalue{B}(p::Policy, x::B)\n\n\n\n\n\nReturns the utility value from policy p given the state\n\n\n\n\nSimulator\n\n\n#\n\n\nPOMDPs.Simulator\n \n \nType\n.\n\n\nBase type for an object defining how a simulation should be carried out\n\n\n#\n\n\nPOMDPs.simulate\n \n \nFunction\n.\n\n\nsimulate{S,A}(simulator::Simulator, problem::MDP{S,A}, policy::Policy, initial_state::S)\n\n\n\n\n\nRun a simulation using the specified policy and returns the accumulated reward\n\n\nsimulate{S,A,O,B}(simulator::Simulator, problem::POMDP{S,A,O}, policy::Policy{B}, updater::Updater{B}, initial_belief::Union{B,AbstractDistribution{S}})\n\n\n\n\n\nRun a simulation using the specified policy and returns the accumulated reward\n\n\n\n\nUtility Tools\n\n\n#\n\n\nPOMDPs.add\n \n \nFunction\n.\n\n\nadd(solver_name::AbstractString, v::Bool=true)\n\n\n\n\n\nDownloads and installs a registered solver with name \nsolver_name\n.  \nv\n is a verbose flag, when set to true, function will notify the user if solver is already installed. This function is not exported, and must be called:\n\n\njulia\n \nusing\n \nPOMDPs\n\n\njulia\n \nPOMDPs\n.\nadd\n(\nMCTS\n)\n\n\n\n\n\n\n#\n\n\nPOMDPs.add_all\n \n \nFunction\n.\n\n\nadd_all()\n\n\n\n\n\nDownloads and installs all the packages supported by JuliaPOMDP\n\n\n#\n\n\nPOMDPs.test_all\n \n \nFunction\n.\n\n\ntest_all()\n\n\n\n\n\nTests all the JuliaPOMDP packages installed on your current machine.\n\n\n#\n\n\nPOMDPs.available\n \n \nFunction\n.\n\n\navailable()\n\n\n\n\n\nPrints all the availiable packages in JuliaPOMDP\n\n\n#\n\n\nPOMDPs.@pomdp_func\n \n \nMacro\n.\n\n\nProvide a default function implementation that throws an error when called.\n\n\n#\n\n\nPOMDPs.strip_arg\n \n \nFunction\n.\n\n\nStrip anything extra (type annotations, default values, etc) from an argument.\n\n\nFor now this cannot handle keyword arguments (it will throw an error).\n\n\n\n\nConstants\n\n\n#\n\n\nPOMDPs.REMOTE_URL\n \n \nConstant\n.\n\n\nurl to remote JuliaPOMDP organization repo\n\n\n#\n\n\nPOMDPs.SUPPORTED_PACKAGES\n \n \nConstant\n.\n\n\nSet containing string names of officially supported solvers and utility packages (e.g. \nMCTS\n, \nSARSOP\n, \nPOMDPToolbox\n, etc).  If you have a validated solver that supports the POMDPs.jl API, contact the developers to add your solver to this list.", 
            "title": "API"
        }, 
        {
            "location": "/api/#api-documentation", 
            "text": "Documentation for the  POMDPs.jl  user interface. You can get help for any type or function in the module by typing  ?  in the Julia REPL followed by the name of type or function. For example:  julia using   POMDPs  julia ?  help ? reward  search :   reward \n\n   reward { S , A , O }( pomdp :: POMDP { S , A , O },   state :: S ,   action :: A ,   statep :: S ) \n\n   Returns   the   immediate   reward   for   the   s - a - s   triple \n\n   reward { S , A , O }( pomdp :: POMDP { S , A , O },   state :: S ,   action :: A ) \n\n   Returns   the   immediate   reward   for   the   s - a   pair", 
            "title": "API Documentation"
        }, 
        {
            "location": "/api/#contents", 
            "text": "API Documentation  Contents  Index  Types  Model Functions  Distribution/Space Functions  Belief Functions  Policy and Solver Functions  Simulator  Utility Tools  Constants", 
            "title": "Contents"
        }, 
        {
            "location": "/api/#index", 
            "text": "POMDPs.REMOTE_URL  POMDPs.SUPPORTED_PACKAGES  POMDPs.AbstractDistribution  POMDPs.AbstractSpace  POMDPs.MDP  POMDPs.POMDP  POMDPs.Policy  POMDPs.Simulator  POMDPs.Solver  POMDPs.Updater  Base.Random.rand  POMDPs.action  POMDPs.action_index  POMDPs.actions  POMDPs.add  POMDPs.add_all  POMDPs.available  POMDPs.create_action  POMDPs.create_belief  POMDPs.create_observation  POMDPs.create_observation_distribution  POMDPs.create_policy  POMDPs.create_state  POMDPs.create_transition_distribution  POMDPs.dimensions  POMDPs.discount  POMDPs.initial_state_distribution  POMDPs.initialize_belief  POMDPs.isterminal  POMDPs.isterminal_obs  POMDPs.iterator  POMDPs.n_actions  POMDPs.n_observations  POMDPs.n_states  POMDPs.obs_index  POMDPs.observation  POMDPs.observations  POMDPs.pdf  POMDPs.reward  POMDPs.simulate  POMDPs.solve  POMDPs.state_index  POMDPs.states  POMDPs.strip_arg  POMDPs.test_all  POMDPs.transition  POMDPs.update  POMDPs.updater  POMDPs.value  POMDPs.@pomdp_func", 
            "title": "Index"
        }, 
        {
            "location": "/api/#types", 
            "text": "#  POMDPs.POMDP     Type .  Abstract base type for a partially observable Markov decision process.  S :   state   type  A :   action   type  O :   observation   type   #  POMDPs.MDP     Type .  Abstract base type for a fully observable Markov decision process.  S :   state   type  A :   action   type   #  POMDPs.AbstractSpace     Type .  Base type for state, action and observation spaces.  T :   type   that   parametarizes   the   space   ( state ,   action ,   or   observation )   #  POMDPs.AbstractDistribution     Type .  Abstract type for a probability distribution.  T :   type   over   which   distribution   is   over   ( state ,   action ,   or   observation )   #  POMDPs.Solver     Type .  Base type for an MDP/POMDP solver  #  POMDPs.Policy     Type .  Base type for a policy (a map from every possible belief, or more abstract policy state, to an optimal or suboptimal action)  B :   a   belief   ( or   policy   state )   that   represents   the   knowledge   an   agent   has   about   the   state   of   the   system   #  POMDPs.Updater     Type .  Abstract type for an object that defines how the belief should be updated  B :   belief   type   that   parametarizes   the   updater   A belief is a general construct that represents the knowledge an agent has about the state of the system. This can be a probability distribution, an action observation history or a more general representation.", 
            "title": "Types"
        }, 
        {
            "location": "/api/#model-functions", 
            "text": "#  POMDPs.states     Function .  states{S,A,O}(problem::POMDP{S,A,O}, state::S)\nstates{S,A}(problem::MDP{S,A}, state::S)  Returns a subset of the state space reachable from  state .   states(problem::POMDP)\nstates(problem::MDP)  Returns the complete state space of a POMDP.   #  POMDPs.actions     Function .  actions{S,A,O}(problem::POMDP{S,A,O}, state::S, aspace::AbstractSpace{A})\nactions{S,A}(problem::MDP{S,A}, state::S, aspace::AbstractSpace{A})  Modifies aspace to the action space accessible from the given state and returns it.  actions(problem::POMDP)\nactions(problem::MDP)  Returns the entire action space of a POMDP.  actions{S,A,O,B}(problem::POMDP{S,A,O}, belief::B, aspace::AbstractSpace{A})  Modifies aspace to the action space accessible from the states with nonzero belief and returns it.  #  POMDPs.observations     Function .  observations{S,A,O}(problem::POMDP{S,A,O}, state::S, obs::AbstractSpace{O}=observations(problem))  Modifies ospace to the observation space accessible from the given state and returns it.  observations(problem::POMDP)  Returns the entire observation space.  #  POMDPs.reward     Function .  reward{S,A,O}(problem::POMDP{S,A,O}, state::S, action::A, statep::S)\nreward{S,A}(problem::MDP{S,A}, state::S, action::A, statep::S)  Returns the immediate reward for the s-a-s' triple  reward{S,A,O}(problem::POMDP{S,A,O}, state::S, action::A)\nreward{S,A}(problem::MDP{S,A}, state::S, action::A)  Returns the immediate reward for the s-a pair  #  POMDPs.transition     Function .  transition{S,A,O}(problem::POMDP{S,A,O}, state::S, action::A,  distribution::AbstractDistribution{S}=create_transition_distribution(problem))     transition{S,A}(problem::MDP{S,A}, state::S, action::A, distribution::AbstractDistribution{S}=create_transition_distribution(problem))  Returns the transition distribution from the current state-action pair  #  POMDPs.observation     Function .  observation{S,A,O}(problem::POMDP{S,A,O}, state::S, action::A, statep::S, distribution::AbstractDistribution{O}=create_observation_distribution(problem))  Returns the observation distribution for the s-a-s' tuple (state, action, and next state)  observation{S,A,O}(problem::POMDP{S,A,O}, action::A, statep::S, distribution::AbstractDistribution{O}=create_observation_distribution(problem))  Modifies distribution to the observation distribution for the a-s' tuple (action and next state) and returns it  #  POMDPs.isterminal     Function .  isterminal{S,A,O}(problem::POMDP{S,A,O}, state::S)\nisterminal{S,A}(problem::MDP{S,A}, state::S)  Checks if state s is terminal  #  POMDPs.isterminal_obs     Function .  isterminal_obs{S,A,O}(problem::POMDP{S,A,O}, observation::O)  Checks if an observation is terminal.  #  POMDPs.discount     Function .  discount(problem::POMDP)\ndiscount(problem::MDP)  Return the discount factor for the problem.  #  POMDPs.n_states     Function .  n_states(problem::POMDP)\nn_states(problem::MDP)  Returns the number of states in  problem . Used for discrete models only.  #  POMDPs.n_actions     Function .  n_actions(problem::POMDP)\nn_actions(problem::MDP)  Returns the number of actions in  problem . Used for discrete models only.  #  POMDPs.n_observations     Function .  n_observations(problem::POMDP)  Returns the number of actions in  problem . Used for discrete models only.  #  POMDPs.state_index     Function .  state_index{S,A,O}(problem::POMDP{S,A,O}, s::S)\nstate_index{S,A}(problem::MDP{S,A}, s::S)  Returns the integer index of state  s . Used for discrete models only.  #  POMDPs.action_index     Function .  action_index{S,A,O}(problem::POMDP{S,A,O}, a::A)\naction_index{S,A}(problem::MDP{S,A}, a::A)  Returns the integer index of action  a . Used for discrete models only.  #  POMDPs.obs_index     Function .  obs_index{S,A,O}(problem::POMDP{S,A,O}, o::O)  Returns the integer index of observation  o . Used for discrete models only.  #  POMDPs.create_state     Function .  create_state(problem::POMDP)\ncreate_state(problem::MDP)  Create a state object (for preallocation purposes).  #  POMDPs.create_action     Function .  create_action(problem::POMDP)\ncreate_action(problem::MDP)  Creates an action object (for preallocation purposes)  #  POMDPs.create_observation     Function .  create_observation(problem::POMDP)  Create an observation object (for preallocation purposes).", 
            "title": "Model Functions"
        }, 
        {
            "location": "/api/#distributionspace-functions", 
            "text": "#  Base.Random.rand     Function .  rand{T}(rng::AbstractRNG, d::AbstractSpace{T}, sample::T)  Returns a random  sample  from space  s .  rand{T}(rng::AbstractRNG, d::AbstractDistribution{T}, sample::T)  Fill  sample  with a random element from distribution  d . The sample can be a state, action or observation.  #  POMDPs.pdf     Function .  pdf{T}(d::AbstractDistribution{T}, x::T)  Value of probability distribution  d  function at sample  x .  #  POMDPs.dimensions     Function .  dimensions{T}(s::AbstractSpace{T})  Returns the number of dimensions in space  s .  #  POMDPs.iterator     Function .  iterator{T}(s::AbstractSpace{T})  Returns an iterable type (array or custom iterator) corresponding to space  s .   iterator{T}(d::AbstractDistribution{T})  Returns an iterable type (array or custom iterator) corresponding to distribution  d .   #  POMDPs.initial_state_distribution     Function .  initial_state_distribution(pomdp::POMDP)  Returns an initial belief for the pomdp.  #  POMDPs.create_transition_distribution     Function .  create_transition_distribution(problem::POMDP)\ncreate_transition_distribution(problem::MDP)  Returns a transition distribution (for memory preallocation).  #  POMDPs.create_observation_distribution     Function .  create_observation_distribution(problem::POMDP)\ncreate_observation_distribution(problem::MDP)  Returns an observation distribution (for memory preallocation).", 
            "title": "Distribution/Space Functions"
        }, 
        {
            "location": "/api/#belief-functions", 
            "text": "#  POMDPs.update     Function .  update{B,A,O}(updater::Updater, belief_old::B, action::A, obs::O,\nbelief_new::B=create_belief(updater))  Returns a new instance of an updated belief given  belief_old  and the latest action and observation.  #  POMDPs.create_belief     Function .  create_belief(updater::Updater)  Creates a belief object of the type used by  updater  (preallocates memory)  create_belief(pomdp::POMDP)  Creates a belief either to be used by updater or pomdp  #  POMDPs.initialize_belief     Function .  initialize_belief{B}(updater::Updater{B}, \n                     state_distribution::AbstractDistribution,\n                     new_belief::B=create_belief(updater))\ninitialize_belief{B}(updater::Updater{B},\n                     belief::Any,\n                     new_belief::B=create_belief(updater))  Returns a belief that can be updated using  updater  that has similar distribution to  state_distribution  or  belief .  The conversion may be lossy. This function is also idempotent, i.e. there is a default implementation that passes the belief through when it is already the correct type:  initialize_belief{B}(updater::Updater{B}, belief::B) = belief", 
            "title": "Belief Functions"
        }, 
        {
            "location": "/api/#policy-and-solver-functions", 
            "text": "#  POMDPs.create_policy     Function .  create_policy(solver::Solver, problem::POMDP)\ncreate_policy(solver::Solver, problem::MDP)  Creates a policy object (for preallocation purposes)  #  POMDPs.solve     Function .  solve(solver::Solver, problem::POMDP, policy=create_policy(solver, problem))  Solves the POMDP using method associated with solver, and returns a policy.   #  POMDPs.updater     Function .  updater(policy::Policy)  Returns a default Updater appropriate for a belief type that policy  p  can use  #  POMDPs.action     Function .  action{B}(p::Policy, x::B, action)  Fills and returns action based on the current state or belief, given the policy. B is a generalized information state - can be a state in an MDP, a distribution in POMDP, or any other representation needed to make a decision using the given policy.   action{B}(policy::Policy, x::B)  Returns an action for the current state or belief, given the policy  If an MDP is being simulated, x will be a state; if a POMDP is being simulated, x will be a belief  #  POMDPs.value     Function .  value{B}(p::Policy, x::B)  Returns the utility value from policy p given the state", 
            "title": "Policy and Solver Functions"
        }, 
        {
            "location": "/api/#simulator", 
            "text": "#  POMDPs.Simulator     Type .  Base type for an object defining how a simulation should be carried out  #  POMDPs.simulate     Function .  simulate{S,A}(simulator::Simulator, problem::MDP{S,A}, policy::Policy, initial_state::S)  Run a simulation using the specified policy and returns the accumulated reward  simulate{S,A,O,B}(simulator::Simulator, problem::POMDP{S,A,O}, policy::Policy{B}, updater::Updater{B}, initial_belief::Union{B,AbstractDistribution{S}})  Run a simulation using the specified policy and returns the accumulated reward", 
            "title": "Simulator"
        }, 
        {
            "location": "/api/#utility-tools", 
            "text": "#  POMDPs.add     Function .  add(solver_name::AbstractString, v::Bool=true)  Downloads and installs a registered solver with name  solver_name .   v  is a verbose flag, when set to true, function will notify the user if solver is already installed. This function is not exported, and must be called:  julia   using   POMDPs  julia   POMDPs . add ( MCTS )   #  POMDPs.add_all     Function .  add_all()  Downloads and installs all the packages supported by JuliaPOMDP  #  POMDPs.test_all     Function .  test_all()  Tests all the JuliaPOMDP packages installed on your current machine.  #  POMDPs.available     Function .  available()  Prints all the availiable packages in JuliaPOMDP  #  POMDPs.@pomdp_func     Macro .  Provide a default function implementation that throws an error when called.  #  POMDPs.strip_arg     Function .  Strip anything extra (type annotations, default values, etc) from an argument.  For now this cannot handle keyword arguments (it will throw an error).", 
            "title": "Utility Tools"
        }, 
        {
            "location": "/api/#constants", 
            "text": "#  POMDPs.REMOTE_URL     Constant .  url to remote JuliaPOMDP organization repo  #  POMDPs.SUPPORTED_PACKAGES     Constant .  Set containing string names of officially supported solvers and utility packages (e.g.  MCTS ,  SARSOP ,  POMDPToolbox , etc).  If you have a validated solver that supports the POMDPs.jl API, contact the developers to add your solver to this list.", 
            "title": "Constants"
        }, 
        {
            "location": "/faq/", 
            "text": "Frequently Asked Questions (FAQ)", 
            "title": "Frequently Asked Questions"
        }, 
        {
            "location": "/faq/#frequently-asked-questions-faq", 
            "text": "", 
            "title": "Frequently Asked Questions (FAQ)"
        }
    ]
}