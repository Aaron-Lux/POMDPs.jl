{
    "docs": [
        {
            "location": "/", 
            "text": "POMDPs\n\n\nA Julia interface for defining, solving and simulating partially observable Markov decision processes and their fully observable counterparts.\n\n\n\n\nPackage Features\n\n\n\n\nGeneral interface that can handle problems with discrete and continuous state/action/observation spaces\n\n\nA number of popular state-of-the-art solvers availiable to use out of the box\n\n\nTools that make it easy to define problems and simulate solutions \n\n\nSimple integration of custom solvers into the existing interface\n\n\n\n\n\n\nAvailible Solvers\n\n\nThe POMDPs.jl package contains only an interface to use for expressing and solving POMDPs. Solvers are contained in external packages that can be downloaded using \nPOMDPs.add\n.\n\n\nThe following MDP solvers support this interface:\n\n\n\n\nValue Iteration\n\n\nMonte Carlo Tree Search\n\n\n\n\nThe following POMDP solvers support this interface:\n\n\n\n\nQMDP\n\n\nSARSOP\n\n\nPOMCP\n\n\nPOMDPSolve\n\n\n\n\n\n\nManual Outline\n\n\n\n\nAPI Documentation\n\n\nContents\n\n\nIndex\n\n\nTypes\n\n\nModel Functions\n\n\nDistribution/Space Functions\n\n\nBelief Functions\n\n\nPolicy and Solver Functions\n\n\nSimulator\n\n\nUtility Tools\n\n\nConstants\n\n\n\n\n\n\nPackage Guide\n\n\nInstallation\n\n\nUsage\n\n\nExample Simulation\n\n\n\n\n\n\nPOMDPs\n\n\nPackage Features\n\n\nAvailible Solvers\n\n\nManual Outline", 
            "title": "Home"
        }, 
        {
            "location": "/#pomdps", 
            "text": "A Julia interface for defining, solving and simulating partially observable Markov decision processes and their fully observable counterparts.", 
            "title": "POMDPs"
        }, 
        {
            "location": "/#package-features", 
            "text": "General interface that can handle problems with discrete and continuous state/action/observation spaces  A number of popular state-of-the-art solvers availiable to use out of the box  Tools that make it easy to define problems and simulate solutions   Simple integration of custom solvers into the existing interface", 
            "title": "Package Features"
        }, 
        {
            "location": "/#availible-solvers", 
            "text": "The POMDPs.jl package contains only an interface to use for expressing and solving POMDPs. Solvers are contained in external packages that can be downloaded using  POMDPs.add .  The following MDP solvers support this interface:   Value Iteration  Monte Carlo Tree Search   The following POMDP solvers support this interface:   QMDP  SARSOP  POMCP  POMDPSolve", 
            "title": "Availible Solvers"
        }, 
        {
            "location": "/#manual-outline", 
            "text": "API Documentation  Contents  Index  Types  Model Functions  Distribution/Space Functions  Belief Functions  Policy and Solver Functions  Simulator  Utility Tools  Constants    Package Guide  Installation  Usage  Example Simulation    POMDPs  Package Features  Availible Solvers  Manual Outline", 
            "title": "Manual Outline"
        }, 
        {
            "location": "/guide/", 
            "text": "Package Guide\n\n\n\n\nInstallation\n\n\nThe package can be installed by cloning the code from the github repository \nPOMDPs.jl\n\n\nInstallation with POMDPs.jl:\n\n\nPkg\n.\nclone\n(\nhttps://github.com/JuliaPOMDP/POMDPs.jl.git\n)\n\n\n\n\n\n\nThe package is currently not registered in meta-data. \n\n\n\n\nUsage\n\n\nPOMDPs serves as the interface used by a number of packages under the \nJuliaPOMDP\n framework. It is essentially the agreed upon API used by all the other packages in JuliaPOMDP. If you are using this framework, you may be trying to accomplish one or more of the following three goals:\n\n\n\n\nSolve a decision or planning problem with stochastic dynamics (MDP) or partial observability (POMDP)\n\n\nEvaluate a solution in simulation\n\n\nTest your custom algorithm for solving MDPs or POMDPs against other state-of-the-art algorithms\n\n\n\n\nIf you are attempting to complete the first two goals, take a look at these Jupyer Notebook tutorials:\n\n\n\n\nMDP Tutorial\n for beginners gives an overview of using Value Iteration and Monte-Carlo Tree Search with the classic grid world problem\n\n\nPOMDP Tutorial\n gives an overview of using SARSOP and QMDP to solve the tiger problem\n\n\n\n\nIf you are trying to write your own algorithm for solving MDPs or POMDPs with this interface take a look at the API section of this guide. \n\n\n\n\nExample Simulation\n\n\nThe following code snippets show how some of the most important functions in the interface are to be used together. (Please note that this example is written for clarity and not efficiency; several other simulators are available in the \nJuliaPOMDP/POMDPToolbox.jl\n package.)\n\n\nFirst, here is a definition of a simple simulator showing the use of the important functions \nrand\n, \ninitialize_belief\n, \nisterminal\n, \naction\n, \ntransition\n, \nreward\n, \nobservation\n, \nupdate\n, and \ndiscount\n.\n\n\nusing\n \nPOMDPs\n\n\n\ntype\n ReferenceSimulator\n\n    \nrng\n::\nAbstractRNG\n\n    \nmax_steps\n::\nInt\n\n\nend\n\n\n\nfunction\n simulate\n{\nS\n,\nA\n,\nO\n,\nB\n}(\nsim\n::\nReferenceSimulator\n,\n\n                           \npomdp\n::\nPOMDP\n{\nS\n,\nA\n,\nO\n},\n\n                           \npolicy\n::\nPolicy\n,\n\n                           \nupdater\n::\nUpdater\n{\nB\n},\n\n                           \ninitial_distribution\n::\nAbstractDistribution\n)\n\n\n    \ns\n \n=\n \nrand\n(\nsim\n.\nrng\n,\n \ninitial_distribution\n)\n\n    \nb\n \n=\n \ninitialize_belief\n(\nupdater\n,\n \ninitial_distribution\n)\n\n\n    \ndisc\n \n=\n \n1.0\n\n    \nr_total\n \n=\n \n0.0\n\n\n    \nstep\n \n=\n \n1\n\n\n    \nwhile\n \n!\nisterminal\n(\npomdp\n,\n \ns\n)\n \n \nstep\n \n=\n \nsim\n.\nmax_steps\n\n        \na\n \n=\n \naction\n(\npolicy\n,\n \nb\n)\n\n\n        \ntrans_dist\n \n=\n \ntransition\n(\npomdp\n,\n \ns\n,\n \na\n)\n\n        \nsp\n \n=\n \nrand\n(\nsim\n.\nrng\n,\n \ntrans_dist\n)\n\n\n        \nr_total\n \n+=\n \ndisc\n*\nreward\n(\npomdp\n,\n \ns\n,\n \na\n,\n \nsp\n)\n\n\n        \nobs_dist\n \n=\n \nobservation\n(\npomdp\n,\n \ns\n,\n \na\n,\n \nsp\n)\n\n        \no\n \n=\n \nrand\n(\nsim\n.\nrng\n,\n \nobs_dist\n)\n\n\n        \nb\n \n=\n \nupdate\n(\nupdater\n,\n \nb\n,\n \na\n,\n \no\n)\n\n\n        \ndisc\n \n*=\n \ndiscount\n(\npomdp\n)\n\n        \ns\n \n=\n \nsp\n\n        \nstep\n \n+=\n \n1\n\n    \nend\n\n\n    \nreturn\n \nr_total\n\n\nend\n\n\n\n\n\n\nThe following snippet shows how a solver should be used to solve a problem and run a simulation.\n\n\nusing\n \nSARSOP\n\n\nusing\n \nPOMDPModels\n\n\n\nsolver\n \n=\n \nSARSOP\n()\n\n\n\nproblem\n \n=\n \nBabyPOMDP\n()\n\n\n\npolicy\n \n=\n \nsolve\n(\nsolver\n,\n \nproblem\n)\n\n\nup\n \n=\n \nupdater\n(\npolicy\n)\n\n\nsim\n \n=\n \nReferenceSimulator\n(\nMersenneTwister\n(\n1\n),\n \n10\n)\n\n\n\nr\n \n=\n \nsimulate\n(\nsim\n,\n \nproblem\n,\n \npolicy\n,\n \nup\n,\n \ninitial_state_distribution\n(\nproblem\n))\n\n\n\nprintln\n(\nReward: \n$r\n)", 
            "title": "Manual"
        }, 
        {
            "location": "/guide/#package-guide", 
            "text": "", 
            "title": "Package Guide"
        }, 
        {
            "location": "/guide/#installation", 
            "text": "The package can be installed by cloning the code from the github repository  POMDPs.jl  Installation with POMDPs.jl:  Pkg . clone ( https://github.com/JuliaPOMDP/POMDPs.jl.git )   The package is currently not registered in meta-data.", 
            "title": "Installation"
        }, 
        {
            "location": "/guide/#usage", 
            "text": "POMDPs serves as the interface used by a number of packages under the  JuliaPOMDP  framework. It is essentially the agreed upon API used by all the other packages in JuliaPOMDP. If you are using this framework, you may be trying to accomplish one or more of the following three goals:   Solve a decision or planning problem with stochastic dynamics (MDP) or partial observability (POMDP)  Evaluate a solution in simulation  Test your custom algorithm for solving MDPs or POMDPs against other state-of-the-art algorithms   If you are attempting to complete the first two goals, take a look at these Jupyer Notebook tutorials:   MDP Tutorial  for beginners gives an overview of using Value Iteration and Monte-Carlo Tree Search with the classic grid world problem  POMDP Tutorial  gives an overview of using SARSOP and QMDP to solve the tiger problem   If you are trying to write your own algorithm for solving MDPs or POMDPs with this interface take a look at the API section of this guide.", 
            "title": "Usage"
        }, 
        {
            "location": "/guide/#example-simulation", 
            "text": "The following code snippets show how some of the most important functions in the interface are to be used together. (Please note that this example is written for clarity and not efficiency; several other simulators are available in the  JuliaPOMDP/POMDPToolbox.jl  package.)  First, here is a definition of a simple simulator showing the use of the important functions  rand ,  initialize_belief ,  isterminal ,  action ,  transition ,  reward ,  observation ,  update , and  discount .  using   POMDPs  type  ReferenceSimulator \n     rng :: AbstractRNG \n     max_steps :: Int  end  function  simulate { S , A , O , B }( sim :: ReferenceSimulator , \n                            pomdp :: POMDP { S , A , O }, \n                            policy :: Policy , \n                            updater :: Updater { B }, \n                            initial_distribution :: AbstractDistribution ) \n\n     s   =   rand ( sim . rng ,   initial_distribution ) \n     b   =   initialize_belief ( updater ,   initial_distribution ) \n\n     disc   =   1.0 \n     r_total   =   0.0 \n\n     step   =   1 \n\n     while   ! isterminal ( pomdp ,   s )     step   =   sim . max_steps \n         a   =   action ( policy ,   b ) \n\n         trans_dist   =   transition ( pomdp ,   s ,   a ) \n         sp   =   rand ( sim . rng ,   trans_dist ) \n\n         r_total   +=   disc * reward ( pomdp ,   s ,   a ,   sp ) \n\n         obs_dist   =   observation ( pomdp ,   s ,   a ,   sp ) \n         o   =   rand ( sim . rng ,   obs_dist ) \n\n         b   =   update ( updater ,   b ,   a ,   o ) \n\n         disc   *=   discount ( pomdp ) \n         s   =   sp \n         step   +=   1 \n     end \n\n     return   r_total  end   The following snippet shows how a solver should be used to solve a problem and run a simulation.  using   SARSOP  using   POMDPModels  solver   =   SARSOP ()  problem   =   BabyPOMDP ()  policy   =   solve ( solver ,   problem )  up   =   updater ( policy )  sim   =   ReferenceSimulator ( MersenneTwister ( 1 ),   10 )  r   =   simulate ( sim ,   problem ,   policy ,   up ,   initial_state_distribution ( problem ))  println ( Reward:  $r )", 
            "title": "Example Simulation"
        }, 
        {
            "location": "/api/", 
            "text": "API Documentation\n\n\nDocumentation for the \nPOMDPs.jl\n user interface. You can get help for any type or  function in the module by typing \n?\n in the Julia REPL followed by the name of  type or function. For example:\n\n\njulia\nusing\n \nPOMDPs\n\n\njulia\n?\n\n\nhelp\n?\nreward\n\n\nsearch\n:\n \nreward\n\n\n  \nreward\n{\nS\n,\nA\n,\nO\n}(\npomdp\n::\nPOMDP\n{\nS\n,\nA\n,\nO\n},\n \nstate\n::\nS\n,\n \naction\n::\nA\n,\n \nstatep\n::\nS\n)\n\n\n  \nReturns\n \nthe\n \nimmediate\n \nreward\n \nfor\n \nthe\n \ns\n-\na\n-\ns\n \ntriple\n\n\n  \nreward\n{\nS\n,\nA\n,\nO\n}(\npomdp\n::\nPOMDP\n{\nS\n,\nA\n,\nO\n},\n \nstate\n::\nS\n,\n \naction\n::\nA\n)\n\n\n  \nReturns\n \nthe\n \nimmediate\n \nreward\n \nfor\n \nthe\n \ns\n-\na\n \npair\n\n\n\n\n\n\n\n\nContents\n\n\n\n\nAPI Documentation\n\n\nContents\n\n\nIndex\n\n\nTypes\n\n\nModel Functions\n\n\nDistribution/Space Functions\n\n\nBelief Functions\n\n\nPolicy and Solver Functions\n\n\nSimulator\n\n\nUtility Tools\n\n\nConstants\n\n\n\n\n\n\n\n\n\n\nIndex\n\n\n\n\nBase.Random.rand\n\n\nPOMDPs.@pomdp_func\n\n\nPOMDPs.AbstractDistribution\n\n\nPOMDPs.AbstractSpace\n\n\nPOMDPs.MDP\n\n\nPOMDPs.POMDP\n\n\nPOMDPs.Policy\n\n\nPOMDPs.REMOTE_URL\n\n\nPOMDPs.SUPPORTED_SOLVERS\n\n\nPOMDPs.Simulator\n\n\nPOMDPs.Solver\n\n\nPOMDPs.Updater\n\n\nPOMDPs.action\n\n\nPOMDPs.action_index\n\n\nPOMDPs.actions\n\n\nPOMDPs.add\n\n\nPOMDPs.create_action\n\n\nPOMDPs.create_belief\n\n\nPOMDPs.create_observation\n\n\nPOMDPs.create_observation_distribution\n\n\nPOMDPs.create_policy\n\n\nPOMDPs.create_state\n\n\nPOMDPs.create_transition_distribution\n\n\nPOMDPs.dimensions\n\n\nPOMDPs.discount\n\n\nPOMDPs.initial_state_distribution\n\n\nPOMDPs.initialize_belief\n\n\nPOMDPs.isterminal\n\n\nPOMDPs.isterminal_obs\n\n\nPOMDPs.iterator\n\n\nPOMDPs.n_actions\n\n\nPOMDPs.n_observations\n\n\nPOMDPs.n_states\n\n\nPOMDPs.obs_index\n\n\nPOMDPs.observation\n\n\nPOMDPs.observations\n\n\nPOMDPs.pdf\n\n\nPOMDPs.reward\n\n\nPOMDPs.simulate\n\n\nPOMDPs.solve\n\n\nPOMDPs.state_index\n\n\nPOMDPs.states\n\n\nPOMDPs.strip_arg\n\n\nPOMDPs.transition\n\n\nPOMDPs.update\n\n\nPOMDPs.updater\n\n\nPOMDPs.value\n\n\n\n\n\n\nTypes\n\n\n#\n\n\nPOMDPs.POMDP\n \n \nType\n.\n\n\n\n\nAbstract base type for a partially observable Markov decision process.\n\n\nS\n:\n \nstate\n \ntype\n\n\nA\n:\n \naction\n \ntype\n\n\nO\n:\n \nobservation\n \ntype\n\n\n\n\n\n\n#\n\n\nPOMDPs.MDP\n \n \nType\n.\n\n\n\n\nAbstract base type for a fully observable Markov decision process.\n\n\nS\n:\n \nstate\n \ntype\n\n\nA\n:\n \naction\n \ntype\n\n\n\n\n\n\n#\n\n\nPOMDPs.AbstractSpace\n \n \nType\n.\n\n\n\n\nBase type for state, action and observation spaces.\n\n\nT\n:\n \ntype\n \nthat\n \nparametarizes\n \nthe\n \nspace\n \n(\nstate\n,\n \naction\n,\n \nor\n \nobservation\n)\n\n\n\n\n\n\n#\n\n\nPOMDPs.AbstractDistribution\n \n \nType\n.\n\n\n\n\nAbstract type for a probability distribution.\n\n\nT\n:\n \ntype\n \nover\n \nwhich\n \ndistribution\n \nis\n \nover\n \n(\nstate\n,\n \naction\n,\n \nor\n \nobservation\n)\n\n\n\n\n\n\n#\n\n\nPOMDPs.Solver\n \n \nType\n.\n\n\n\n\nBase type for an MDP/POMDP solver\n\n\n#\n\n\nPOMDPs.Policy\n \n \nType\n.\n\n\n\n\nBase type for a policy (a map from every possible belief, or more abstract policy state, to an optimal or suboptimal action)\n\n\nB\n:\n \na\n \nbelief\n \n(\nor\n \npolicy\n \nstate\n)\n \nthat\n \nrepresents\n \nthe\n \nknowledge\n \nan\n \nagent\n \nhas\n \nabout\n \nthe\n \nstate\n \nof\n \nthe\n \nsystem\n\n\n\n\n\n\n#\n\n\nPOMDPs.Updater\n \n \nType\n.\n\n\n\n\nAbstract type for an object that defines how the belief should be updated\n\n\nB\n:\n \nbelief\n \ntype\n \nthat\n \nparametarizes\n \nthe\n \nupdater\n\n\n\n\n\n\nA belief is a general construct that represents the knowledge an agent has about the state of the system. This can be a probability distribution, an action observation history or a more general representation. \n\n\n\n\nModel Functions\n\n\n#\n\n\nPOMDPs.states\n \n \nFunction\n.\n\n\n\n\nstates{S,A,O}(problem::POMDP{S,A,O}, state::S)\nstates{S,A}(problem::MDP{S,A}, state::S)\n\n\n\n\n\nReturns a subset of the state space reachable from \nstate\n. \n\n\nstates(problem::POMDP)\nstates(problem::MDP)\n\n\n\n\n\nReturns the complete state space of a POMDP. \n\n\n#\n\n\nPOMDPs.actions\n \n \nFunction\n.\n\n\n\n\nactions{S,A,O}(problem::POMDP{S,A,O}, state::S, aspace::AbstractSpace{A})\nactions{S,A}(problem::MDP{S,A}, state::S, aspace::AbstractSpace{A})\n\n\n\n\n\nModifies aspace to the action space accessible from the given state and returns it.\n\n\nactions(problem::POMDP)\nactions(problem::MDP)\n\n\n\n\n\nReturns the entire action space of a POMDP.\n\n\nactions{S,A,O,B}(problem::POMDP{S,A,O}, belief::B, aspace::AbstractSpace{A})\n\n\n\n\n\nModifies aspace to the action space accessible from the states with nonzero belief and returns it.\n\n\n#\n\n\nPOMDPs.observations\n \n \nFunction\n.\n\n\n\n\nobservations{S,A,O}(problem::POMDP{S,A,O}, state::S, obs::AbstractSpace{O}=observations(problem))\n\n\n\n\n\nModifies ospace to the observation space accessible from the given state and returns it.\n\n\nobservations(problem::POMDP)\n\n\n\n\n\nReturns the entire observation space.\n\n\n#\n\n\nPOMDPs.reward\n \n \nFunction\n.\n\n\n\n\nreward{S,A,O}(problem::POMDP{S,A,O}, state::S, action::A, statep::S)\nreward{S,A}(problem::MDP{S,A}, state::S, action::A, statep::S)\n\n\n\n\n\nReturns the immediate reward for the s-a-s' triple\n\n\nreward{S,A,O}(problem::POMDP{S,A,O}, state::S, action::A)\nreward{S,A}(problem::MDP{S,A}, state::S, action::A)\n\n\n\n\n\nReturns the immediate reward for the s-a pair\n\n\n#\n\n\nPOMDPs.transition\n \n \nFunction\n.\n\n\n\n\ntransition{S,A,O}(problem::POMDP{S,A,O}, state::S, action::A,\n\n\n\n\n\ndistribution::AbstractDistribution{S}=create_transition_distribution(problem))     transition{S,A}(problem::MDP{S,A}, state::S, action::A, distribution::AbstractDistribution{S}=create_transition_distribution(problem))\n\n\nReturns the transition distribution from the current state-action pair\n\n\n#\n\n\nPOMDPs.observation\n \n \nFunction\n.\n\n\n\n\nobservation{S,A,O}(problem::POMDP{S,A,O}, state::S, action::A, statep::S, distribution::AbstractDistribution{O}=create_observation_distribution(problem))\n\n\n\n\n\nReturns the observation distribution for the s-a-s' tuple (state, action, and next state)\n\n\nobservation{S,A,O}(problem::POMDP{S,A,O}, action::A, statep::S, distribution::AbstractDistribution{O}=create_observation_distribution(problem))\n\n\n\n\n\nModifies distribution to the observation distribution for the a-s' tuple (action and next state) and returns it\n\n\n#\n\n\nPOMDPs.isterminal\n \n \nFunction\n.\n\n\n\n\nisterminal{S,A,O}(problem::POMDP{S,A,O}, state::S)\nisterminal{S,A}(problem::MDP{S,A}, state::S)\n\n\n\n\n\nChecks if state s is terminal\n\n\n#\n\n\nPOMDPs.isterminal_obs\n \n \nFunction\n.\n\n\n\n\nisterminal_obs{S,A,O}(problem::POMDP{S,A,O}, observation::O)\n\n\n\n\n\nChecks if an observation is terminal.\n\n\n#\n\n\nPOMDPs.discount\n \n \nFunction\n.\n\n\n\n\ndiscount(problem::POMDP)\ndiscount(problem::MDP)\n\n\n\n\n\nReturn the discount factor for the problem.\n\n\n#\n\n\nPOMDPs.n_states\n \n \nFunction\n.\n\n\n\n\nn_states(problem::POMDP)\nn_states(problem::MDP)\n\n\n\n\n\nReturns the number of states in \nproblem\n. Used for discrete models only.\n\n\n#\n\n\nPOMDPs.n_actions\n \n \nFunction\n.\n\n\n\n\nn_actions(problem::POMDP)\nn_actions(problem::MDP)\n\n\n\n\n\nReturns the number of actions in \nproblem\n. Used for discrete models only.\n\n\n#\n\n\nPOMDPs.n_observations\n \n \nFunction\n.\n\n\n\n\nn_observations(problem::POMDP)\n\n\n\n\n\nReturns the number of actions in \nproblem\n. Used for discrete models only.\n\n\n#\n\n\nPOMDPs.state_index\n \n \nFunction\n.\n\n\n\n\nstate_index{S,A,O}(problem::POMDP{S,A,O}, s::S)\nstate_index{S,A}(problem::MDP{S,A}, s::S)\n\n\n\n\n\nReturns the integer index of state \ns\n. Used for discrete models only.\n\n\n#\n\n\nPOMDPs.action_index\n \n \nFunction\n.\n\n\n\n\naction_index{S,A,O}(problem::POMDP{S,A,O}, a::A)\naction_index{S,A}(problem::MDP{S,A}, a::A)\n\n\n\n\n\nReturns the integer index of action \na\n. Used for discrete models only.\n\n\n#\n\n\nPOMDPs.obs_index\n \n \nFunction\n.\n\n\n\n\nobs_index{S,A,O}(problem::POMDP{S,A,O}, o::O)\n\n\n\n\n\nReturns the integer index of observation \no\n. Used for discrete models only.\n\n\n#\n\n\nPOMDPs.create_state\n \n \nFunction\n.\n\n\n\n\ncreate_state(problem::POMDP)\ncreate_state(problem::MDP)\n\n\n\n\n\nCreate a state object (for preallocation purposes).\n\n\n#\n\n\nPOMDPs.create_action\n \n \nFunction\n.\n\n\n\n\ncreate_action(problem::POMDP)\ncreate_action(problem::MDP)\n\n\n\n\n\nCreates an action object (for preallocation purposes)\n\n\n#\n\n\nPOMDPs.create_observation\n \n \nFunction\n.\n\n\n\n\ncreate_observation(problem::POMDP)\n\n\n\n\n\nCreate an observation object (for preallocation purposes).\n\n\n\n\nDistribution/Space Functions\n\n\n#\n\n\nBase.Random.rand\n \n \nFunction\n.\n\n\n\n\nrand{T}(rng::AbstractRNG, d::AbstractSpace{T}, sample::T)\n\n\n\n\n\nReturns a random \nsample\n from space \ns\n.\n\n\nrand{T}(rng::AbstractRNG, d::AbstractDistribution{T}, sample::T)\n\n\n\n\n\nFill \nsample\n with a random element from distribution \nd\n. The sample can be a state, action or observation.\n\n\n#\n\n\nPOMDPs.pdf\n \n \nFunction\n.\n\n\n\n\npdf{T}(d::AbstractDistribution{T}, x::T)\n\n\n\n\n\nValue of probability distribution \nd\n function at sample \nx\n.\n\n\n#\n\n\nPOMDPs.dimensions\n \n \nFunction\n.\n\n\n\n\ndimensions{T}(s::AbstractSpace{T})\n\n\n\n\n\nReturns the number of dimensions in space \ns\n.\n\n\n#\n\n\nPOMDPs.iterator\n \n \nFunction\n.\n\n\n\n\niterator{T}(s::AbstractSpace{T})\n\n\n\n\n\nReturns an iterable type (array or custom iterator) corresponding to space \ns\n. \n\n\niterator{T}(d::AbstractDistribution{T})\n\n\n\n\n\nReturns an iterable type (array or custom iterator) corresponding to distribution \nd\n. \n\n\n#\n\n\nPOMDPs.initial_state_distribution\n \n \nFunction\n.\n\n\n\n\ninitial_state_distribution(pomdp::POMDP)\n\n\n\n\n\nReturns an initial belief for the pomdp.\n\n\n#\n\n\nPOMDPs.create_transition_distribution\n \n \nFunction\n.\n\n\n\n\ncreate_transition_distribution(problem::POMDP)\ncreate_transition_distribution(problem::MDP)\n\n\n\n\n\nReturns a transition distribution (for memory preallocation).\n\n\n#\n\n\nPOMDPs.create_observation_distribution\n \n \nFunction\n.\n\n\n\n\ncreate_observation_distribution(problem::POMDP)\ncreate_observation_distribution(problem::MDP)\n\n\n\n\n\nReturns an observation distribution (for memory preallocation).\n\n\n\n\nBelief Functions\n\n\n#\n\n\nPOMDPs.update\n \n \nFunction\n.\n\n\n\n\nupdate{B,A,O}(updater::Updater, belief_old::B, action::A, obs::O,\nbelief_new::B=create_belief(updater))\n\n\n\n\n\nReturns a new instance of an updated belief given \nbelief_old\n and the latest action and observation.\n\n\n#\n\n\nPOMDPs.create_belief\n \n \nFunction\n.\n\n\n\n\ncreate_belief(updater::Updater)\n\n\n\n\n\nCreates a belief object of the type used by \nupdater\n (preallocates memory)\n\n\ncreate_belief(pomdp::POMDP)\n\n\n\n\n\nCreates a belief either to be used by updater or pomdp\n\n\n#\n\n\nPOMDPs.initialize_belief\n \n \nFunction\n.\n\n\n\n\ninitialize_belief{B}(updater::Updater{B}, \n                     state_distribution::AbstractDistribution,\n                     new_belief::B=create_belief(updater))\ninitialize_belief{B}(updater::Updater{B},\n                     belief::Any,\n                     new_belief::B=create_belief(updater))\n\n\n\n\n\nReturns a belief that can be updated using \nupdater\n that has similar distribution to \nstate_distribution\n or \nbelief\n.\n\n\nThe conversion may be lossy. This function is also idempotent, i.e. there is a default implementation that passes the belief through when it is already the correct type: \ninitialize_belief{B}(updater::Updater{B}, belief::B) = belief\n\n\n\n\nPolicy and Solver Functions\n\n\n#\n\n\nPOMDPs.create_policy\n \n \nFunction\n.\n\n\n\n\ncreate_policy(solver::Solver, problem::POMDP)\ncreate_policy(solver::Solver, problem::MDP)\n\n\n\n\n\nCreates a policy object (for preallocation purposes)\n\n\n#\n\n\nPOMDPs.solve\n \n \nFunction\n.\n\n\n\n\nsolve(solver::Solver, problem::POMDP, policy=create_policy(solver, problem))\n\n\n\n\n\nSolves the POMDP using method associated with solver, and returns a policy. \n\n\n#\n\n\nPOMDPs.updater\n \n \nFunction\n.\n\n\n\n\nupdater(policy::Policy)\n\n\n\n\n\nReturns a default Updater appropriate for a belief type that policy \np\n can use\n\n\n#\n\n\nPOMDPs.action\n \n \nFunction\n.\n\n\n\n\naction{B}(p::Policy, x::B, action)\n\n\n\n\n\nFills and returns action based on the current state or belief, given the policy. B is a generalized information state - can be a state in an MDP, a distribution in POMDP, or any other representation needed to make a decision using the given policy. \n\n\naction{B}(policy::Policy, x::B)\n\n\n\n\n\nReturns an action for the current state or belief, given the policy\n\n\nIf an MDP is being simulated, x will be a state; if a POMDP is being simulated, x will be a belief\n\n\n#\n\n\nPOMDPs.value\n \n \nFunction\n.\n\n\n\n\nvalue{B}(p::Policy, x::B)\n\n\n\n\n\nReturns the utility value from policy p given the state\n\n\n\n\nSimulator\n\n\n#\n\n\nPOMDPs.Simulator\n \n \nType\n.\n\n\n\n\nBase type for an object defining how a simulation should be carried out\n\n\n#\n\n\nPOMDPs.simulate\n \n \nFunction\n.\n\n\n\n\nsimulate{S,A}(simulator::Simulator, problem::MDP{S,A}, policy::Policy, initial_state::S)\n\n\n\n\n\nRuns a simulation using the specified policy and returns the accumulated reward\n\n\nsimulate{S,A,O,B}(simulator::Simulator, problem::POMDP{S,A,O}, policy::Policy{B}, updater::Updater{B}, initial_belief::Union{B,AbstractDistribution{S}})\n\n\n\n\n\nRuns a simulation using the specified policy and returns the accumulated reward\n\n\n\n\nUtility Tools\n\n\n#\n\n\nPOMDPs.add\n \n \nFunction\n.\n\n\n\n\nadd(solver_name::AbstractString)\n\n\n\n\n\nDownloads and installs a registered solver with name \nsolver_name\n.  This function is not exported, and must be called:\n\n\njulia\n \nusing\n \nPOMDPs\n\n\njulia\n \nPOMDPs\n.\nadd\n(\nMCTS\n)\n\n\n\n\n\n\n#\n\n\nPOMDPs.@pomdp_func\n \n \nMacro\n.\n\n\n\n\nProvide a default function implementation that throws an error when called.\n\n\n#\n\n\nPOMDPs.strip_arg\n \n \nFunction\n.\n\n\n\n\nStrip anything extra (type annotations, default values, etc) from an argument.\n\n\nFor now this cannot handle keyword arguments (it will throw an error).\n\n\n\n\nConstants\n\n\n#\n\n\nPOMDPs.REMOTE_URL\n \n \nConstant\n.\n\n\n\n\nurl to remote JuliaPOMDP organization repo\n\n\n#\n\n\nPOMDPs.SUPPORTED_SOLVERS\n \n \nConstant\n.\n\n\n\n\nSet containing string names of officially supported solvers  (e.g. \nMCTS\n, \nSARSOP\n, etc).  If you have a validated solver that supports the POMDPs.jl API, contact the developers to add your solver to this list.", 
            "title": "API"
        }, 
        {
            "location": "/api/#api-documentation", 
            "text": "Documentation for the  POMDPs.jl  user interface. You can get help for any type or  function in the module by typing  ?  in the Julia REPL followed by the name of  type or function. For example:  julia using   POMDPs  julia ?  help ? reward  search :   reward \n\n   reward { S , A , O }( pomdp :: POMDP { S , A , O },   state :: S ,   action :: A ,   statep :: S ) \n\n   Returns   the   immediate   reward   for   the   s - a - s   triple \n\n   reward { S , A , O }( pomdp :: POMDP { S , A , O },   state :: S ,   action :: A ) \n\n   Returns   the   immediate   reward   for   the   s - a   pair", 
            "title": "API Documentation"
        }, 
        {
            "location": "/api/#contents", 
            "text": "API Documentation  Contents  Index  Types  Model Functions  Distribution/Space Functions  Belief Functions  Policy and Solver Functions  Simulator  Utility Tools  Constants", 
            "title": "Contents"
        }, 
        {
            "location": "/api/#index", 
            "text": "Base.Random.rand  POMDPs.@pomdp_func  POMDPs.AbstractDistribution  POMDPs.AbstractSpace  POMDPs.MDP  POMDPs.POMDP  POMDPs.Policy  POMDPs.REMOTE_URL  POMDPs.SUPPORTED_SOLVERS  POMDPs.Simulator  POMDPs.Solver  POMDPs.Updater  POMDPs.action  POMDPs.action_index  POMDPs.actions  POMDPs.add  POMDPs.create_action  POMDPs.create_belief  POMDPs.create_observation  POMDPs.create_observation_distribution  POMDPs.create_policy  POMDPs.create_state  POMDPs.create_transition_distribution  POMDPs.dimensions  POMDPs.discount  POMDPs.initial_state_distribution  POMDPs.initialize_belief  POMDPs.isterminal  POMDPs.isterminal_obs  POMDPs.iterator  POMDPs.n_actions  POMDPs.n_observations  POMDPs.n_states  POMDPs.obs_index  POMDPs.observation  POMDPs.observations  POMDPs.pdf  POMDPs.reward  POMDPs.simulate  POMDPs.solve  POMDPs.state_index  POMDPs.states  POMDPs.strip_arg  POMDPs.transition  POMDPs.update  POMDPs.updater  POMDPs.value", 
            "title": "Index"
        }, 
        {
            "location": "/api/#types", 
            "text": "#  POMDPs.POMDP     Type .   Abstract base type for a partially observable Markov decision process.  S :   state   type  A :   action   type  O :   observation   type   #  POMDPs.MDP     Type .   Abstract base type for a fully observable Markov decision process.  S :   state   type  A :   action   type   #  POMDPs.AbstractSpace     Type .   Base type for state, action and observation spaces.  T :   type   that   parametarizes   the   space   ( state ,   action ,   or   observation )   #  POMDPs.AbstractDistribution     Type .   Abstract type for a probability distribution.  T :   type   over   which   distribution   is   over   ( state ,   action ,   or   observation )   #  POMDPs.Solver     Type .   Base type for an MDP/POMDP solver  #  POMDPs.Policy     Type .   Base type for a policy (a map from every possible belief, or more abstract policy state, to an optimal or suboptimal action)  B :   a   belief   ( or   policy   state )   that   represents   the   knowledge   an   agent   has   about   the   state   of   the   system   #  POMDPs.Updater     Type .   Abstract type for an object that defines how the belief should be updated  B :   belief   type   that   parametarizes   the   updater   A belief is a general construct that represents the knowledge an agent has about the state of the system. This can be a probability distribution, an action observation history or a more general representation.", 
            "title": "Types"
        }, 
        {
            "location": "/api/#model-functions", 
            "text": "#  POMDPs.states     Function .   states{S,A,O}(problem::POMDP{S,A,O}, state::S)\nstates{S,A}(problem::MDP{S,A}, state::S)  Returns a subset of the state space reachable from  state .   states(problem::POMDP)\nstates(problem::MDP)  Returns the complete state space of a POMDP.   #  POMDPs.actions     Function .   actions{S,A,O}(problem::POMDP{S,A,O}, state::S, aspace::AbstractSpace{A})\nactions{S,A}(problem::MDP{S,A}, state::S, aspace::AbstractSpace{A})  Modifies aspace to the action space accessible from the given state and returns it.  actions(problem::POMDP)\nactions(problem::MDP)  Returns the entire action space of a POMDP.  actions{S,A,O,B}(problem::POMDP{S,A,O}, belief::B, aspace::AbstractSpace{A})  Modifies aspace to the action space accessible from the states with nonzero belief and returns it.  #  POMDPs.observations     Function .   observations{S,A,O}(problem::POMDP{S,A,O}, state::S, obs::AbstractSpace{O}=observations(problem))  Modifies ospace to the observation space accessible from the given state and returns it.  observations(problem::POMDP)  Returns the entire observation space.  #  POMDPs.reward     Function .   reward{S,A,O}(problem::POMDP{S,A,O}, state::S, action::A, statep::S)\nreward{S,A}(problem::MDP{S,A}, state::S, action::A, statep::S)  Returns the immediate reward for the s-a-s' triple  reward{S,A,O}(problem::POMDP{S,A,O}, state::S, action::A)\nreward{S,A}(problem::MDP{S,A}, state::S, action::A)  Returns the immediate reward for the s-a pair  #  POMDPs.transition     Function .   transition{S,A,O}(problem::POMDP{S,A,O}, state::S, action::A,  distribution::AbstractDistribution{S}=create_transition_distribution(problem))     transition{S,A}(problem::MDP{S,A}, state::S, action::A, distribution::AbstractDistribution{S}=create_transition_distribution(problem))  Returns the transition distribution from the current state-action pair  #  POMDPs.observation     Function .   observation{S,A,O}(problem::POMDP{S,A,O}, state::S, action::A, statep::S, distribution::AbstractDistribution{O}=create_observation_distribution(problem))  Returns the observation distribution for the s-a-s' tuple (state, action, and next state)  observation{S,A,O}(problem::POMDP{S,A,O}, action::A, statep::S, distribution::AbstractDistribution{O}=create_observation_distribution(problem))  Modifies distribution to the observation distribution for the a-s' tuple (action and next state) and returns it  #  POMDPs.isterminal     Function .   isterminal{S,A,O}(problem::POMDP{S,A,O}, state::S)\nisterminal{S,A}(problem::MDP{S,A}, state::S)  Checks if state s is terminal  #  POMDPs.isterminal_obs     Function .   isterminal_obs{S,A,O}(problem::POMDP{S,A,O}, observation::O)  Checks if an observation is terminal.  #  POMDPs.discount     Function .   discount(problem::POMDP)\ndiscount(problem::MDP)  Return the discount factor for the problem.  #  POMDPs.n_states     Function .   n_states(problem::POMDP)\nn_states(problem::MDP)  Returns the number of states in  problem . Used for discrete models only.  #  POMDPs.n_actions     Function .   n_actions(problem::POMDP)\nn_actions(problem::MDP)  Returns the number of actions in  problem . Used for discrete models only.  #  POMDPs.n_observations     Function .   n_observations(problem::POMDP)  Returns the number of actions in  problem . Used for discrete models only.  #  POMDPs.state_index     Function .   state_index{S,A,O}(problem::POMDP{S,A,O}, s::S)\nstate_index{S,A}(problem::MDP{S,A}, s::S)  Returns the integer index of state  s . Used for discrete models only.  #  POMDPs.action_index     Function .   action_index{S,A,O}(problem::POMDP{S,A,O}, a::A)\naction_index{S,A}(problem::MDP{S,A}, a::A)  Returns the integer index of action  a . Used for discrete models only.  #  POMDPs.obs_index     Function .   obs_index{S,A,O}(problem::POMDP{S,A,O}, o::O)  Returns the integer index of observation  o . Used for discrete models only.  #  POMDPs.create_state     Function .   create_state(problem::POMDP)\ncreate_state(problem::MDP)  Create a state object (for preallocation purposes).  #  POMDPs.create_action     Function .   create_action(problem::POMDP)\ncreate_action(problem::MDP)  Creates an action object (for preallocation purposes)  #  POMDPs.create_observation     Function .   create_observation(problem::POMDP)  Create an observation object (for preallocation purposes).", 
            "title": "Model Functions"
        }, 
        {
            "location": "/api/#distributionspace-functions", 
            "text": "#  Base.Random.rand     Function .   rand{T}(rng::AbstractRNG, d::AbstractSpace{T}, sample::T)  Returns a random  sample  from space  s .  rand{T}(rng::AbstractRNG, d::AbstractDistribution{T}, sample::T)  Fill  sample  with a random element from distribution  d . The sample can be a state, action or observation.  #  POMDPs.pdf     Function .   pdf{T}(d::AbstractDistribution{T}, x::T)  Value of probability distribution  d  function at sample  x .  #  POMDPs.dimensions     Function .   dimensions{T}(s::AbstractSpace{T})  Returns the number of dimensions in space  s .  #  POMDPs.iterator     Function .   iterator{T}(s::AbstractSpace{T})  Returns an iterable type (array or custom iterator) corresponding to space  s .   iterator{T}(d::AbstractDistribution{T})  Returns an iterable type (array or custom iterator) corresponding to distribution  d .   #  POMDPs.initial_state_distribution     Function .   initial_state_distribution(pomdp::POMDP)  Returns an initial belief for the pomdp.  #  POMDPs.create_transition_distribution     Function .   create_transition_distribution(problem::POMDP)\ncreate_transition_distribution(problem::MDP)  Returns a transition distribution (for memory preallocation).  #  POMDPs.create_observation_distribution     Function .   create_observation_distribution(problem::POMDP)\ncreate_observation_distribution(problem::MDP)  Returns an observation distribution (for memory preallocation).", 
            "title": "Distribution/Space Functions"
        }, 
        {
            "location": "/api/#belief-functions", 
            "text": "#  POMDPs.update     Function .   update{B,A,O}(updater::Updater, belief_old::B, action::A, obs::O,\nbelief_new::B=create_belief(updater))  Returns a new instance of an updated belief given  belief_old  and the latest action and observation.  #  POMDPs.create_belief     Function .   create_belief(updater::Updater)  Creates a belief object of the type used by  updater  (preallocates memory)  create_belief(pomdp::POMDP)  Creates a belief either to be used by updater or pomdp  #  POMDPs.initialize_belief     Function .   initialize_belief{B}(updater::Updater{B}, \n                     state_distribution::AbstractDistribution,\n                     new_belief::B=create_belief(updater))\ninitialize_belief{B}(updater::Updater{B},\n                     belief::Any,\n                     new_belief::B=create_belief(updater))  Returns a belief that can be updated using  updater  that has similar distribution to  state_distribution  or  belief .  The conversion may be lossy. This function is also idempotent, i.e. there is a default implementation that passes the belief through when it is already the correct type:  initialize_belief{B}(updater::Updater{B}, belief::B) = belief", 
            "title": "Belief Functions"
        }, 
        {
            "location": "/api/#policy-and-solver-functions", 
            "text": "#  POMDPs.create_policy     Function .   create_policy(solver::Solver, problem::POMDP)\ncreate_policy(solver::Solver, problem::MDP)  Creates a policy object (for preallocation purposes)  #  POMDPs.solve     Function .   solve(solver::Solver, problem::POMDP, policy=create_policy(solver, problem))  Solves the POMDP using method associated with solver, and returns a policy.   #  POMDPs.updater     Function .   updater(policy::Policy)  Returns a default Updater appropriate for a belief type that policy  p  can use  #  POMDPs.action     Function .   action{B}(p::Policy, x::B, action)  Fills and returns action based on the current state or belief, given the policy. B is a generalized information state - can be a state in an MDP, a distribution in POMDP, or any other representation needed to make a decision using the given policy.   action{B}(policy::Policy, x::B)  Returns an action for the current state or belief, given the policy  If an MDP is being simulated, x will be a state; if a POMDP is being simulated, x will be a belief  #  POMDPs.value     Function .   value{B}(p::Policy, x::B)  Returns the utility value from policy p given the state", 
            "title": "Policy and Solver Functions"
        }, 
        {
            "location": "/api/#simulator", 
            "text": "#  POMDPs.Simulator     Type .   Base type for an object defining how a simulation should be carried out  #  POMDPs.simulate     Function .   simulate{S,A}(simulator::Simulator, problem::MDP{S,A}, policy::Policy, initial_state::S)  Runs a simulation using the specified policy and returns the accumulated reward  simulate{S,A,O,B}(simulator::Simulator, problem::POMDP{S,A,O}, policy::Policy{B}, updater::Updater{B}, initial_belief::Union{B,AbstractDistribution{S}})  Runs a simulation using the specified policy and returns the accumulated reward", 
            "title": "Simulator"
        }, 
        {
            "location": "/api/#utility-tools", 
            "text": "#  POMDPs.add     Function .   add(solver_name::AbstractString)  Downloads and installs a registered solver with name  solver_name .  This function is not exported, and must be called:  julia   using   POMDPs  julia   POMDPs . add ( MCTS )   #  POMDPs.@pomdp_func     Macro .   Provide a default function implementation that throws an error when called.  #  POMDPs.strip_arg     Function .   Strip anything extra (type annotations, default values, etc) from an argument.  For now this cannot handle keyword arguments (it will throw an error).", 
            "title": "Utility Tools"
        }, 
        {
            "location": "/api/#constants", 
            "text": "#  POMDPs.REMOTE_URL     Constant .   url to remote JuliaPOMDP organization repo  #  POMDPs.SUPPORTED_SOLVERS     Constant .   Set containing string names of officially supported solvers  (e.g.  MCTS ,  SARSOP , etc).  If you have a validated solver that supports the POMDPs.jl API, contact the developers to add your solver to this list.", 
            "title": "Constants"
        }
    ]
}