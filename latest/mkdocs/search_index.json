{
    "docs": [
        {
            "location": "/", 
            "text": "POMDPs\n\n\nA Julia interface for defining, solving and simulating partially observable Markov decision processes and their fully observable counterparts.\n\n\n\n\nPackage Features\n\n\n\n\nGeneral interface that can handle problems with discrete and continuous state/action/observation spaces\n\n\nA number of popular state-of-the-art solvers availiable to use out of the box\n\n\nTools that make it easy to define problems and simulate solutions\n\n\nSimple integration of custom solvers into the existing interface\n\n\n\n\n\n\nAvailible Packages\n\n\nThe POMDPs.jl package contains the interface used for expressing and solving Markov decision processes (MDPs) and partially observable Markov decision processes (POMDPs) in the Julia programming language. The \nJuliaPOMDP\n community maintains these packages. The packages currently maintained by JuliaPOMDP are as follows: \n\n\n\n\nMDP solvers:\n\n\n\n\nValue Iteration\n\n\nMonte Carlo Tree Search\n\n\n\n\n\n\nPOMDP solvers:\n\n\n\n\nQMDP\n\n\nSARSOP\n\n\nPOMCP\n\n\nDESPOT\n\n\nMCVI\n\n\nPOMDPSolve\n\n\n\n\n\n\nSupport Tools:\n\n\n\n\nPOMDPToolbox\n\n\nPOMDPModels\n\n\n\n\n\n\nInterface Extensions:\n\n\n\n\nGenerativeModels\n\n\nPOMDPBounds\n\n\n\n\n\n\nManual Outline\n\n\n\n\nPOMDPs\n\n\nPackage Features\n\n\nAvailible Packages\n\n\nManual Outline\n\n\n\n\n\n\nDefining a POMDP\n\n\nFunctional Form POMDP\n\n\nTabular Form POMDP\n\n\nContinous POMDP\n\n\n\n\n\n\nFrequently Asked Questions (FAQ)\n\n\nWhy am I getting a \"No implemnetation for ...\" error?\n\n\nHow do I save my policies?\n\n\nWhy isn't the solver working?\n\n\nWhy do I need to put type assertions pomdp::POMDP into the function signature?\n\n\nWhy are all the solvers in seperate modules?\n\n\n\n\n\n\nInstallation\n\n\nGetting Started\n\n\nAPI Documentation\n\n\nContents\n\n\nIndex\n\n\nTypes\n\n\nModel Functions\n\n\nDistribution/Space Functions\n\n\nBelief Functions\n\n\nPolicy and Solver Functions\n\n\nSimulator\n\n\nUtility Tools\n\n\nConstants\n\n\n\n\n\n\nConcepts and Architecture\n\n\nPOMDPs and MDPs\n\n\nBeliefs and Updaters\n\n\nSolvers and Policies\n\n\nSimulators\n\n\n\n\n\n\nDefining a Solver\n\n\nBackground\n\n\nQMDP Algorithm\n\n\nRequirements for a Solver\n\n\nDefining the Solver and Policy Types\n\n\nWriting the Solve Function\n\n\nCreating an Updater\n\n\nEvaluating the Solver", 
            "title": "About"
        }, 
        {
            "location": "/#pomdps", 
            "text": "A Julia interface for defining, solving and simulating partially observable Markov decision processes and their fully observable counterparts.", 
            "title": "POMDPs"
        }, 
        {
            "location": "/#package-features", 
            "text": "General interface that can handle problems with discrete and continuous state/action/observation spaces  A number of popular state-of-the-art solvers availiable to use out of the box  Tools that make it easy to define problems and simulate solutions  Simple integration of custom solvers into the existing interface", 
            "title": "Package Features"
        }, 
        {
            "location": "/#availible-packages", 
            "text": "The POMDPs.jl package contains the interface used for expressing and solving Markov decision processes (MDPs) and partially observable Markov decision processes (POMDPs) in the Julia programming language. The  JuliaPOMDP  community maintains these packages. The packages currently maintained by JuliaPOMDP are as follows:", 
            "title": "Availible Packages"
        }, 
        {
            "location": "/#mdp-solvers", 
            "text": "Value Iteration  Monte Carlo Tree Search", 
            "title": "MDP solvers:"
        }, 
        {
            "location": "/#pomdp-solvers", 
            "text": "QMDP  SARSOP  POMCP  DESPOT  MCVI  POMDPSolve", 
            "title": "POMDP solvers:"
        }, 
        {
            "location": "/#support-tools", 
            "text": "POMDPToolbox  POMDPModels", 
            "title": "Support Tools:"
        }, 
        {
            "location": "/#interface-extensions", 
            "text": "GenerativeModels  POMDPBounds", 
            "title": "Interface Extensions:"
        }, 
        {
            "location": "/#manual-outline", 
            "text": "POMDPs  Package Features  Availible Packages  Manual Outline    Defining a POMDP  Functional Form POMDP  Tabular Form POMDP  Continous POMDP    Frequently Asked Questions (FAQ)  Why am I getting a \"No implemnetation for ...\" error?  How do I save my policies?  Why isn't the solver working?  Why do I need to put type assertions pomdp::POMDP into the function signature?  Why are all the solvers in seperate modules?    Installation  Getting Started  API Documentation  Contents  Index  Types  Model Functions  Distribution/Space Functions  Belief Functions  Policy and Solver Functions  Simulator  Utility Tools  Constants    Concepts and Architecture  POMDPs and MDPs  Beliefs and Updaters  Solvers and Policies  Simulators    Defining a Solver  Background  QMDP Algorithm  Requirements for a Solver  Defining the Solver and Policy Types  Writing the Solve Function  Creating an Updater  Evaluating the Solver", 
            "title": "Manual Outline"
        }, 
        {
            "location": "/install/", 
            "text": "Installation\n\n\nIf you have a running Julia distriubtion (Julia 0.4 or greaer), you have everything you need to install POMDPs.jl. To install the package, simply run the following from the Julia REPL:\n\n\nPkg.add(\nPOMDPs\n) # installs the POMDPs.jl package\n\n\n\n\nOnce you have POMDPs.jl installed, you can install any package that is part of the JuliaPOMDP community by running:\n\n\nusing POMDPs\nPOMDPs.add(\nSARSOP\n) # installs the SARSOP solver\n\n\n\n\nThe code above will download and install all the dependencies automatically. All JuliaPOMDP packages have been tested on Linux and OS X, and most have been tested on Windows. \n\n\nTo get a list of all the availible packages run:\n\n\nPOMDPs.available() # prints a list of all the availible packages that can be installed with POMDPs.add\n\n\n\n\nDue to the modular nature of the framework, you can install only the solvers/support tools you plan on using. However, if you want to install all of the supported JuliaPOMDP packages you can run the following code:\n\n\nPOMDPs.add_all() # installs all the JuliaPOMDP packages (may take a few minutes)", 
            "title": "Installation"
        }, 
        {
            "location": "/install/#installation", 
            "text": "If you have a running Julia distriubtion (Julia 0.4 or greaer), you have everything you need to install POMDPs.jl. To install the package, simply run the following from the Julia REPL:  Pkg.add( POMDPs ) # installs the POMDPs.jl package  Once you have POMDPs.jl installed, you can install any package that is part of the JuliaPOMDP community by running:  using POMDPs\nPOMDPs.add( SARSOP ) # installs the SARSOP solver  The code above will download and install all the dependencies automatically. All JuliaPOMDP packages have been tested on Linux and OS X, and most have been tested on Windows.   To get a list of all the availible packages run:  POMDPs.available() # prints a list of all the availible packages that can be installed with POMDPs.add  Due to the modular nature of the framework, you can install only the solvers/support tools you plan on using. However, if you want to install all of the supported JuliaPOMDP packages you can run the following code:  POMDPs.add_all() # installs all the JuliaPOMDP packages (may take a few minutes)", 
            "title": "Installation"
        }, 
        {
            "location": "/get_started/", 
            "text": "Getting Started\n\n\nBefore writing our own POMDP problems or solvers, let's try out some of the availiable solvers and problem models availible in JuliaPOMDP.\n\n\nHere is a short piece of code that solves the Tiger POMDP using SARSOP, and evaluates the results. Note that you must have the SARSOP, POMDPModels, and POMDPToolbox modules installed. \n\n\nusing SARSOP, POMDPModels, POMDPToolbox\n\n# initialize problem and solver\npomdp = TigerPOMDP() # from POMDPModels\nsolver = SARSOPSolver() # from SARSOP\n\n# compute a policy\npolicy = solve(solver, pomdp)\n\n#evaluate the policy\nbelief_updater = updater(policy) # the default QMPD belief updater (discrete Bayesian filter)\ninit_dist = initial_state_distribution(pomdp) # from POMDPModels\nhist = HistoryRecorder(max_steps=100) # from POMDPToolbox\nr = simulate(hist, pomdp, policy, belief_updater, init_dist) # run 100 step simulation\n\n\n\n\nThe first part of the code loads the desired packages and initializes the problem and the solver. Next, we compute a POMDP policy. Lastly, we evaluate the results. \n\n\nThere are a few things to mention here. First, the TigerPOMDP type implements all the functions required by SARSOPSolver to compute a policy. Second, each policy has a default updater (essentially a filter used to update the belief of the POMDP). To learn more about Updaters check out the Concepts section.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/get_started/#getting-started", 
            "text": "Before writing our own POMDP problems or solvers, let's try out some of the availiable solvers and problem models availible in JuliaPOMDP.  Here is a short piece of code that solves the Tiger POMDP using SARSOP, and evaluates the results. Note that you must have the SARSOP, POMDPModels, and POMDPToolbox modules installed.   using SARSOP, POMDPModels, POMDPToolbox\n\n# initialize problem and solver\npomdp = TigerPOMDP() # from POMDPModels\nsolver = SARSOPSolver() # from SARSOP\n\n# compute a policy\npolicy = solve(solver, pomdp)\n\n#evaluate the policy\nbelief_updater = updater(policy) # the default QMPD belief updater (discrete Bayesian filter)\ninit_dist = initial_state_distribution(pomdp) # from POMDPModels\nhist = HistoryRecorder(max_steps=100) # from POMDPToolbox\nr = simulate(hist, pomdp, policy, belief_updater, init_dist) # run 100 step simulation  The first part of the code loads the desired packages and initializes the problem and the solver. Next, we compute a POMDP policy. Lastly, we evaluate the results.   There are a few things to mention here. First, the TigerPOMDP type implements all the functions required by SARSOPSolver to compute a policy. Second, each policy has a default updater (essentially a filter used to update the belief of the POMDP). To learn more about Updaters check out the Concepts section.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/concepts/", 
            "text": "Concepts and Architecture\n\n\nPOMDPs.jl aims to coordinate the development of three software components: 1) a problem, 2) a solver, 3) an experiment. Each of these components has a set of abstract types associated with it and a set of functions that allow a user to define each component's behavior in a standardized way. An outline of the architecture is shown below.\n\n\n\n\nThe MDP and POMDP types are associated with the problem definition. The Solver and Policy types are associated with the solver or decision-making agent. Typically, the Updater type is also associated with the solver, but a solver may sometimes be used with an updater that was implemented separately. The Simulator type is associated with the experiment. \n\n\n\n\nPOMDPs and MDPs\n\n\nAn MDP is a definition of a problem where the state of the problem is fully observable. Mathematically, an MDP is a tuple (S,A,T,R), where S is the state space, A is the action space, T is a function that defines the probability of transitioning to each state given the state and action at the previous time, and R is a reward function mapping every possible transition (s,a,s') to a real reward value. For more information see a textbook such as [1]. In POMDPs.jl an MDP is represented by a concrete subtype of the \nMDP\n abstract type and a set of methods that define each of its components. S and A are defined by implementing methods of the \nstates\n and \nactions\n functions for the \nMDP\n subtype, though for some solvers, the state space does not need to be explicitly defined. T and R are defined by implementing methods of the \ntransition\n and \nreward\n functions. \n\n\nA POMDP is a problem definition where the state is only partially observable by the decision making agent. Mathematically, a POMDP is a tuple (S,A,T,R,O,Z) where S, A, T, and R have the same meaning as in the MDP case, Z is the set of observations that the decision-making agent might receive and O is a function defining the probability of receiving each observation at a transition. In POMDPs.jl, a POMDP is represented by a concrete subtype of the \nPOMDP\n abstract type, \nZ\n may be defined by the \nobservations\n function (though an explicit definition is often not required), and \nO\n is defined by implementing a method of \nobservation\n for the POMDP type.\n\n\nPOMDPs.jl also contains functions for defining optional problem behavior such as a discount factor or a set of terminal states.\n\n\nIt is important to note that, in some cases, it is difficult to explicitly represent the transition and observation distributions for a problem but easy to generate a sampled next state or observation. In these cases it may be significantly easier to use the \nGenerativeModels.jl\n interface extension \ninstead of\n implementing methods of \ntransition\n and \nobservation\n\n\n\n\nBeliefs and Updaters\n\n\nIn a POMDP domain, the decision-making agent does not have complete information about the state of the problem, so the agent can only make choices based on its \"belief\" about the state. In the POMDP literature, the term \"belief\" is typically defined to mean a probability distribution over all possible states of the system. However, in practice, the agent often makes decisions based on an incomplete or lossy record of past observations that has a structure much different from a probability distribution. For example, if the agent is represented by a finite-state controller as is the case for Monte-Carlo Value Iteration [2], the belief is the controller state, which is a node in a graph. Another example is an agent represented by a recurrent neural network. In this case, the agent's belief is the state of the network. In order to accommodate a wide variety of decision-making approaches, in POMDPs.jl, we use the term \"belief\" to denote the set of information that the agent makes a decision on, which could be an exact state distribution, an action-observation history, a set of weighted particles, or the examples mentioned before. In code, the belief can be represented by any built-in or user defined type.\n\n\nWhen an action is taken and a new observation is received, the belief is updated by the belief updater. In code, a belief updater is represented by a concrete subtype of the \nUpdater\n abstract type, and the \nupdate\n function defines how the belief is updated when a new observation is received.\n\n\nAlthough the agent may use a specialized belief structure to make decisions, the information initially given to the agent about the state of the problem is usually most conveniently represented as a state distribution, thus the \ninitialize_belief\n function is provided to convert a state distribution to a specialized belief structure that an updater can work with.\n\n\nIn many cases, the belief structure is closely related to the solution technique, so it will be implemented by the programmer who writes the solver. In other cases, the agent can use a variety of belief structures to make decisions, so a domain-specific updater implemented by the programmer that wrote the problem description may be appropriate. Finally, some advanced generic belief updaters such as particle filters may be implemented by a third party. The convenience function \nupdater\n can be used to get a suitable default updater for a policy, however many policies can work with other updaters.\n\n\n\n\nSolvers and Policies\n\n\nSequential decision making under uncertainty involves both online and offline calculations. In the broad sense, the term \"solver\" as used in the node in the figure above refers to the package of software that performs the calculations at both of these times. However, the code is broken up into two pieces, the solver that performs calculations offline and the policy that performs calculations online.\n\n\nIn the abstract sense, a policy is a mapping from every belief that an agent might take to an action. A policy is represented in code by a concrete subtype of the \nPolicy\n abstract type. The programmer defines a method of the \naction\n function to describe what computations need to be done online. For an online solver such as POMCP, all of the decision computation occurs within \naction\n while for an offline solver like SARSOP, there is very little computation within \naction\n\n\nThe offline portion of the computation is carried out by the solver, which is represented by a concrete subtype of the \nSolver\n abstract type. Computations occur within the \nsolve\n function. For an offline solver like SARSOP, nearly all of the decision computation occurs within this function, but for some online solvers such as POMCP, \nsolve\n merely embeds the problem in the policy.\n\n\n\n\nSimulators\n\n\nA simulator defines a way to run a single simulation. It is represented by a concrete subtype of the \nSimulator\n abstract type and the simulation is implemented in a method of the \nsimulate\n function. \nsimulate\n should return the discounted sum of the stagewise rewards, and the simulator may or may not keep track of the state trajectory or other statistics or display the simulation as it is carried out.\n\n\n[1] \nDecision Making Under Uncertainty: Theory and Application\n by Mykel J. Kochenderfer, MIT Press, 2015\n\n\n[2] Bai, H., Hsu, D., \n Lee, W. S. (2014). Integrated perception and planning in the continuous space: A POMDP approach. The International Journal of Robotics Research, 33(9), 1288-1302", 
            "title": "Concepts and Architecture"
        }, 
        {
            "location": "/concepts/#concepts-and-architecture", 
            "text": "POMDPs.jl aims to coordinate the development of three software components: 1) a problem, 2) a solver, 3) an experiment. Each of these components has a set of abstract types associated with it and a set of functions that allow a user to define each component's behavior in a standardized way. An outline of the architecture is shown below.   The MDP and POMDP types are associated with the problem definition. The Solver and Policy types are associated with the solver or decision-making agent. Typically, the Updater type is also associated with the solver, but a solver may sometimes be used with an updater that was implemented separately. The Simulator type is associated with the experiment.", 
            "title": "Concepts and Architecture"
        }, 
        {
            "location": "/concepts/#pomdps-and-mdps", 
            "text": "An MDP is a definition of a problem where the state of the problem is fully observable. Mathematically, an MDP is a tuple (S,A,T,R), where S is the state space, A is the action space, T is a function that defines the probability of transitioning to each state given the state and action at the previous time, and R is a reward function mapping every possible transition (s,a,s') to a real reward value. For more information see a textbook such as [1]. In POMDPs.jl an MDP is represented by a concrete subtype of the  MDP  abstract type and a set of methods that define each of its components. S and A are defined by implementing methods of the  states  and  actions  functions for the  MDP  subtype, though for some solvers, the state space does not need to be explicitly defined. T and R are defined by implementing methods of the  transition  and  reward  functions.   A POMDP is a problem definition where the state is only partially observable by the decision making agent. Mathematically, a POMDP is a tuple (S,A,T,R,O,Z) where S, A, T, and R have the same meaning as in the MDP case, Z is the set of observations that the decision-making agent might receive and O is a function defining the probability of receiving each observation at a transition. In POMDPs.jl, a POMDP is represented by a concrete subtype of the  POMDP  abstract type,  Z  may be defined by the  observations  function (though an explicit definition is often not required), and  O  is defined by implementing a method of  observation  for the POMDP type.  POMDPs.jl also contains functions for defining optional problem behavior such as a discount factor or a set of terminal states.  It is important to note that, in some cases, it is difficult to explicitly represent the transition and observation distributions for a problem but easy to generate a sampled next state or observation. In these cases it may be significantly easier to use the  GenerativeModels.jl  interface extension  instead of  implementing methods of  transition  and  observation", 
            "title": "POMDPs and MDPs"
        }, 
        {
            "location": "/concepts/#beliefs-and-updaters", 
            "text": "In a POMDP domain, the decision-making agent does not have complete information about the state of the problem, so the agent can only make choices based on its \"belief\" about the state. In the POMDP literature, the term \"belief\" is typically defined to mean a probability distribution over all possible states of the system. However, in practice, the agent often makes decisions based on an incomplete or lossy record of past observations that has a structure much different from a probability distribution. For example, if the agent is represented by a finite-state controller as is the case for Monte-Carlo Value Iteration [2], the belief is the controller state, which is a node in a graph. Another example is an agent represented by a recurrent neural network. In this case, the agent's belief is the state of the network. In order to accommodate a wide variety of decision-making approaches, in POMDPs.jl, we use the term \"belief\" to denote the set of information that the agent makes a decision on, which could be an exact state distribution, an action-observation history, a set of weighted particles, or the examples mentioned before. In code, the belief can be represented by any built-in or user defined type.  When an action is taken and a new observation is received, the belief is updated by the belief updater. In code, a belief updater is represented by a concrete subtype of the  Updater  abstract type, and the  update  function defines how the belief is updated when a new observation is received.  Although the agent may use a specialized belief structure to make decisions, the information initially given to the agent about the state of the problem is usually most conveniently represented as a state distribution, thus the  initialize_belief  function is provided to convert a state distribution to a specialized belief structure that an updater can work with.  In many cases, the belief structure is closely related to the solution technique, so it will be implemented by the programmer who writes the solver. In other cases, the agent can use a variety of belief structures to make decisions, so a domain-specific updater implemented by the programmer that wrote the problem description may be appropriate. Finally, some advanced generic belief updaters such as particle filters may be implemented by a third party. The convenience function  updater  can be used to get a suitable default updater for a policy, however many policies can work with other updaters.", 
            "title": "Beliefs and Updaters"
        }, 
        {
            "location": "/concepts/#solvers-and-policies", 
            "text": "Sequential decision making under uncertainty involves both online and offline calculations. In the broad sense, the term \"solver\" as used in the node in the figure above refers to the package of software that performs the calculations at both of these times. However, the code is broken up into two pieces, the solver that performs calculations offline and the policy that performs calculations online.  In the abstract sense, a policy is a mapping from every belief that an agent might take to an action. A policy is represented in code by a concrete subtype of the  Policy  abstract type. The programmer defines a method of the  action  function to describe what computations need to be done online. For an online solver such as POMCP, all of the decision computation occurs within  action  while for an offline solver like SARSOP, there is very little computation within  action  The offline portion of the computation is carried out by the solver, which is represented by a concrete subtype of the  Solver  abstract type. Computations occur within the  solve  function. For an offline solver like SARSOP, nearly all of the decision computation occurs within this function, but for some online solvers such as POMCP,  solve  merely embeds the problem in the policy.", 
            "title": "Solvers and Policies"
        }, 
        {
            "location": "/concepts/#simulators", 
            "text": "A simulator defines a way to run a single simulation. It is represented by a concrete subtype of the  Simulator  abstract type and the simulation is implemented in a method of the  simulate  function.  simulate  should return the discounted sum of the stagewise rewards, and the simulator may or may not keep track of the state trajectory or other statistics or display the simulation as it is carried out.  [1]  Decision Making Under Uncertainty: Theory and Application  by Mykel J. Kochenderfer, MIT Press, 2015  [2] Bai, H., Hsu, D.,   Lee, W. S. (2014). Integrated perception and planning in the continuous space: A POMDP approach. The International Journal of Robotics Research, 33(9), 1288-1302", 
            "title": "Simulators"
        }, 
        {
            "location": "/def_pomdp/", 
            "text": "Defining a POMDP\n\n\nThe expressive nature of POMDPs.jl gives problem writers the flexiblity to write their problem in many forms. In this section we will take a look at two ways to write a discrete problem, and a way of writing a continuous problem. \n\n\n\n\nFunctional Form POMDP\n\n\nThe first, and most straighforward way to define a POMDP problem is to implement the model functions that you may need. For example, all POMDPs will need \ntransition\n, \nreward\n, and \nobservation\n functions. In this example we'll start with the simple Tiger POMDP problem. We want to use the SARSOP solver to compute a policy. To use a solver from JuliaPOMDP, a problem writer must define a set of functions required by the solver. To see what functions are required by SARSOP, check out its documentation \nhere\n. \n\n\nLet's first define the Tiger POMDP type.\n\n\nusing POMDPs # load the interface\ntype TigerPOMDP \n: POMDP{Bool, Int64, Bool} # parametarized inheritance POMDP{state, action, observation}\n    r_listen::Float64 # reward for listening (negative)\n    r_findtiger::Float64 # reward for finding the tiger (negative)\n    r_escapetiger::Float64 # reward for escaping\n    p_listen_correctly::Float64 # probbility that we hear the tiger correctly\n    discount_factor::Float64 # discount factor\nend\nTigerPOMDP() = TigerPOMDP(-1.0, -100.0, 10.0, 0.85, 0.95) # default contructor\n\n\n\n\nNotice that the \nTigerPOMDP\n is inheriting from the abstract \nPOMDP\n type that comes from POMDPs.jl. The abstract \nPOMDP\n is parametarized by a \nBool\n, \nInt64\n, \nBool\n combination with the syntax \nTigerPOMDP \n: POMDP{Bool, Int64, Bool}\n. The parametarization defines how we choose to represent the state, actions, and observations in our problem. In the \nTigerPOMDP\n we use a boolean to represent our states and observations (because there are two of each) and an integer to represent our actions (because there are 3). If you wanted to create a custom concrete type to represent your states, actions, or observations you could do that as well. Let's say we made a type to represent our states called \nAwesomeTigerState\n. That type could contain integers, floats, arrays, or other complex data structures (depending on what's convenient). We would then parametrize the Tiger POMDP in the following way: \ntype TigerPOMDP \n: POMDP{AwesomeTigerState, Int64, Bool}\n.\n\n\nNow, let's consider another important component of POMDPs, probability distributions. In the POMDPs.jl interface, we think in terms of distribution types. We want to be able to sample from these distriubtions and compute their probability masses or densities. In the Tiger POMDP, our distriubtions are over binary variables (boolean state or observation), so we can implement a simple version of a Bernoulli distribution.\n\n\ntype TigerDistribution \n: AbstractDistribution # inherits from a POMDPs.jl abstract type\n    p::Float64 # probability of 1\n    it::Vector{Bool} # pre-allocate the domain of the distriubtion\nend\nTigerDistribution() = TigerDistribution(0.5, [true, false]) # default constructo\n\niterator(d::TigerDistribution) = d.it # convenience function used by discrete solvers (iterator over the discrete distriubtion)\n\n\n\n\nLet's implement the pdf and rand function that returns the probability mass and samples from the distribution.\n\n\n# returns the probability mass \nfunction pdf(d::TigerDistribution, so::Bool)\n    so ? (return d.p) : (return 1.0-d.p)\nend\n\n# samples the dsitribution\nrand(rng::AbstractRNG, d::TigerDistribution, s::Bool) = rand(rng) \n= d.p\n\n\n\n\nWe also want some convenience functions for initializing the distriubtions.\n\n\ncreate_transition_distribution(::TigerPOMDP) = TigerDistribution()\ncreate_observation_distribution(::TigerPOMDP) = TigerDistribution()\n\n\n\n\nLet's define our transition, observation, and reward functions.\n\n\nfunction transition(pomdp::TigerPOMDP, s::Bool, a::Int64, d::TigerDistribution=create_transition_distribution(pomdp))\n    # Resets the problem after opening door; does nothing after listening        \n    if a == 1 || a == 2\n        d.p = 0.5\n    elseif s\n        d.p = 1.0\n    else\n        d.p = 0.0\n    end\n    d\nend\n\nfunction observation(pomdp::TigerPOMDP, s::Bool, a::Int64, d::TigerDistribution=create_observation_distribution(pomdp))\n    # correct observation wiht prob pc        \n    pc = pomdp.p_listen_correctly\n    if a == 0\n        s ? (d.p = pc) : (d.p = 1.0-pc)\n    else\n        d.p = 0.5\n    end\n    d\nend\n# convenience function\nfunction observation(pomdp::TigerPOMDP, s::Bool, a::Int64, sp::Bool, d::TigerDistribution=create_observation_distribution(pomdp))\n    return observation(pomdp, s, a, d)\nend\n\nfunction reward(pomdp::TigerPOMDP, s::Bool, a::Int64)\n    # rewarded for escaping, penalized for listening and getting caught\n    r = 0.0\n    a == 0 ? (r+=pomdp.r_listen) : (nothing)\n    if a == 1\n        s ? (r += pomdp.r_findtiger) : (r += pomdp.r_escapetiger)\n    end\n    if a == 2\n        s ? (r += pomdp.r_escapetiger) : (r += pomdp.r_findtiger)\n    end\n    return r\nend\n# convenience function\nreward(pomdp::TigerPOMDP, s::Bool, a::Int64, sp::Bool) = reward(pomdp, s, a)\n\n\n\n\nThe last important component of a POMDP are the spaces. There is a special \nAbstractSpace\n type in POMDPs.jl which all spaces inherit from. We define the state, action, and observation spaces below as well as functions for intializing them and sampling from them.\n\n\n# STATE SPACE\ntype TigerStateSpace \n: AbstractSpace\n    states::Vector{Bool} # states are boolean\nend\n# initialize the state space\nstates(::TigerPOMDP) = TigerStateSpace([true, false])\n# for iterating over discrete spaces\niterator(space::TigerStateSpace) = space.states\ndimensions(::TigerStateSpace) = 1\n# sample from the state sapce\nrand(rng::AbstractRNG, space::TigerStateSpace, s::Bool) = rand(rng) \n 0.5 ? (return true) : (return false)\n\n# ACTION SPACE\ntype TigerActionSpace \n: AbstractSpace\n    actions::Vector{Int64} # three possible actions\nend\n# initialize the action space\nactions(::TigerPOMDP) = TigerActionSpace([0,1,2])\n# iterate of the action space\niterator(space::TigerActionSpace) = space.actions\ndimensions(::TigerActionSpace) = 1\n# sample from the aciton space\nrand(rng::AbstractRNG, space::TigerActionSpace, a::Int64) = rand(rng, 0:2)\n\n# OBSERVATION SPACE\ntype TigerObservationSpace \n: AbstractSpace\n    obs::Vector{Bool}\nend\n# initialize\nobservations(::TigerPOMDP) = TigerObservationSpace([true, false])\n# iterate over obs space\niterator(space::TigerObservationSpace) = space.obs\ndimensions(::TigerObservationSpace) = 1\n# sample from the obs sapce\nrand(rng::AbstractRNG, space::TigerObservationSpace, s::Bool) = rand(rng) \n 0.5 ? (return true) : (return false)\n\n\n\n\nThe last important component of a POMDP is the initial distribution over the state space. In POMDPs.jl we make a strong distinction between this distribution and a belief. In most literature these two concepts are considered the same. However, in most general terms, a belief is something that is mapped to an action using a POMDP policy. If the policy is represented as something other than alpha-vectors (a policy graph, tree, or a reccurent neural netowrk to give a few examples), it may not make sense to think of a belief as a probability distribution over the state space. Thus, in POMDPs.jl we abstract the concept of a belief beyond a probability distribution (of course it can be a probability distriubtion if it makes sense). \n\n\nIn order to reconcile this difference, each policy has a function called \ninitialize_belief\n which takes in an initial state distirubtion (this is a probability distribution over the state space) and a policy, and converts the distribution into what we call a belief in POMDPs.jl - a representation of a POMDP that is mapped to an action using the policy. \n\n\nLet's define the initial state distribution function for our POMDP.\n\n\ninitial_state_distribution(pomdp::TigerPOMDP) = TigerDistribution(0.5, [true, false])\n\n\n\n\nNow that've defined all the main components, we need to wrap up our model by creating some convenience functions below.\n\n\n# initialization functions\ncreate_state(::TigerPOMDP) = zero(Bool)\ncreate_observation(::TigerPOMDP) = zero(Bool)\ncreate_action(::TigerPOMDP) = zero(Int64)\n\n# for discrete problems\nn_states(::TigerPOMDP) = 2\nn_actions(::TigerPOMDP) = 3\nn_observations(::TigerPOMDP) = 2\n\n# for indexing discrete states\nstate_index(::TigerPOMDP, s::Bool) = Int64(s) + 1\n\ndiscount(pomdp::TigerPOMDP) = pomdp.discount_factor\n\n\n\n\nNow that we've defined all these functions, we can use one of the JuliaPOMDP solvers to compute and evaluate a policy. \n\n\nusing QMDP, POMDPToolbox\n\npomdp = TigerPOMDP()\nsolver = QMDPSolver()\npolicy = solve(solver, pomdp)\n\ninit_dist = initial_state_distribution(pomdp)\nhist = HistoryRecorder(max_steps=100) # from POMDPToolbox\nr = simulate(hist, pomdp, policy, belief_updater, init_dist) # run 100 step simulation\n\n\n\n\nPlease note that you do not need to define all the functions for most solvers. If you want to use an individual solver, you usually need only a subset of what's above. \n\n\n\n\nTabular Form POMDP\n\n\nAnother way to define discrete POMDP problems is by writing them in tabular form. Specifically, if you can write the transition and observation probabilities as well as the rewards in matrix form, you can use the \nDiscreteMDP\n or \nDiscretePOMDP\n types form \nPOMDPModels\n which automatically implements all the functions you'll need for you. Let's do this with the Tiger POMDP.\n\n\nusing POMDPModels\n\n# write out the matrix forms\n\n# REWARDS\nR = [-1. -100 10; -1 10 -100] # |S|x|A| state-action pair rewards\n\n# TRANSITIONS\nT = zeros(2,3,2) # |S|x|A|x|S|, T[s', a, s] = p(s'|a,s)\nT[:,:,1] = [1. 0.5 0.5; 0 0.5 0.5]\nT[:,:,2] = [0. 0.5 0.5; 1 0.5 0.5]\n\n# OBSERVATIONS\nO = zeros(2,3,2) # |O|x|A|x|S|, O[o, a, s] = p(o|a,s)\nO[:,:,1] = [0.85 0.5 0.5; 0.15 0.5 0.5]\nO[:,:,2] = [0.15 0.5 0.5; 0.85 0.5 0.5]\n\ndiscount = 0.95\npomdp = DiscretePOMDP(T, R, O, discount)\n\n# solve the POMDP the same way\nsolver = SARSOPSolver()\npolicy = solve(solver, pomdp)\n\n\n\n\nIt is usually fairly simple to define smaller problems in the tabular form. However, for larger problems it can be tedious and the functional form may be preffered. You can usually use any supported POMDP solver to sovle these types of problems (the performance of the policy may vary however - SARSOP will usually outperform QMDP). \n\n\n\n\nContinous POMDP\n\n\nWithin the POMDPs.jl interface, we can also define problems with continuous spaces.  There are a few solvers that can handle these types of problems, namely, MCVI and POMCP (with some tunning). Light-Dark problem here. What should we say about bounds?", 
            "title": "Defining a POMDP"
        }, 
        {
            "location": "/def_pomdp/#defining-a-pomdp", 
            "text": "The expressive nature of POMDPs.jl gives problem writers the flexiblity to write their problem in many forms. In this section we will take a look at two ways to write a discrete problem, and a way of writing a continuous problem.", 
            "title": "Defining a POMDP"
        }, 
        {
            "location": "/def_pomdp/#functional-form-pomdp", 
            "text": "The first, and most straighforward way to define a POMDP problem is to implement the model functions that you may need. For example, all POMDPs will need  transition ,  reward , and  observation  functions. In this example we'll start with the simple Tiger POMDP problem. We want to use the SARSOP solver to compute a policy. To use a solver from JuliaPOMDP, a problem writer must define a set of functions required by the solver. To see what functions are required by SARSOP, check out its documentation  here .   Let's first define the Tiger POMDP type.  using POMDPs # load the interface\ntype TigerPOMDP  : POMDP{Bool, Int64, Bool} # parametarized inheritance POMDP{state, action, observation}\n    r_listen::Float64 # reward for listening (negative)\n    r_findtiger::Float64 # reward for finding the tiger (negative)\n    r_escapetiger::Float64 # reward for escaping\n    p_listen_correctly::Float64 # probbility that we hear the tiger correctly\n    discount_factor::Float64 # discount factor\nend\nTigerPOMDP() = TigerPOMDP(-1.0, -100.0, 10.0, 0.85, 0.95) # default contructor  Notice that the  TigerPOMDP  is inheriting from the abstract  POMDP  type that comes from POMDPs.jl. The abstract  POMDP  is parametarized by a  Bool ,  Int64 ,  Bool  combination with the syntax  TigerPOMDP  : POMDP{Bool, Int64, Bool} . The parametarization defines how we choose to represent the state, actions, and observations in our problem. In the  TigerPOMDP  we use a boolean to represent our states and observations (because there are two of each) and an integer to represent our actions (because there are 3). If you wanted to create a custom concrete type to represent your states, actions, or observations you could do that as well. Let's say we made a type to represent our states called  AwesomeTigerState . That type could contain integers, floats, arrays, or other complex data structures (depending on what's convenient). We would then parametrize the Tiger POMDP in the following way:  type TigerPOMDP  : POMDP{AwesomeTigerState, Int64, Bool} .  Now, let's consider another important component of POMDPs, probability distributions. In the POMDPs.jl interface, we think in terms of distribution types. We want to be able to sample from these distriubtions and compute their probability masses or densities. In the Tiger POMDP, our distriubtions are over binary variables (boolean state or observation), so we can implement a simple version of a Bernoulli distribution.  type TigerDistribution  : AbstractDistribution # inherits from a POMDPs.jl abstract type\n    p::Float64 # probability of 1\n    it::Vector{Bool} # pre-allocate the domain of the distriubtion\nend\nTigerDistribution() = TigerDistribution(0.5, [true, false]) # default constructo\n\niterator(d::TigerDistribution) = d.it # convenience function used by discrete solvers (iterator over the discrete distriubtion)  Let's implement the pdf and rand function that returns the probability mass and samples from the distribution.  # returns the probability mass \nfunction pdf(d::TigerDistribution, so::Bool)\n    so ? (return d.p) : (return 1.0-d.p)\nend\n\n# samples the dsitribution\nrand(rng::AbstractRNG, d::TigerDistribution, s::Bool) = rand(rng)  = d.p  We also want some convenience functions for initializing the distriubtions.  create_transition_distribution(::TigerPOMDP) = TigerDistribution()\ncreate_observation_distribution(::TigerPOMDP) = TigerDistribution()  Let's define our transition, observation, and reward functions.  function transition(pomdp::TigerPOMDP, s::Bool, a::Int64, d::TigerDistribution=create_transition_distribution(pomdp))\n    # Resets the problem after opening door; does nothing after listening        \n    if a == 1 || a == 2\n        d.p = 0.5\n    elseif s\n        d.p = 1.0\n    else\n        d.p = 0.0\n    end\n    d\nend\n\nfunction observation(pomdp::TigerPOMDP, s::Bool, a::Int64, d::TigerDistribution=create_observation_distribution(pomdp))\n    # correct observation wiht prob pc        \n    pc = pomdp.p_listen_correctly\n    if a == 0\n        s ? (d.p = pc) : (d.p = 1.0-pc)\n    else\n        d.p = 0.5\n    end\n    d\nend\n# convenience function\nfunction observation(pomdp::TigerPOMDP, s::Bool, a::Int64, sp::Bool, d::TigerDistribution=create_observation_distribution(pomdp))\n    return observation(pomdp, s, a, d)\nend\n\nfunction reward(pomdp::TigerPOMDP, s::Bool, a::Int64)\n    # rewarded for escaping, penalized for listening and getting caught\n    r = 0.0\n    a == 0 ? (r+=pomdp.r_listen) : (nothing)\n    if a == 1\n        s ? (r += pomdp.r_findtiger) : (r += pomdp.r_escapetiger)\n    end\n    if a == 2\n        s ? (r += pomdp.r_escapetiger) : (r += pomdp.r_findtiger)\n    end\n    return r\nend\n# convenience function\nreward(pomdp::TigerPOMDP, s::Bool, a::Int64, sp::Bool) = reward(pomdp, s, a)  The last important component of a POMDP are the spaces. There is a special  AbstractSpace  type in POMDPs.jl which all spaces inherit from. We define the state, action, and observation spaces below as well as functions for intializing them and sampling from them.  # STATE SPACE\ntype TigerStateSpace  : AbstractSpace\n    states::Vector{Bool} # states are boolean\nend\n# initialize the state space\nstates(::TigerPOMDP) = TigerStateSpace([true, false])\n# for iterating over discrete spaces\niterator(space::TigerStateSpace) = space.states\ndimensions(::TigerStateSpace) = 1\n# sample from the state sapce\nrand(rng::AbstractRNG, space::TigerStateSpace, s::Bool) = rand(rng)   0.5 ? (return true) : (return false)\n\n# ACTION SPACE\ntype TigerActionSpace  : AbstractSpace\n    actions::Vector{Int64} # three possible actions\nend\n# initialize the action space\nactions(::TigerPOMDP) = TigerActionSpace([0,1,2])\n# iterate of the action space\niterator(space::TigerActionSpace) = space.actions\ndimensions(::TigerActionSpace) = 1\n# sample from the aciton space\nrand(rng::AbstractRNG, space::TigerActionSpace, a::Int64) = rand(rng, 0:2)\n\n# OBSERVATION SPACE\ntype TigerObservationSpace  : AbstractSpace\n    obs::Vector{Bool}\nend\n# initialize\nobservations(::TigerPOMDP) = TigerObservationSpace([true, false])\n# iterate over obs space\niterator(space::TigerObservationSpace) = space.obs\ndimensions(::TigerObservationSpace) = 1\n# sample from the obs sapce\nrand(rng::AbstractRNG, space::TigerObservationSpace, s::Bool) = rand(rng)   0.5 ? (return true) : (return false)  The last important component of a POMDP is the initial distribution over the state space. In POMDPs.jl we make a strong distinction between this distribution and a belief. In most literature these two concepts are considered the same. However, in most general terms, a belief is something that is mapped to an action using a POMDP policy. If the policy is represented as something other than alpha-vectors (a policy graph, tree, or a reccurent neural netowrk to give a few examples), it may not make sense to think of a belief as a probability distribution over the state space. Thus, in POMDPs.jl we abstract the concept of a belief beyond a probability distribution (of course it can be a probability distriubtion if it makes sense).   In order to reconcile this difference, each policy has a function called  initialize_belief  which takes in an initial state distirubtion (this is a probability distribution over the state space) and a policy, and converts the distribution into what we call a belief in POMDPs.jl - a representation of a POMDP that is mapped to an action using the policy.   Let's define the initial state distribution function for our POMDP.  initial_state_distribution(pomdp::TigerPOMDP) = TigerDistribution(0.5, [true, false])  Now that've defined all the main components, we need to wrap up our model by creating some convenience functions below.  # initialization functions\ncreate_state(::TigerPOMDP) = zero(Bool)\ncreate_observation(::TigerPOMDP) = zero(Bool)\ncreate_action(::TigerPOMDP) = zero(Int64)\n\n# for discrete problems\nn_states(::TigerPOMDP) = 2\nn_actions(::TigerPOMDP) = 3\nn_observations(::TigerPOMDP) = 2\n\n# for indexing discrete states\nstate_index(::TigerPOMDP, s::Bool) = Int64(s) + 1\n\ndiscount(pomdp::TigerPOMDP) = pomdp.discount_factor  Now that we've defined all these functions, we can use one of the JuliaPOMDP solvers to compute and evaluate a policy.   using QMDP, POMDPToolbox\n\npomdp = TigerPOMDP()\nsolver = QMDPSolver()\npolicy = solve(solver, pomdp)\n\ninit_dist = initial_state_distribution(pomdp)\nhist = HistoryRecorder(max_steps=100) # from POMDPToolbox\nr = simulate(hist, pomdp, policy, belief_updater, init_dist) # run 100 step simulation  Please note that you do not need to define all the functions for most solvers. If you want to use an individual solver, you usually need only a subset of what's above.", 
            "title": "Functional Form POMDP"
        }, 
        {
            "location": "/def_pomdp/#tabular-form-pomdp", 
            "text": "Another way to define discrete POMDP problems is by writing them in tabular form. Specifically, if you can write the transition and observation probabilities as well as the rewards in matrix form, you can use the  DiscreteMDP  or  DiscretePOMDP  types form  POMDPModels  which automatically implements all the functions you'll need for you. Let's do this with the Tiger POMDP.  using POMDPModels\n\n# write out the matrix forms\n\n# REWARDS\nR = [-1. -100 10; -1 10 -100] # |S|x|A| state-action pair rewards\n\n# TRANSITIONS\nT = zeros(2,3,2) # |S|x|A|x|S|, T[s', a, s] = p(s'|a,s)\nT[:,:,1] = [1. 0.5 0.5; 0 0.5 0.5]\nT[:,:,2] = [0. 0.5 0.5; 1 0.5 0.5]\n\n# OBSERVATIONS\nO = zeros(2,3,2) # |O|x|A|x|S|, O[o, a, s] = p(o|a,s)\nO[:,:,1] = [0.85 0.5 0.5; 0.15 0.5 0.5]\nO[:,:,2] = [0.15 0.5 0.5; 0.85 0.5 0.5]\n\ndiscount = 0.95\npomdp = DiscretePOMDP(T, R, O, discount)\n\n# solve the POMDP the same way\nsolver = SARSOPSolver()\npolicy = solve(solver, pomdp)  It is usually fairly simple to define smaller problems in the tabular form. However, for larger problems it can be tedious and the functional form may be preffered. You can usually use any supported POMDP solver to sovle these types of problems (the performance of the policy may vary however - SARSOP will usually outperform QMDP).", 
            "title": "Tabular Form POMDP"
        }, 
        {
            "location": "/def_pomdp/#continous-pomdp", 
            "text": "Within the POMDPs.jl interface, we can also define problems with continuous spaces.  There are a few solvers that can handle these types of problems, namely, MCVI and POMCP (with some tunning). Light-Dark problem here. What should we say about bounds?", 
            "title": "Continous POMDP"
        }, 
        {
            "location": "/def_solver/", 
            "text": "Defining a Solver\n\n\nIn this section, we will walk through an implementation of the \nQMDP\n algorithm. QMDP is the fully observable approximation of a POMDP policy, and relies on the Q-values to determine actions. \n\n\n\n\nBackground\n\n\nLet's say we are working with a POMDP defined by the tuple $(\\mathcal{S}, \\mathcal{A}, \\mathcal{Z}, T, R, O, \\gamma)$, where $\\mathcal{S}$, $\\mathcal{A}$, $\\mathcal{Z}$ are the of discrete state, action, and observation spaces respectively. The QMDP algorithm assumes that the POMDP its solving is discrete. In our model $T : \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow [0, 1]$ is the transition function, $R: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$ is the reward function, and $O: \\mathcal{Z} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow [0,1]$ is the observation function. In a POMDP, our goal is to compute a policy $\\pi$ that maps beliefs to actions $\\pi: b \\rightarrow a$. For QMDP, a belief can be represented by a discrete probability distribution over the state space (although there may be other ways to define a belief in general and POMDPs.jl allows this flexibility). \n\n\nBefore thinking about how we can compute a policy, lets first think of how we can write the optimal value function for a POMDP. Recall that in an MDP, the optimal value function simply represents the maximum expected utility from a given state. The idea is similar in a POMDP, but now we can think of the optimal value function with respect to a belief, and not just a single state. Since our belief is a probability distribution over the states, we can write the value function as follows:\n\n\n$U^{*}(b) = \\max_{a} \\sum_{s} b(s)R(s,a)$\n\n\nIf we let $\\alpha_{a}$ represent $R(:,a)$ as a vector, and $b$ represent our distribution over state (i.e. belief), then we can write above as\n\n\n$U^{*}(b) = \\max_{a} \\alpha_{a}^{T}b$\n\n\nThe $\\alpha_{a}$ in the equation above is what's often called an alpha vector. These alpha vectors can be though of as compact representations of a POMDP policy. Just as in an MDP, we often want to compute the Q-matrix, in a POMDP, we want to compute these alpha vectors. Note that an alpha vectors can be though of as a part of a piecewise linear and convex approximation to a POMDP value function (which is itself convex). Also note that using $R(:,a)$ as an approximation for an alpha vectors will often give you a very poor approximation. So now that we know that we must compute these alpha vectors, how do we do it?\n\n\n\n\nQMDP Algorithm\n\n\nOne of the simplest algorithms for computing these alpha vectors is known as QMDP. It uses the Q-matrix $Q(s,a)$ obtained by solving the MDP associated with the POMDP, and setting each alpha vector equal to the columns of that matrix $\\alpha_{a} = Q(:, s)$. If you are familiar with the value iteration algorithm for MDPs, the procedure for finding these alpha vectors is identical. Let's first initialize the alpha vectors $\\alpha_{a}^{0} = 0$ for all $s$, and then iterate\n\n\n$\\alpha_{a}^{k+1}(s) = R(s,a) + \\gamma \\sum_{s'} T(s'|s,a) \\max_{a'} \\alpha_{a'}^{k}(s')$\n\n\nAfter enough iterations, the alpha vectors converge to the QMDP approximation. \n\n\nRemember that QMDP is just an approximation method, and does not guarantee that the alpha vectors you obtain actually represent your POMDP value function. Specifically, QMDP has trouble in problems with information gathering actions (because we completely ignored the observation function when computing our policy). However, QMDP works very well in problems where a particular choice of action has little impact on the reduction in state uncertainty. \n\n\n\n\nRequirements for a Solver\n\n\nBefore getting into the implementation details, let's first go through what a POMDP solver must be able to do and support. We need three custom types that inherit from abstract types in POMDPs.jl. These type are Solver, Policy, and Updater. It is usaully useful to have a custom type that represents the belief used by your policy as well. \n\n\nThe requirements are as follows:\n\n\n# types \nQMDPSolver\nQMDPPolicy\nDiscreteUpdater # already implemented for us in POMDPToolbox\nDiscreteBelief # already implemented for us in POMDPToolbox\n# methods\ncreate_policy(solver::QMDPSolver, pomdp::POMDP) # initalizes a QMDP policy\ncreate_belief(up::DiscreteUpdater) # initializes a QMDP belief\nupdater(p::QMDPPolicy) # initializes a QMDP belief udpater\ninitialize_belief(bu::QMDPUpdater, initial_state_dist::AbstractDistribution) # returns a QMDP belief\nsolve(solver::QMDPSolver, pomdp::POMDP) # solver the POMDP and returns a policy\nupdate{A,O}(bu::DiscreteUpdater, belief_old::DiscreteBelief, action::A, obs::O) # returns an updated belied (already implemented)\naction(policy::QMDPPolicy, b::DiscreteBelief) # returns a QMDP action\n\n\n\n\nYou can find the implementations of these types and mehtods below.\n\n\n\n\nDefining the Solver and Policy Types\n\n\nLet's first define the Solver type. The QMDP solver type should contain all the information needed to compute a policy (other than the problem itself). This information can be though of as the hyperparameters of the solver. In QMDP, we only need two hyper-parameters. We may want to set the maximum number of iterations that the algorithm runs for, and a tolerance value (also known as the Bellman residual). Both of these quantities define terminating criteria for the algorithm. The algorithm stops either when the maximum number of iterations has been reached or when the infinity norm of the diference in utility values between two iterations goes below the tolerance value. The type definition has the form: \n\n\nusing POMDPs # first load the POMDPs module\ntype QMDPSolver \n: Solver\n    max_iterations::Int64 # max number of iterations QMDP runs for\n    tolerance::Float64 # Bellman residual: terminates when max||Ut-Ut-1|| \n tolerance\nend\n# default constructor\nQMDPSolver(;max_iterations::Int64=100, tolerance::Float64=1e-3) = QMDPSolver(max_iterations, tolerance)\n\n\n\n\nNote that the QMDPSolver inherits from the abstract Solver type that's part of POMDPs.jl. \n\n\nNow, let's define a policy type. In general, the policy should contain all the information needed to map a belief to an action. As mentioned earlier, we need alpha vectors to be part of our policy. We can represent the alpha vectors using a matrix of size $\\mathcal{S} \\times \\mathcal{A}$. Recall that in POMDPs.jl, the actions can be represented in a number of ways (Int64, concrete types, etc), so we need a way to map these actions to integers so we can index into our alpha matrix. The type looks like:\n\n\ntype QMDPPolicy \n: Policy\n    alphas::Matrix{Float64} # matrix of alpha vectors |S|x|A|\n    action_map::Vector{Any} # maps indices to actions\n    pomdp::POMDP            # models for convenience\nend\n# default constructor\nfunction QMDPPolicy(pomdp::POMDP)\n    ns = n_states(pomdp)\n    na = n_actions(pomdp)\n    alphas = zeros(ns, na)\n    am = Any[]\n    space = actions(pomdp)\n    for a in iterator(space)\n        push!(am, a)\n    end\n    return QMDPPolicy(alphas, am, pomdp)\nend\n# initalization function (required by POMDPs.jl)\nPOMDPs.create_policy(solver::QMDPSolver, pomdp::POMDP) = QMDPPolicy(pomdp)\n\n\n\n\nNow that we have our solver and policy types, we can write the solve function to compute the policy. \n\n\n\n\nWriting the Solve Function\n\n\nThe solve function takes in a solver, a pomdp, and an optional policy argument. Let's compute those alpha vectors!\n\n\nfunction POMDPs.solve(solver::QMDPSolver, pomdp::POMDP, policy::QMDPPolicy=create_policy(solver, pomdp))\n\n    # get solver parameters\n    max_iterations = solver.max_iterations\n    tolerance = solver.tolerance\n    discount_factor = discount(pomdp)\n\n    # intialize the alpha-vectors\n    alphas = policy.alphas\n\n    # pre-allocate the transtion distirbution and the interpolants\n    # we use the POMDPs.jl function for initializing a transition distribution    \n    dist = create_transition_distribution(pomdp)\n\n    # initalize space\n    sspace = states(pomdp)  # returns a discrete state space object of the pomdp\n    aspace = actions(pomdp) # returns a discrete action space object\n\n    # main loop\n    for i = 1:max_iterations\n        residual = 0.0\n        # state loop\n        # the iterator function returns an iterable object (array, iterator, etc) over a discrete space\n        for (istate, s) in enumerate(iterator(sspace))\n            old_alpha = maximum(alphas[istate,:]) # for residual \n            max_alpha = -Inf\n            # action loop\n            # alpha(s) = R(s,a) + discount_factor * sum(T(s'|s,a)max(alpha(s'))\n            for (iaction, a) in enumerate(iterator(aspace))\n                # the transition function modifies the dist argument to a distribution availible from that state-action pair\n                dist = transition(pomdp, s, a, dist) # fills distribution over neighbors\n                q_new = 0.0\n                for sp in iterator(sspace)\n                    # pdf returns the probability mass of sp in dist\n                    p = pdf(dist, sp)\n                    p == 0.0 ? continue : nothing # skip if zero prob\n                    # returns the reward from s-a-sp triple\n                    r = reward(pomdp, s, a, sp)\n\n                    # state_index returns an integer \n                    sidx = state_index(pomdp, sp)\n                    q_new += p * (r + discount_factor * maximum(alphas[sidx,:]))\n                end\n                new_alpha = q_new\n                alphas[istate, iaction] = new_alpha\n                new_alpha \n max_alpha ? (max_alpha = new_alpha) : nothing\n            end # actiom\n            # update the value array\n            diff = abs(max_alpha - old_alpha)\n            diff \n residual ? (residual = diff) : nothing\n        end # state\n        # check if below Bellman residual      \n        residual \n tolerance ? break : nothing \n    end # main\n    # return the policy\n    policy\nend\n\n\n\n\nAt each iteration, the algorithm iterates over the state space and computes an alpha vector for each action. There is a check at the end to see if the Bellman residual has been statisfied. The solve function assumes the following POMDPs.jl functions are implemented by the user of QMDP:\n\n\ncreate_transition_distribution(pomdp) # initializes a transition distribution that we can sample and call pdf on\nstates(pomdp) # returns a state space object of the pomdp\nactions(pomdp) # returns the action space object of the pomdp\niterator(space) # returns an iterable object (array or iterator), used for discrete spaces only\ntransition(pomdp, s, a, dist) # modifies and returns dist (optional argument) to be the transition distribution for the s, a pair\nreward(pomdp, s, a, sp) # returns real valued reward from s, a, sp triple\npdf(dist, sp) # returns the probability of sp being in dist\nstate_index(pomdp, sp) # returns the integer index of sp (for discrete state spaces)\n\n\n\n\nNow that we have a solve function, let's see users can interface with our policy.\n\n\n\n\nCreating an Updater\n\n\nLet's now talk about how we deal with beliefs. Since QMDP is a discrete POMDP solver, we can assume that the user will represent their belief as a probaiblity distribution over states. That means that we can also use a discrete belief to work with our policy! Lucky for us, the JuliaPOMDP organization contains tools that we can use out of the box for working with discrete beliefs. The POMDPToolbox package conatins a DiscreteBelief type that does exactly what we need. Let's define the helper functions the deal with beliefs and updaters:\n\n\nusing POMDPToolbox # remeber to load the package that implements discrete beliefs for us\nPOMDPs.create_belief(bu::DiscreteUpdater) = DiscreteBelief(n_states(bu.du.pomdp)) # initializes a QMDP belief\nPOMDPs.updater(p::QMDPPolicy) = DiscreteUpdater(p.pomdp) # initialize the QMDP updater\n\n\n\n\nNow we need a function that turns the initial distribution over state of the POMDP to our discrete belief.\n\n\nfunction POMDPs.initialize_belief(bu::DiscreteUpdater, initial_state_dist::AbstractDistribution, new_belief::QMDPBelief=create_belief(bu))\n    pomdp = bu.du.pomdp\n    for (si, s) in enumerate(iterator(states(pomdp)))\n        new_belief.b[si] = pdf(initial_state_dist, s) # DiscreteBelief has a field called b which is an array of probabilities\n    end\n    return new_belief\nend\n\n\n\n\nThe function above assumes that the \ninitial_state_dist\n is a distribution that implements a pdf function. \n\n\nLastly, let's define the action function which maps the belief to an action using the QMDP policy. \n\n\nfunction POMDPs.action(policy::QMDPPolicy, b::QMDPBelief)\n    alphas = policy.alphas\n    ihi = 0\n    vhi = -Inf\n    (ns, na) = size(alphas)\n    @assert length(b.b) == ns \nLength of belief and alpha-vector size mismatch\n\n    # see which action gives the highest util value\n    for ai = 1:na\n        util = dot(alphas[:,ai], b.b)    \n        if util \n vhi\n            vhi = util\n            ihi = ai\n        end\n    end\n    # map the index to action\n    return policy.action_map[ihi]\nend\n\n\n\n\nThese are all the functions that you'll need to have a working POMDPs.jl solver. Let's now use existing benchmark models to evaluate it.\n\n\n\n\nEvaluating the Solver\n\n\nWe'll use the POMDPModels package from JuliaPOMDP to initialize a Tiger POMDP problem and solve it with QMDP.\n\n\nusing POMDPModels\n\n# initialize model and solver\npomdp = TigerPOMDP()\nsolver = QMDPSolver()\n\n# compute the QMDP policy\npolicy = solve(solver, pomdp)\n\n# initalize updater and belief\nb_up = updater(policy)\nb = initialize_belief(b_up, initial_state_dist(pomdp)\n\n# create a simulator object for recording histories\nsim_hist = HistoryRecorder(max_steps=100)\n\n# run a simulation\nr = simulate(sim_hist, pomdp, policy, b_up, b)\n\n\n\n\nThat's all you need to define a solver and evaluate its performance!", 
            "title": "Defining a Solver"
        }, 
        {
            "location": "/def_solver/#defining-a-solver", 
            "text": "In this section, we will walk through an implementation of the  QMDP  algorithm. QMDP is the fully observable approximation of a POMDP policy, and relies on the Q-values to determine actions.", 
            "title": "Defining a Solver"
        }, 
        {
            "location": "/def_solver/#background", 
            "text": "Let's say we are working with a POMDP defined by the tuple $(\\mathcal{S}, \\mathcal{A}, \\mathcal{Z}, T, R, O, \\gamma)$, where $\\mathcal{S}$, $\\mathcal{A}$, $\\mathcal{Z}$ are the of discrete state, action, and observation spaces respectively. The QMDP algorithm assumes that the POMDP its solving is discrete. In our model $T : \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow [0, 1]$ is the transition function, $R: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$ is the reward function, and $O: \\mathcal{Z} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow [0,1]$ is the observation function. In a POMDP, our goal is to compute a policy $\\pi$ that maps beliefs to actions $\\pi: b \\rightarrow a$. For QMDP, a belief can be represented by a discrete probability distribution over the state space (although there may be other ways to define a belief in general and POMDPs.jl allows this flexibility).   Before thinking about how we can compute a policy, lets first think of how we can write the optimal value function for a POMDP. Recall that in an MDP, the optimal value function simply represents the maximum expected utility from a given state. The idea is similar in a POMDP, but now we can think of the optimal value function with respect to a belief, and not just a single state. Since our belief is a probability distribution over the states, we can write the value function as follows:  $U^{*}(b) = \\max_{a} \\sum_{s} b(s)R(s,a)$  If we let $\\alpha_{a}$ represent $R(:,a)$ as a vector, and $b$ represent our distribution over state (i.e. belief), then we can write above as  $U^{*}(b) = \\max_{a} \\alpha_{a}^{T}b$  The $\\alpha_{a}$ in the equation above is what's often called an alpha vector. These alpha vectors can be though of as compact representations of a POMDP policy. Just as in an MDP, we often want to compute the Q-matrix, in a POMDP, we want to compute these alpha vectors. Note that an alpha vectors can be though of as a part of a piecewise linear and convex approximation to a POMDP value function (which is itself convex). Also note that using $R(:,a)$ as an approximation for an alpha vectors will often give you a very poor approximation. So now that we know that we must compute these alpha vectors, how do we do it?", 
            "title": "Background"
        }, 
        {
            "location": "/def_solver/#qmdp-algorithm", 
            "text": "One of the simplest algorithms for computing these alpha vectors is known as QMDP. It uses the Q-matrix $Q(s,a)$ obtained by solving the MDP associated with the POMDP, and setting each alpha vector equal to the columns of that matrix $\\alpha_{a} = Q(:, s)$. If you are familiar with the value iteration algorithm for MDPs, the procedure for finding these alpha vectors is identical. Let's first initialize the alpha vectors $\\alpha_{a}^{0} = 0$ for all $s$, and then iterate  $\\alpha_{a}^{k+1}(s) = R(s,a) + \\gamma \\sum_{s'} T(s'|s,a) \\max_{a'} \\alpha_{a'}^{k}(s')$  After enough iterations, the alpha vectors converge to the QMDP approximation.   Remember that QMDP is just an approximation method, and does not guarantee that the alpha vectors you obtain actually represent your POMDP value function. Specifically, QMDP has trouble in problems with information gathering actions (because we completely ignored the observation function when computing our policy). However, QMDP works very well in problems where a particular choice of action has little impact on the reduction in state uncertainty.", 
            "title": "QMDP Algorithm"
        }, 
        {
            "location": "/def_solver/#requirements-for-a-solver", 
            "text": "Before getting into the implementation details, let's first go through what a POMDP solver must be able to do and support. We need three custom types that inherit from abstract types in POMDPs.jl. These type are Solver, Policy, and Updater. It is usaully useful to have a custom type that represents the belief used by your policy as well.   The requirements are as follows:  # types \nQMDPSolver\nQMDPPolicy\nDiscreteUpdater # already implemented for us in POMDPToolbox\nDiscreteBelief # already implemented for us in POMDPToolbox\n# methods\ncreate_policy(solver::QMDPSolver, pomdp::POMDP) # initalizes a QMDP policy\ncreate_belief(up::DiscreteUpdater) # initializes a QMDP belief\nupdater(p::QMDPPolicy) # initializes a QMDP belief udpater\ninitialize_belief(bu::QMDPUpdater, initial_state_dist::AbstractDistribution) # returns a QMDP belief\nsolve(solver::QMDPSolver, pomdp::POMDP) # solver the POMDP and returns a policy\nupdate{A,O}(bu::DiscreteUpdater, belief_old::DiscreteBelief, action::A, obs::O) # returns an updated belied (already implemented)\naction(policy::QMDPPolicy, b::DiscreteBelief) # returns a QMDP action  You can find the implementations of these types and mehtods below.", 
            "title": "Requirements for a Solver"
        }, 
        {
            "location": "/def_solver/#defining-the-solver-and-policy-types", 
            "text": "Let's first define the Solver type. The QMDP solver type should contain all the information needed to compute a policy (other than the problem itself). This information can be though of as the hyperparameters of the solver. In QMDP, we only need two hyper-parameters. We may want to set the maximum number of iterations that the algorithm runs for, and a tolerance value (also known as the Bellman residual). Both of these quantities define terminating criteria for the algorithm. The algorithm stops either when the maximum number of iterations has been reached or when the infinity norm of the diference in utility values between two iterations goes below the tolerance value. The type definition has the form:   using POMDPs # first load the POMDPs module\ntype QMDPSolver  : Solver\n    max_iterations::Int64 # max number of iterations QMDP runs for\n    tolerance::Float64 # Bellman residual: terminates when max||Ut-Ut-1||   tolerance\nend\n# default constructor\nQMDPSolver(;max_iterations::Int64=100, tolerance::Float64=1e-3) = QMDPSolver(max_iterations, tolerance)  Note that the QMDPSolver inherits from the abstract Solver type that's part of POMDPs.jl.   Now, let's define a policy type. In general, the policy should contain all the information needed to map a belief to an action. As mentioned earlier, we need alpha vectors to be part of our policy. We can represent the alpha vectors using a matrix of size $\\mathcal{S} \\times \\mathcal{A}$. Recall that in POMDPs.jl, the actions can be represented in a number of ways (Int64, concrete types, etc), so we need a way to map these actions to integers so we can index into our alpha matrix. The type looks like:  type QMDPPolicy  : Policy\n    alphas::Matrix{Float64} # matrix of alpha vectors |S|x|A|\n    action_map::Vector{Any} # maps indices to actions\n    pomdp::POMDP            # models for convenience\nend\n# default constructor\nfunction QMDPPolicy(pomdp::POMDP)\n    ns = n_states(pomdp)\n    na = n_actions(pomdp)\n    alphas = zeros(ns, na)\n    am = Any[]\n    space = actions(pomdp)\n    for a in iterator(space)\n        push!(am, a)\n    end\n    return QMDPPolicy(alphas, am, pomdp)\nend\n# initalization function (required by POMDPs.jl)\nPOMDPs.create_policy(solver::QMDPSolver, pomdp::POMDP) = QMDPPolicy(pomdp)  Now that we have our solver and policy types, we can write the solve function to compute the policy.", 
            "title": "Defining the Solver and Policy Types"
        }, 
        {
            "location": "/def_solver/#writing-the-solve-function", 
            "text": "The solve function takes in a solver, a pomdp, and an optional policy argument. Let's compute those alpha vectors!  function POMDPs.solve(solver::QMDPSolver, pomdp::POMDP, policy::QMDPPolicy=create_policy(solver, pomdp))\n\n    # get solver parameters\n    max_iterations = solver.max_iterations\n    tolerance = solver.tolerance\n    discount_factor = discount(pomdp)\n\n    # intialize the alpha-vectors\n    alphas = policy.alphas\n\n    # pre-allocate the transtion distirbution and the interpolants\n    # we use the POMDPs.jl function for initializing a transition distribution    \n    dist = create_transition_distribution(pomdp)\n\n    # initalize space\n    sspace = states(pomdp)  # returns a discrete state space object of the pomdp\n    aspace = actions(pomdp) # returns a discrete action space object\n\n    # main loop\n    for i = 1:max_iterations\n        residual = 0.0\n        # state loop\n        # the iterator function returns an iterable object (array, iterator, etc) over a discrete space\n        for (istate, s) in enumerate(iterator(sspace))\n            old_alpha = maximum(alphas[istate,:]) # for residual \n            max_alpha = -Inf\n            # action loop\n            # alpha(s) = R(s,a) + discount_factor * sum(T(s'|s,a)max(alpha(s'))\n            for (iaction, a) in enumerate(iterator(aspace))\n                # the transition function modifies the dist argument to a distribution availible from that state-action pair\n                dist = transition(pomdp, s, a, dist) # fills distribution over neighbors\n                q_new = 0.0\n                for sp in iterator(sspace)\n                    # pdf returns the probability mass of sp in dist\n                    p = pdf(dist, sp)\n                    p == 0.0 ? continue : nothing # skip if zero prob\n                    # returns the reward from s-a-sp triple\n                    r = reward(pomdp, s, a, sp)\n\n                    # state_index returns an integer \n                    sidx = state_index(pomdp, sp)\n                    q_new += p * (r + discount_factor * maximum(alphas[sidx,:]))\n                end\n                new_alpha = q_new\n                alphas[istate, iaction] = new_alpha\n                new_alpha   max_alpha ? (max_alpha = new_alpha) : nothing\n            end # actiom\n            # update the value array\n            diff = abs(max_alpha - old_alpha)\n            diff   residual ? (residual = diff) : nothing\n        end # state\n        # check if below Bellman residual      \n        residual   tolerance ? break : nothing \n    end # main\n    # return the policy\n    policy\nend  At each iteration, the algorithm iterates over the state space and computes an alpha vector for each action. There is a check at the end to see if the Bellman residual has been statisfied. The solve function assumes the following POMDPs.jl functions are implemented by the user of QMDP:  create_transition_distribution(pomdp) # initializes a transition distribution that we can sample and call pdf on\nstates(pomdp) # returns a state space object of the pomdp\nactions(pomdp) # returns the action space object of the pomdp\niterator(space) # returns an iterable object (array or iterator), used for discrete spaces only\ntransition(pomdp, s, a, dist) # modifies and returns dist (optional argument) to be the transition distribution for the s, a pair\nreward(pomdp, s, a, sp) # returns real valued reward from s, a, sp triple\npdf(dist, sp) # returns the probability of sp being in dist\nstate_index(pomdp, sp) # returns the integer index of sp (for discrete state spaces)  Now that we have a solve function, let's see users can interface with our policy.", 
            "title": "Writing the Solve Function"
        }, 
        {
            "location": "/def_solver/#creating-an-updater", 
            "text": "Let's now talk about how we deal with beliefs. Since QMDP is a discrete POMDP solver, we can assume that the user will represent their belief as a probaiblity distribution over states. That means that we can also use a discrete belief to work with our policy! Lucky for us, the JuliaPOMDP organization contains tools that we can use out of the box for working with discrete beliefs. The POMDPToolbox package conatins a DiscreteBelief type that does exactly what we need. Let's define the helper functions the deal with beliefs and updaters:  using POMDPToolbox # remeber to load the package that implements discrete beliefs for us\nPOMDPs.create_belief(bu::DiscreteUpdater) = DiscreteBelief(n_states(bu.du.pomdp)) # initializes a QMDP belief\nPOMDPs.updater(p::QMDPPolicy) = DiscreteUpdater(p.pomdp) # initialize the QMDP updater  Now we need a function that turns the initial distribution over state of the POMDP to our discrete belief.  function POMDPs.initialize_belief(bu::DiscreteUpdater, initial_state_dist::AbstractDistribution, new_belief::QMDPBelief=create_belief(bu))\n    pomdp = bu.du.pomdp\n    for (si, s) in enumerate(iterator(states(pomdp)))\n        new_belief.b[si] = pdf(initial_state_dist, s) # DiscreteBelief has a field called b which is an array of probabilities\n    end\n    return new_belief\nend  The function above assumes that the  initial_state_dist  is a distribution that implements a pdf function.   Lastly, let's define the action function which maps the belief to an action using the QMDP policy.   function POMDPs.action(policy::QMDPPolicy, b::QMDPBelief)\n    alphas = policy.alphas\n    ihi = 0\n    vhi = -Inf\n    (ns, na) = size(alphas)\n    @assert length(b.b) == ns  Length of belief and alpha-vector size mismatch \n    # see which action gives the highest util value\n    for ai = 1:na\n        util = dot(alphas[:,ai], b.b)    \n        if util   vhi\n            vhi = util\n            ihi = ai\n        end\n    end\n    # map the index to action\n    return policy.action_map[ihi]\nend  These are all the functions that you'll need to have a working POMDPs.jl solver. Let's now use existing benchmark models to evaluate it.", 
            "title": "Creating an Updater"
        }, 
        {
            "location": "/def_solver/#evaluating-the-solver", 
            "text": "We'll use the POMDPModels package from JuliaPOMDP to initialize a Tiger POMDP problem and solve it with QMDP.  using POMDPModels\n\n# initialize model and solver\npomdp = TigerPOMDP()\nsolver = QMDPSolver()\n\n# compute the QMDP policy\npolicy = solve(solver, pomdp)\n\n# initalize updater and belief\nb_up = updater(policy)\nb = initialize_belief(b_up, initial_state_dist(pomdp)\n\n# create a simulator object for recording histories\nsim_hist = HistoryRecorder(max_steps=100)\n\n# run a simulation\nr = simulate(sim_hist, pomdp, policy, b_up, b)  That's all you need to define a solver and evaluate its performance!", 
            "title": "Evaluating the Solver"
        }, 
        {
            "location": "/api/", 
            "text": "API Documentation\n\n\nDocumentation for the \nPOMDPs.jl\n user interface. You can get help for any type or function in the module by typing \n?\n in the Julia REPL followed by the name of type or function. For example:\n\n\njulia\nusing POMDPs\njulia\n?\nhelp?\nreward\nsearch: reward\n\n  reward{S,A,O}(pomdp::POMDP{S,A,O}, state::S, action::A, statep::S)\n\n  Returns the immediate reward for the s-a-s triple\n\n  reward{S,A,O}(pomdp::POMDP{S,A,O}, state::S, action::A)\n\n  Returns the immediate reward for the s-a pair\n\n\n\n\n\n\n\nContents\n\n\n\n\nAPI Documentation\n\n\nContents\n\n\nIndex\n\n\nTypes\n\n\nModel Functions\n\n\nDistribution/Space Functions\n\n\nBelief Functions\n\n\nPolicy and Solver Functions\n\n\nSimulator\n\n\nUtility Tools\n\n\nConstants\n\n\n\n\n\n\n\n\n\n\nIndex\n\n\n\n\nPOMDPs.REMOTE_URL\n\n\nPOMDPs.SUPPORTED_PACKAGES\n\n\nPOMDPs.AbstractDistribution\n\n\nPOMDPs.AbstractSpace\n\n\nPOMDPs.MDP\n\n\nPOMDPs.POMDP\n\n\nPOMDPs.Policy\n\n\nPOMDPs.Simulator\n\n\nPOMDPs.Solver\n\n\nPOMDPs.Updater\n\n\nBase.Random.rand\n\n\nPOMDPs.action\n\n\nPOMDPs.action_index\n\n\nPOMDPs.actions\n\n\nPOMDPs.add\n\n\nPOMDPs.add_all\n\n\nPOMDPs.available\n\n\nPOMDPs.create_action\n\n\nPOMDPs.create_belief\n\n\nPOMDPs.create_observation\n\n\nPOMDPs.create_observation_distribution\n\n\nPOMDPs.create_policy\n\n\nPOMDPs.create_state\n\n\nPOMDPs.create_transition_distribution\n\n\nPOMDPs.dimensions\n\n\nPOMDPs.discount\n\n\nPOMDPs.initial_state_distribution\n\n\nPOMDPs.initialize_belief\n\n\nPOMDPs.isterminal\n\n\nPOMDPs.isterminal_obs\n\n\nPOMDPs.iterator\n\n\nPOMDPs.n_actions\n\n\nPOMDPs.n_observations\n\n\nPOMDPs.n_states\n\n\nPOMDPs.obs_index\n\n\nPOMDPs.observation\n\n\nPOMDPs.observations\n\n\nPOMDPs.pdf\n\n\nPOMDPs.reward\n\n\nPOMDPs.simulate\n\n\nPOMDPs.solve\n\n\nPOMDPs.state_index\n\n\nPOMDPs.states\n\n\nPOMDPs.strip_arg\n\n\nPOMDPs.test_all\n\n\nPOMDPs.transition\n\n\nPOMDPs.update\n\n\nPOMDPs.updater\n\n\nPOMDPs.value\n\n\nPOMDPs.@pomdp_func\n\n\n\n\n\n\nTypes\n\n\n#\n\n\nPOMDPs.POMDP\n \n \nType\n.\n\n\nAbstract base type for a partially observable Markov decision process.\n\n\nS: state type\nA: action type\nO: observation type\n\n\n\n\n#\n\n\nPOMDPs.MDP\n \n \nType\n.\n\n\nAbstract base type for a fully observable Markov decision process.\n\n\nS: state type\nA: action type\n\n\n\n\n#\n\n\nPOMDPs.AbstractSpace\n \n \nType\n.\n\n\nBase type for state, action and observation spaces.\n\n\nT: type that parametrizes the space (state, action, or observation)\n\n\n\n\n#\n\n\nPOMDPs.AbstractDistribution\n \n \nType\n.\n\n\nAbstract type for a probability distribution.\n\n\nT: type over which distribution is over (state, action, or observation)\n\n\n\n\n#\n\n\nPOMDPs.Solver\n \n \nType\n.\n\n\nBase type for an MDP/POMDP solver\n\n\n#\n\n\nPOMDPs.Policy\n \n \nType\n.\n\n\nBase type for a policy (a map from every possible belief, or more abstract policy state, to an optimal or suboptimal action)\n\n\nB: a belief (or policy state) that represents the knowledge an agent has about the state of the system\n\n\n\n\n#\n\n\nPOMDPs.Updater\n \n \nType\n.\n\n\nAbstract type for an object that defines how the belief should be updated\n\n\nB: belief type that parametrizes the updater\n\n\n\n\nA belief is a general construct that represents the knowledge an agent has about the state of the system. This can be a probability distribution, an action observation history or a more general representation. \n\n\n\n\nModel Functions\n\n\n#\n\n\nPOMDPs.states\n \n \nFunction\n.\n\n\nstates{S,A,O}(problem::POMDP{S,A,O}, state::S)\nstates{S,A}(problem::MDP{S,A}, state::S)\n\n\n\n\nReturns a subset of the state space reachable from \nstate\n. \n\n\nstates(problem::POMDP)\nstates(problem::MDP)\n\n\n\n\nReturns the complete state space of a POMDP. \n\n\n#\n\n\nPOMDPs.actions\n \n \nFunction\n.\n\n\nactions{S,A,O}(problem::POMDP{S,A,O}, state::S, aspace::AbstractSpace{A})\nactions{S,A}(problem::MDP{S,A}, state::S, aspace::AbstractSpace{A})\n\n\n\n\nModifies \naspace\n to the action space accessible from the given state and returns it.\n\n\nactions(problem::POMDP)\nactions(problem::MDP)\n\n\n\n\nReturns the entire action space of a POMDP.\n\n\nactions{S,A,O,B}(problem::POMDP{S,A,O}, belief::B, aspace::AbstractSpace{A})\n\n\n\n\nModifies \naspace\n to the action space accessible from the states with nonzero belief and returns it.\n\n\n#\n\n\nPOMDPs.observations\n \n \nFunction\n.\n\n\nobservations{S,A,O}(problem::POMDP{S,A,O}, state::S, obs::AbstractSpace{O}=observations(problem))\n\n\n\n\nModifies \nobs\n to the observation space accessible from the given state and returns it.\n\n\nobservations(problem::POMDP)\n\n\n\n\nReturns the entire observation space.\n\n\n#\n\n\nPOMDPs.reward\n \n \nFunction\n.\n\n\nreward{S,A,O}(problem::POMDP{S,A,O}, state::S, action::A, statep::S)\nreward{S,A}(problem::MDP{S,A}, state::S, action::A, statep::S)\n\n\n\n\nReturns the immediate reward for the s-a-s' triple\n\n\nreward{S,A,O}(problem::POMDP{S,A,O}, state::S, action::A)\nreward{S,A}(problem::MDP{S,A}, state::S, action::A)\n\n\n\n\nReturns the immediate reward for the s-a pair\n\n\n#\n\n\nPOMDPs.transition\n \n \nFunction\n.\n\n\ntransition{S,A,O}(problem::POMDP{S,A,O}, state::S, action::A,\n\n\n\n\ndistribution::AbstractDistribution{S}=create_transition_distribution(problem))     transition{S,A}(problem::MDP{S,A}, state::S, action::A, distribution::AbstractDistribution{S}=create_transition_distribution(problem))\n\n\nReturns the transition distribution from the current state-action pair\n\n\n#\n\n\nPOMDPs.observation\n \n \nFunction\n.\n\n\nobservation{S,A,O}(problem::POMDP{S,A,O}, state::S, action::A, statep::S, distribution::AbstractDistribution{O}=create_observation_distribution(problem))\n\n\n\n\nReturns the observation distribution for the s-a-s' tuple (state, action, and next state)\n\n\nobservation{S,A,O}(problem::POMDP{S,A,O}, action::A, statep::S, distribution::AbstractDistribution{O}=create_observation_distribution(problem))\n\n\n\n\nModifies distribution to the observation distribution for the a-s' tuple (action and next state) and returns it\n\n\n#\n\n\nPOMDPs.isterminal\n \n \nFunction\n.\n\n\nisterminal{S,A,O}(problem::POMDP{S,A,O}, state::S)\nisterminal{S,A}(problem::MDP{S,A}, state::S)\n\n\n\n\nChecks if state s is terminal\n\n\n#\n\n\nPOMDPs.isterminal_obs\n \n \nFunction\n.\n\n\nisterminal_obs{S,A,O}(problem::POMDP{S,A,O}, observation::O)\n\n\n\n\nChecks if an observation is terminal.\n\n\n#\n\n\nPOMDPs.discount\n \n \nFunction\n.\n\n\ndiscount(problem::POMDP)\ndiscount(problem::MDP)\n\n\n\n\nReturn the discount factor for the problem.\n\n\n#\n\n\nPOMDPs.n_states\n \n \nFunction\n.\n\n\nn_states(problem::POMDP)\nn_states(problem::MDP)\n\n\n\n\nReturns the number of states in \nproblem\n. Used for discrete models only.\n\n\n#\n\n\nPOMDPs.n_actions\n \n \nFunction\n.\n\n\nn_actions(problem::POMDP)\nn_actions(problem::MDP)\n\n\n\n\nReturns the number of actions in \nproblem\n. Used for discrete models only.\n\n\n#\n\n\nPOMDPs.n_observations\n \n \nFunction\n.\n\n\nn_observations(problem::POMDP)\n\n\n\n\nReturns the number of actions in \nproblem\n. Used for discrete models only.\n\n\n#\n\n\nPOMDPs.state_index\n \n \nFunction\n.\n\n\nstate_index{S,A,O}(problem::POMDP{S,A,O}, s::S)\nstate_index{S,A}(problem::MDP{S,A}, s::S)\n\n\n\n\nReturns the integer index of state \ns\n. Used for discrete models only.\n\n\n#\n\n\nPOMDPs.action_index\n \n \nFunction\n.\n\n\naction_index{S,A,O}(problem::POMDP{S,A,O}, a::A)\naction_index{S,A}(problem::MDP{S,A}, a::A)\n\n\n\n\nReturns the integer index of action \na\n. Used for discrete models only.\n\n\n#\n\n\nPOMDPs.obs_index\n \n \nFunction\n.\n\n\nobs_index{S,A,O}(problem::POMDP{S,A,O}, o::O)\n\n\n\n\nReturns the integer index of observation \no\n. Used for discrete models only.\n\n\n#\n\n\nPOMDPs.create_state\n \n \nFunction\n.\n\n\ncreate_state(problem::POMDP)\ncreate_state(problem::MDP)\n\n\n\n\nCreate a state object (for preallocation purposes).\n\n\n#\n\n\nPOMDPs.create_action\n \n \nFunction\n.\n\n\ncreate_action(problem::POMDP)\ncreate_action(problem::MDP)\n\n\n\n\nCreates an action object (for preallocation purposes)\n\n\n#\n\n\nPOMDPs.create_observation\n \n \nFunction\n.\n\n\ncreate_observation(problem::POMDP)\n\n\n\n\nCreate an observation object (for preallocation purposes).\n\n\n\n\nDistribution/Space Functions\n\n\n#\n\n\nBase.Random.rand\n \n \nFunction\n.\n\n\nrand{T}(rng::AbstractRNG, d::AbstractSpace{T}, sample::T)\n\n\n\n\nReturns a random \nsample\n from space \ns\n.\n\n\nrand{T}(rng::AbstractRNG, d::AbstractDistribution{T}, sample::T)\n\n\n\n\nFill \nsample\n with a random element from distribution \nd\n. The sample can be a state, action or observation.\n\n\n#\n\n\nPOMDPs.pdf\n \n \nFunction\n.\n\n\npdf{T}(d::AbstractDistribution{T}, x::T)\n\n\n\n\nValue of probability distribution \nd\n function at sample \nx\n.\n\n\n#\n\n\nPOMDPs.dimensions\n \n \nFunction\n.\n\n\ndimensions{T}(s::AbstractSpace{T})\n\n\n\n\nReturns the number of dimensions in space \ns\n.\n\n\n#\n\n\nPOMDPs.iterator\n \n \nFunction\n.\n\n\niterator{T}(s::AbstractSpace{T})\n\n\n\n\nReturns an iterable type (array or custom iterator) corresponding to space \ns\n. \n\n\niterator{T}(d::AbstractDistribution{T})\n\n\n\n\nReturns an iterable type (array or custom iterator) corresponding to distribution \nd\n. \n\n\n#\n\n\nPOMDPs.initial_state_distribution\n \n \nFunction\n.\n\n\ninitial_state_distribution(pomdp::POMDP)\n\n\n\n\nReturns an initial belief for the pomdp.\n\n\n#\n\n\nPOMDPs.create_transition_distribution\n \n \nFunction\n.\n\n\ncreate_transition_distribution(problem::POMDP)\ncreate_transition_distribution(problem::MDP)\n\n\n\n\nReturns a transition distribution (for memory preallocation).\n\n\n#\n\n\nPOMDPs.create_observation_distribution\n \n \nFunction\n.\n\n\ncreate_observation_distribution(problem::POMDP)\ncreate_observation_distribution(problem::MDP)\n\n\n\n\nReturns an observation distribution (for memory preallocation).\n\n\n\n\nBelief Functions\n\n\n#\n\n\nPOMDPs.update\n \n \nFunction\n.\n\n\nupdate{B,A,O}(updater::Updater, belief_old::B, action::A, obs::O,\n              belief_new::B=create_belief(updater))\n\n\n\n\nReturns a new instance of an updated belief given \nbelief_old\n and the latest action and observation.\n\n\nupdate()\n\n\n\n\nUpdates all the installed packages\n\n\n#\n\n\nPOMDPs.create_belief\n \n \nFunction\n.\n\n\ncreate_belief(updater::Updater)\n\n\n\n\nCreates a belief object of the type used by \nupdater\n (preallocates memory)\n\n\ncreate_belief(pomdp::POMDP)\n\n\n\n\nCreates a belief either to be used by updater or pomdp\n\n\n#\n\n\nPOMDPs.initialize_belief\n \n \nFunction\n.\n\n\ninitialize_belief{B}(updater::Updater{B}, \n                     state_distribution::AbstractDistribution,\n                     new_belief::B=create_belief(updater))\ninitialize_belief{B}(updater::Updater{B},\n                     belief::Any,\n                     new_belief::B=create_belief(updater))\n\n\n\n\nReturns a belief that can be updated using \nupdater\n that has similar distribution to \nstate_distribution\n or \nbelief\n.\n\n\nThe conversion may be lossy. This function is also idempotent, i.e. there is a default implementation that passes the belief through when it is already the correct type: \ninitialize_belief{B}(updater::Updater{B}, belief::B) = belief\n\n\n\n\nPolicy and Solver Functions\n\n\n#\n\n\nPOMDPs.create_policy\n \n \nFunction\n.\n\n\ncreate_policy(solver::Solver, problem::POMDP)\ncreate_policy(solver::Solver, problem::MDP)\n\n\n\n\nCreates a policy object (for preallocation purposes)\n\n\n#\n\n\nPOMDPs.solve\n \n \nFunction\n.\n\n\nsolve(solver::Solver, problem::POMDP, policy=create_policy(solver, problem))\n\n\n\n\nSolves the POMDP using method associated with solver, and returns a policy. \n\n\n#\n\n\nPOMDPs.updater\n \n \nFunction\n.\n\n\nupdater(policy::Policy)\n\n\n\n\nReturns a default Updater appropriate for a belief type that policy \np\n can use\n\n\n#\n\n\nPOMDPs.action\n \n \nFunction\n.\n\n\naction{B}(p::Policy, x::B, action)\n\n\n\n\nFills and returns action based on the current state or belief, given the policy. \nB\n is a generalized information state - can be a state in an MDP, a distribution in POMDP, or any other representation needed to make a decision using the given policy. \n\n\naction{B}(policy::Policy, x::B)\n\n\n\n\nReturns an action for the current state or belief, given the policy\n\n\nIf an MDP is being simulated, \nx\n will be a state; if a POMDP is being simulated, \nx\n will be a belief\n\n\n#\n\n\nPOMDPs.value\n \n \nFunction\n.\n\n\nvalue{B}(p::Policy, x::B)\n\n\n\n\nReturns the utility value from policy \np\n given the state\n\n\n\n\nSimulator\n\n\n#\n\n\nPOMDPs.Simulator\n \n \nType\n.\n\n\nBase type for an object defining how a simulation should be carried out\n\n\n#\n\n\nPOMDPs.simulate\n \n \nFunction\n.\n\n\nsimulate{S,A}(simulator::Simulator, problem::MDP{S,A}, policy::Policy, initial_state::S)\n\n\n\n\nRun a simulation using the specified policy and returns the accumulated reward\n\n\nsimulate{S,A,O,B}(simulator::Simulator, problem::POMDP{S,A,O}, policy::Policy{B}, updater::Updater{B}, initial_belief::Union{B,AbstractDistribution{S}})\n\n\n\n\nRun a simulation using the specified policy and returns the accumulated reward\n\n\n\n\nUtility Tools\n\n\n#\n\n\nPOMDPs.add\n \n \nFunction\n.\n\n\nadd(solver_name::AbstractString, v::Bool=true)\n\n\n\n\nDownloads and installs a registered solver with name \nsolver_name\n.  \nv\n is a verbose flag, when set to true, function will notify the user if solver is already installed. This function is not exported, and must be called:\n\n\njulia\n using POMDPs\njulia\n POMDPs.add(\nMCTS\n)\n\n\n\n\n#\n\n\nPOMDPs.add_all\n \n \nFunction\n.\n\n\nadd_all()\n\n\n\n\nDownloads and installs all the packages supported by JuliaPOMDP\n\n\n#\n\n\nPOMDPs.test_all\n \n \nFunction\n.\n\n\ntest_all()\n\n\n\n\nTests all the JuliaPOMDP packages installed on your current machine.\n\n\n#\n\n\nPOMDPs.available\n \n \nFunction\n.\n\n\navailable()\n\n\n\n\nPrints all the available packages in JuliaPOMDP\n\n\n#\n\n\nPOMDPs.@pomdp_func\n \n \nMacro\n.\n\n\nProvide a default function implementation that throws an error when called.\n\n\n#\n\n\nPOMDPs.strip_arg\n \n \nFunction\n.\n\n\nStrip anything extra (type annotations, default values, etc) from an argument.\n\n\nFor now this cannot handle keyword arguments (it will throw an error).\n\n\n\n\nConstants\n\n\n#\n\n\nPOMDPs.REMOTE_URL\n \n \nConstant\n.\n\n\nurl to remote JuliaPOMDP organization repo\n\n\n#\n\n\nPOMDPs.SUPPORTED_PACKAGES\n \n \nConstant\n.\n\n\nSet containing string names of officially supported solvers and utility packages (e.g. \nMCTS\n, \nSARSOP\n, \nPOMDPToolbox\n, etc).  If you have a validated solver that supports the POMDPs.jl API, contact the developers to add your solver to this list.", 
            "title": "API"
        }, 
        {
            "location": "/api/#api-documentation", 
            "text": "Documentation for the  POMDPs.jl  user interface. You can get help for any type or function in the module by typing  ?  in the Julia REPL followed by the name of type or function. For example:  julia using POMDPs\njulia ?\nhelp? reward\nsearch: reward\n\n  reward{S,A,O}(pomdp::POMDP{S,A,O}, state::S, action::A, statep::S)\n\n  Returns the immediate reward for the s-a-s triple\n\n  reward{S,A,O}(pomdp::POMDP{S,A,O}, state::S, action::A)\n\n  Returns the immediate reward for the s-a pair", 
            "title": "API Documentation"
        }, 
        {
            "location": "/api/#contents", 
            "text": "API Documentation  Contents  Index  Types  Model Functions  Distribution/Space Functions  Belief Functions  Policy and Solver Functions  Simulator  Utility Tools  Constants", 
            "title": "Contents"
        }, 
        {
            "location": "/api/#index", 
            "text": "POMDPs.REMOTE_URL  POMDPs.SUPPORTED_PACKAGES  POMDPs.AbstractDistribution  POMDPs.AbstractSpace  POMDPs.MDP  POMDPs.POMDP  POMDPs.Policy  POMDPs.Simulator  POMDPs.Solver  POMDPs.Updater  Base.Random.rand  POMDPs.action  POMDPs.action_index  POMDPs.actions  POMDPs.add  POMDPs.add_all  POMDPs.available  POMDPs.create_action  POMDPs.create_belief  POMDPs.create_observation  POMDPs.create_observation_distribution  POMDPs.create_policy  POMDPs.create_state  POMDPs.create_transition_distribution  POMDPs.dimensions  POMDPs.discount  POMDPs.initial_state_distribution  POMDPs.initialize_belief  POMDPs.isterminal  POMDPs.isterminal_obs  POMDPs.iterator  POMDPs.n_actions  POMDPs.n_observations  POMDPs.n_states  POMDPs.obs_index  POMDPs.observation  POMDPs.observations  POMDPs.pdf  POMDPs.reward  POMDPs.simulate  POMDPs.solve  POMDPs.state_index  POMDPs.states  POMDPs.strip_arg  POMDPs.test_all  POMDPs.transition  POMDPs.update  POMDPs.updater  POMDPs.value  POMDPs.@pomdp_func", 
            "title": "Index"
        }, 
        {
            "location": "/api/#types", 
            "text": "#  POMDPs.POMDP     Type .  Abstract base type for a partially observable Markov decision process.  S: state type\nA: action type\nO: observation type  #  POMDPs.MDP     Type .  Abstract base type for a fully observable Markov decision process.  S: state type\nA: action type  #  POMDPs.AbstractSpace     Type .  Base type for state, action and observation spaces.  T: type that parametrizes the space (state, action, or observation)  #  POMDPs.AbstractDistribution     Type .  Abstract type for a probability distribution.  T: type over which distribution is over (state, action, or observation)  #  POMDPs.Solver     Type .  Base type for an MDP/POMDP solver  #  POMDPs.Policy     Type .  Base type for a policy (a map from every possible belief, or more abstract policy state, to an optimal or suboptimal action)  B: a belief (or policy state) that represents the knowledge an agent has about the state of the system  #  POMDPs.Updater     Type .  Abstract type for an object that defines how the belief should be updated  B: belief type that parametrizes the updater  A belief is a general construct that represents the knowledge an agent has about the state of the system. This can be a probability distribution, an action observation history or a more general representation.", 
            "title": "Types"
        }, 
        {
            "location": "/api/#model-functions", 
            "text": "#  POMDPs.states     Function .  states{S,A,O}(problem::POMDP{S,A,O}, state::S)\nstates{S,A}(problem::MDP{S,A}, state::S)  Returns a subset of the state space reachable from  state .   states(problem::POMDP)\nstates(problem::MDP)  Returns the complete state space of a POMDP.   #  POMDPs.actions     Function .  actions{S,A,O}(problem::POMDP{S,A,O}, state::S, aspace::AbstractSpace{A})\nactions{S,A}(problem::MDP{S,A}, state::S, aspace::AbstractSpace{A})  Modifies  aspace  to the action space accessible from the given state and returns it.  actions(problem::POMDP)\nactions(problem::MDP)  Returns the entire action space of a POMDP.  actions{S,A,O,B}(problem::POMDP{S,A,O}, belief::B, aspace::AbstractSpace{A})  Modifies  aspace  to the action space accessible from the states with nonzero belief and returns it.  #  POMDPs.observations     Function .  observations{S,A,O}(problem::POMDP{S,A,O}, state::S, obs::AbstractSpace{O}=observations(problem))  Modifies  obs  to the observation space accessible from the given state and returns it.  observations(problem::POMDP)  Returns the entire observation space.  #  POMDPs.reward     Function .  reward{S,A,O}(problem::POMDP{S,A,O}, state::S, action::A, statep::S)\nreward{S,A}(problem::MDP{S,A}, state::S, action::A, statep::S)  Returns the immediate reward for the s-a-s' triple  reward{S,A,O}(problem::POMDP{S,A,O}, state::S, action::A)\nreward{S,A}(problem::MDP{S,A}, state::S, action::A)  Returns the immediate reward for the s-a pair  #  POMDPs.transition     Function .  transition{S,A,O}(problem::POMDP{S,A,O}, state::S, action::A,  distribution::AbstractDistribution{S}=create_transition_distribution(problem))     transition{S,A}(problem::MDP{S,A}, state::S, action::A, distribution::AbstractDistribution{S}=create_transition_distribution(problem))  Returns the transition distribution from the current state-action pair  #  POMDPs.observation     Function .  observation{S,A,O}(problem::POMDP{S,A,O}, state::S, action::A, statep::S, distribution::AbstractDistribution{O}=create_observation_distribution(problem))  Returns the observation distribution for the s-a-s' tuple (state, action, and next state)  observation{S,A,O}(problem::POMDP{S,A,O}, action::A, statep::S, distribution::AbstractDistribution{O}=create_observation_distribution(problem))  Modifies distribution to the observation distribution for the a-s' tuple (action and next state) and returns it  #  POMDPs.isterminal     Function .  isterminal{S,A,O}(problem::POMDP{S,A,O}, state::S)\nisterminal{S,A}(problem::MDP{S,A}, state::S)  Checks if state s is terminal  #  POMDPs.isterminal_obs     Function .  isterminal_obs{S,A,O}(problem::POMDP{S,A,O}, observation::O)  Checks if an observation is terminal.  #  POMDPs.discount     Function .  discount(problem::POMDP)\ndiscount(problem::MDP)  Return the discount factor for the problem.  #  POMDPs.n_states     Function .  n_states(problem::POMDP)\nn_states(problem::MDP)  Returns the number of states in  problem . Used for discrete models only.  #  POMDPs.n_actions     Function .  n_actions(problem::POMDP)\nn_actions(problem::MDP)  Returns the number of actions in  problem . Used for discrete models only.  #  POMDPs.n_observations     Function .  n_observations(problem::POMDP)  Returns the number of actions in  problem . Used for discrete models only.  #  POMDPs.state_index     Function .  state_index{S,A,O}(problem::POMDP{S,A,O}, s::S)\nstate_index{S,A}(problem::MDP{S,A}, s::S)  Returns the integer index of state  s . Used for discrete models only.  #  POMDPs.action_index     Function .  action_index{S,A,O}(problem::POMDP{S,A,O}, a::A)\naction_index{S,A}(problem::MDP{S,A}, a::A)  Returns the integer index of action  a . Used for discrete models only.  #  POMDPs.obs_index     Function .  obs_index{S,A,O}(problem::POMDP{S,A,O}, o::O)  Returns the integer index of observation  o . Used for discrete models only.  #  POMDPs.create_state     Function .  create_state(problem::POMDP)\ncreate_state(problem::MDP)  Create a state object (for preallocation purposes).  #  POMDPs.create_action     Function .  create_action(problem::POMDP)\ncreate_action(problem::MDP)  Creates an action object (for preallocation purposes)  #  POMDPs.create_observation     Function .  create_observation(problem::POMDP)  Create an observation object (for preallocation purposes).", 
            "title": "Model Functions"
        }, 
        {
            "location": "/api/#distributionspace-functions", 
            "text": "#  Base.Random.rand     Function .  rand{T}(rng::AbstractRNG, d::AbstractSpace{T}, sample::T)  Returns a random  sample  from space  s .  rand{T}(rng::AbstractRNG, d::AbstractDistribution{T}, sample::T)  Fill  sample  with a random element from distribution  d . The sample can be a state, action or observation.  #  POMDPs.pdf     Function .  pdf{T}(d::AbstractDistribution{T}, x::T)  Value of probability distribution  d  function at sample  x .  #  POMDPs.dimensions     Function .  dimensions{T}(s::AbstractSpace{T})  Returns the number of dimensions in space  s .  #  POMDPs.iterator     Function .  iterator{T}(s::AbstractSpace{T})  Returns an iterable type (array or custom iterator) corresponding to space  s .   iterator{T}(d::AbstractDistribution{T})  Returns an iterable type (array or custom iterator) corresponding to distribution  d .   #  POMDPs.initial_state_distribution     Function .  initial_state_distribution(pomdp::POMDP)  Returns an initial belief for the pomdp.  #  POMDPs.create_transition_distribution     Function .  create_transition_distribution(problem::POMDP)\ncreate_transition_distribution(problem::MDP)  Returns a transition distribution (for memory preallocation).  #  POMDPs.create_observation_distribution     Function .  create_observation_distribution(problem::POMDP)\ncreate_observation_distribution(problem::MDP)  Returns an observation distribution (for memory preallocation).", 
            "title": "Distribution/Space Functions"
        }, 
        {
            "location": "/api/#belief-functions", 
            "text": "#  POMDPs.update     Function .  update{B,A,O}(updater::Updater, belief_old::B, action::A, obs::O,\n              belief_new::B=create_belief(updater))  Returns a new instance of an updated belief given  belief_old  and the latest action and observation.  update()  Updates all the installed packages  #  POMDPs.create_belief     Function .  create_belief(updater::Updater)  Creates a belief object of the type used by  updater  (preallocates memory)  create_belief(pomdp::POMDP)  Creates a belief either to be used by updater or pomdp  #  POMDPs.initialize_belief     Function .  initialize_belief{B}(updater::Updater{B}, \n                     state_distribution::AbstractDistribution,\n                     new_belief::B=create_belief(updater))\ninitialize_belief{B}(updater::Updater{B},\n                     belief::Any,\n                     new_belief::B=create_belief(updater))  Returns a belief that can be updated using  updater  that has similar distribution to  state_distribution  or  belief .  The conversion may be lossy. This function is also idempotent, i.e. there is a default implementation that passes the belief through when it is already the correct type:  initialize_belief{B}(updater::Updater{B}, belief::B) = belief", 
            "title": "Belief Functions"
        }, 
        {
            "location": "/api/#policy-and-solver-functions", 
            "text": "#  POMDPs.create_policy     Function .  create_policy(solver::Solver, problem::POMDP)\ncreate_policy(solver::Solver, problem::MDP)  Creates a policy object (for preallocation purposes)  #  POMDPs.solve     Function .  solve(solver::Solver, problem::POMDP, policy=create_policy(solver, problem))  Solves the POMDP using method associated with solver, and returns a policy.   #  POMDPs.updater     Function .  updater(policy::Policy)  Returns a default Updater appropriate for a belief type that policy  p  can use  #  POMDPs.action     Function .  action{B}(p::Policy, x::B, action)  Fills and returns action based on the current state or belief, given the policy.  B  is a generalized information state - can be a state in an MDP, a distribution in POMDP, or any other representation needed to make a decision using the given policy.   action{B}(policy::Policy, x::B)  Returns an action for the current state or belief, given the policy  If an MDP is being simulated,  x  will be a state; if a POMDP is being simulated,  x  will be a belief  #  POMDPs.value     Function .  value{B}(p::Policy, x::B)  Returns the utility value from policy  p  given the state", 
            "title": "Policy and Solver Functions"
        }, 
        {
            "location": "/api/#simulator", 
            "text": "#  POMDPs.Simulator     Type .  Base type for an object defining how a simulation should be carried out  #  POMDPs.simulate     Function .  simulate{S,A}(simulator::Simulator, problem::MDP{S,A}, policy::Policy, initial_state::S)  Run a simulation using the specified policy and returns the accumulated reward  simulate{S,A,O,B}(simulator::Simulator, problem::POMDP{S,A,O}, policy::Policy{B}, updater::Updater{B}, initial_belief::Union{B,AbstractDistribution{S}})  Run a simulation using the specified policy and returns the accumulated reward", 
            "title": "Simulator"
        }, 
        {
            "location": "/api/#utility-tools", 
            "text": "#  POMDPs.add     Function .  add(solver_name::AbstractString, v::Bool=true)  Downloads and installs a registered solver with name  solver_name .   v  is a verbose flag, when set to true, function will notify the user if solver is already installed. This function is not exported, and must be called:  julia  using POMDPs\njulia  POMDPs.add( MCTS )  #  POMDPs.add_all     Function .  add_all()  Downloads and installs all the packages supported by JuliaPOMDP  #  POMDPs.test_all     Function .  test_all()  Tests all the JuliaPOMDP packages installed on your current machine.  #  POMDPs.available     Function .  available()  Prints all the available packages in JuliaPOMDP  #  POMDPs.@pomdp_func     Macro .  Provide a default function implementation that throws an error when called.  #  POMDPs.strip_arg     Function .  Strip anything extra (type annotations, default values, etc) from an argument.  For now this cannot handle keyword arguments (it will throw an error).", 
            "title": "Utility Tools"
        }, 
        {
            "location": "/api/#constants", 
            "text": "#  POMDPs.REMOTE_URL     Constant .  url to remote JuliaPOMDP organization repo  #  POMDPs.SUPPORTED_PACKAGES     Constant .  Set containing string names of officially supported solvers and utility packages (e.g.  MCTS ,  SARSOP ,  POMDPToolbox , etc).  If you have a validated solver that supports the POMDPs.jl API, contact the developers to add your solver to this list.", 
            "title": "Constants"
        }, 
        {
            "location": "/faq/", 
            "text": "Frequently Asked Questions (FAQ)\n\n\n\n\nWhy am I getting a \"No implemnetation for ...\" error?\n\n\nYou will typically see this error when you haven't implemented a function that a solver is trying to call.  For example, if you are using the QMDP solver, and have not implemented \nnum_states\n for your POMDP, you will see the no implementation error. To fix the error, you need to create a \nnum_states\n function that takes in your POMDP. To see the required functions for a given solver you can run:\n\n\nusing QMDP\nQMDP.required()\n\n\n\n\n\n\nHow do I save my policies?\n\n\nWe recommend using \nJLD\n to save the whole policy object. This is the simplest, and fairly efficient way to save Julia objects. JLD uses HDF5 format underneath. If you've already computed a policy, you can simply run:\n\n\nusing JLD\nsave(\nmy_policy.jld\n, \npolicy\n, policy) \n\n\n\n\n\n\nWhy isn't the solver working?\n\n\nThere could be a number of things that are going wrong. Remeber, POMDPs can be failry hard to work with, but don't panic.  If you have a discrete POMDP or MDP and you're using a solver that requires the explicit transition probabilities (you've implemented a \npdf\n function), the first thing to try is make sure that your probability masses sum up to unity.  We've provide some tools in POMDPToolbox that can check this for you. If you have a POMDP called pomdp, you can run the checks by doing the following:\n\n\nusing POMDPToolbox\nprobability_check(pomdp) # checks that both observation and transition functions give probs that sum to unity\nobs_prob_consistancy_check(pomdp) # checks the observation probabilities\ntrans_prob_consistancy_check(pomdp) # check the transition probabilities\n\n\n\n\nIf these throw an error, you may need to fix your \ntransition\n or \nobservation\n functions. \n\n\n\n\nWhy do I need to put type assertions pomdp::POMDP into the function signature?\n\n\nSpecifying the type in your function signature allows Julia to call the appropriate function when your custom type is passed into it.  For example if a POMDPs.jl solver calls \nstates\n on the POMDP that you passed into it, the correct \nstates\n function will only get dispatched if you specified that the \nstates\n function you wrote works with your POMDP type. Because Julia supports multiple-dispatch, these type assertion are a way for doing object-oriented programming in Julia. \n\n\n\n\nWhy are all the solvers in seperate modules?\n\n\nWe did not put all the solvers and support tools into POMDPs.jl, because we wanted POMDPs.jl to be a lighweight interface package.  This has a number of advantages. The first is that if a user only wants to use a few solvers from the\n\n\nJuliaPOMDP organization, they do not have to install all the other sovlers and their dependencies.  The second advantage is that people who are not directly part of the JuliaPOMDP organization can write their own solvers without going into the source code of other solvers. This makes the framework easier to adopt and to extend.", 
            "title": "Frequently Asked Questions"
        }, 
        {
            "location": "/faq/#frequently-asked-questions-faq", 
            "text": "", 
            "title": "Frequently Asked Questions (FAQ)"
        }, 
        {
            "location": "/faq/#why-am-i-getting-a-no-implemnetation-for-error", 
            "text": "You will typically see this error when you haven't implemented a function that a solver is trying to call.  For example, if you are using the QMDP solver, and have not implemented  num_states  for your POMDP, you will see the no implementation error. To fix the error, you need to create a  num_states  function that takes in your POMDP. To see the required functions for a given solver you can run:  using QMDP\nQMDP.required()", 
            "title": "Why am I getting a \"No implemnetation for ...\" error?"
        }, 
        {
            "location": "/faq/#how-do-i-save-my-policies", 
            "text": "We recommend using  JLD  to save the whole policy object. This is the simplest, and fairly efficient way to save Julia objects. JLD uses HDF5 format underneath. If you've already computed a policy, you can simply run:  using JLD\nsave( my_policy.jld ,  policy , policy)", 
            "title": "How do I save my policies?"
        }, 
        {
            "location": "/faq/#why-isnt-the-solver-working", 
            "text": "There could be a number of things that are going wrong. Remeber, POMDPs can be failry hard to work with, but don't panic.  If you have a discrete POMDP or MDP and you're using a solver that requires the explicit transition probabilities (you've implemented a  pdf  function), the first thing to try is make sure that your probability masses sum up to unity.  We've provide some tools in POMDPToolbox that can check this for you. If you have a POMDP called pomdp, you can run the checks by doing the following:  using POMDPToolbox\nprobability_check(pomdp) # checks that both observation and transition functions give probs that sum to unity\nobs_prob_consistancy_check(pomdp) # checks the observation probabilities\ntrans_prob_consistancy_check(pomdp) # check the transition probabilities  If these throw an error, you may need to fix your  transition  or  observation  functions.", 
            "title": "Why isn't the solver working?"
        }, 
        {
            "location": "/faq/#why-do-i-need-to-put-type-assertions-pomdppomdp-into-the-function-signature", 
            "text": "Specifying the type in your function signature allows Julia to call the appropriate function when your custom type is passed into it.  For example if a POMDPs.jl solver calls  states  on the POMDP that you passed into it, the correct  states  function will only get dispatched if you specified that the  states  function you wrote works with your POMDP type. Because Julia supports multiple-dispatch, these type assertion are a way for doing object-oriented programming in Julia.", 
            "title": "Why do I need to put type assertions pomdp::POMDP into the function signature?"
        }, 
        {
            "location": "/faq/#why-are-all-the-solvers-in-seperate-modules", 
            "text": "We did not put all the solvers and support tools into POMDPs.jl, because we wanted POMDPs.jl to be a lighweight interface package.  This has a number of advantages. The first is that if a user only wants to use a few solvers from the  JuliaPOMDP organization, they do not have to install all the other sovlers and their dependencies.  The second advantage is that people who are not directly part of the JuliaPOMDP organization can write their own solvers without going into the source code of other solvers. This makes the framework easier to adopt and to extend.", 
            "title": "Why are all the solvers in seperate modules?"
        }
    ]
}