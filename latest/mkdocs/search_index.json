{
    "docs": [
        {
            "location": "/", 
            "text": "POMDPs\n\n\nA Julia interface for defining, solving and simulating partially observable Markov decision processes and their fully observable counterparts.\n\n\n\n\nPackage Features\n\n\n\n\nGeneral interface that can handle problems with discrete and continuous state/action/observation spaces\n\n\nA number of popular state-of-the-art solvers availiable to use out of the box\n\n\nTools that make it easy to define problems and simulate solutions \n\n\nSimple integration of custom solvers into the existing interface\n\n\n\n\n\n\nAvailible Solvers\n\n\nThe following MDP solvers support this interface:\n\n\n\n\nValue Iteration\n\n\nMonte Carlo Tree Search\n\n\n\n\nThe following POMDP solvers support this interface:\n\n\n\n\nQMDP\n\n\nSARSOP\n\n\nPOMCP\n\n\nPOMDPSolve\n\n\n\n\n\n\nManual Outline\n\n\n\n\nSolver Documentation\n\n\nContents\n\n\nIndex\n\n\nTypes\n\n\nModel Functions\n\n\nDistribution/Space Functions\n\n\nBelief Functions\n\n\nPolicy and Solver Functions\n\n\nSimulator\n\n\nUtility Tools\n\n\nConstants\n\n\n\n\n\n\nPackage Guide\n\n\nInstallation\n\n\nUsage\n\n\nExample Simulation Implementation\n\n\n\n\n\n\nPOMDPs\n\n\nPackage Features\n\n\nAvailible Solvers\n\n\nManual Outline", 
            "title": "Home"
        }, 
        {
            "location": "/#pomdps", 
            "text": "A Julia interface for defining, solving and simulating partially observable Markov decision processes and their fully observable counterparts.", 
            "title": "POMDPs"
        }, 
        {
            "location": "/#package-features", 
            "text": "General interface that can handle problems with discrete and continuous state/action/observation spaces  A number of popular state-of-the-art solvers availiable to use out of the box  Tools that make it easy to define problems and simulate solutions   Simple integration of custom solvers into the existing interface", 
            "title": "Package Features"
        }, 
        {
            "location": "/#availible-solvers", 
            "text": "The following MDP solvers support this interface:   Value Iteration  Monte Carlo Tree Search   The following POMDP solvers support this interface:   QMDP  SARSOP  POMCP  POMDPSolve", 
            "title": "Availible Solvers"
        }, 
        {
            "location": "/#manual-outline", 
            "text": "Solver Documentation  Contents  Index  Types  Model Functions  Distribution/Space Functions  Belief Functions  Policy and Solver Functions  Simulator  Utility Tools  Constants    Package Guide  Installation  Usage  Example Simulation Implementation    POMDPs  Package Features  Availible Solvers  Manual Outline", 
            "title": "Manual Outline"
        }, 
        {
            "location": "/guide/", 
            "text": "Package Guide\n\n\n\n\nInstallation\n\n\nThe package can be installed by cloning the code from the github repository \nPOMDPs.jl\n\n\nInstallation with POMDPs.jl:\n\n\nPkg\n.\nclone\n(\nhttps://github.com/JuliaPOMDP/POMDPs.jl.git\n)\n\n\n\n\n\n\nThe package is currently not registered in meta-data. \n\n\n\n\nUsage\n\n\nPOMDPs serves as the interface used by a number of packages under the \nJuliaPOMDP\n framework. It is essentially the agreed upon API used by all the other packages in JuliaPOMDP. If you are using this framework, you may be trying to accomplish one or more of the following three goals:\n\n\n\n\nSolve a decision or planning problem with stochastic dynamics (MDP) or partial observability (POMDP)\n\n\nEvaluate a solution in simulation\n\n\nTest your custom algorithm for solving MDPs or POMDPs against other state-of-the-art algorithms\n\n\n\n\nIf you are attempting to complete the first two goals, take a look at these Jupyer Notebook tutorials:\n\n\n\n\nMDP Tutorial\n for beginners gives an overview of using Value Iteration and Monte-Carlo Tree Search with the classic grid world problem\n\n\nPOMDP Tutorial\n gives an overview of using SARSOP and QMDP to solve the tiger problem\n\n\n\n\nIf you are trying to write your own algorithm for solving MDPs or POMDPs with this interface take a look at the API section of this guide. \n\n\n\n\nExample Simulation Implementation\n\n\nThis reference simulation implementation shows how the various functions will be used. Please note that this example is written for clarity and not efficiency.\n\n\ntype\n ReferenceSimulator\n\n    \nrng\n::\nAbstractRNG\n\n    \nmax_steps\n\n\nend\n\n\n\nfunction\n simulate\n(\nsimulator\n::\nReferenceSimulator\n,\n \npomdp\n::\nPOMDP\n,\n \npolicy\n::\nPolicy\n,\n \nupdater\n::\nBeliefUpdater\n,\n \ninitial_belief\n::\nBelief\n)\n\n\n    \ns\n \n=\n \ncreate_state\n(\npomdp\n)\n\n    \no\n \n=\n \ncreate_observation\n(\npomdp\n)\n\n    \nrand\n(\nsim\n.\nrng\n,\n \ninitial_belief\n,\n \ns\n)\n\n\n    \nb\n \n=\n \nconvert_belief\n(\nupdater\n,\n \ninitial_belief\n)\n\n\n    \nstep\n \n=\n \n1\n\n    \ndisc\n \n=\n \n1.0\n\n    \nr\n \n=\n \n0.0\n\n\n    \nwhile\n \nstep\n \n=\n \nsim\n.\nmax_steps\n \n \n!\nisterminal\n(\npomdp\n,\n \ns\n)\n\n        \na\n \n=\n \naction\n(\npolicy\n,\n \nb\n)\n\n\n        \nsp\n \n=\n \ncreate_state\n(\npomdp\n)\n\n        \ntrans_dist\n \n=\n \ntransition\n(\npomdp\n,\n \ns\n,\n \na\n)\n\n        \nrand\n(\nsim\n.\nrng\n,\n \ntrans_dist\n,\n \nsp\n)\n\n\n        \nr\n \n+=\n \ndisc\n*\nreward\n(\npomdp\n,\n \ns\n,\n \na\n,\n \nsp\n)\n\n\n        \nobs_dist\n \n=\n \nobservation\n(\npomdp\n,\n \ns\n,\n \na\n,\n \nsp\n)\n\n        \nrand\n(\nsim\n.\nrng\n,\n \nobs_dist\n,\n \no\n)\n\n\n        \nb\n \n=\n \nupdate\n(\nupdater\n,\n \nb\n,\n \na\n,\n \no\n)\n\n\n        \ns\n \n=\n \nsp\n\n        \ndisc\n \n*=\n \ndiscount\n(\npomdp\n)\n\n        \nstep\n \n+=\n \n1\n\n    \nend\n\n\n\nend", 
            "title": "Manual"
        }, 
        {
            "location": "/guide/#package-guide", 
            "text": "", 
            "title": "Package Guide"
        }, 
        {
            "location": "/guide/#installation", 
            "text": "The package can be installed by cloning the code from the github repository  POMDPs.jl  Installation with POMDPs.jl:  Pkg . clone ( https://github.com/JuliaPOMDP/POMDPs.jl.git )   The package is currently not registered in meta-data.", 
            "title": "Installation"
        }, 
        {
            "location": "/guide/#usage", 
            "text": "POMDPs serves as the interface used by a number of packages under the  JuliaPOMDP  framework. It is essentially the agreed upon API used by all the other packages in JuliaPOMDP. If you are using this framework, you may be trying to accomplish one or more of the following three goals:   Solve a decision or planning problem with stochastic dynamics (MDP) or partial observability (POMDP)  Evaluate a solution in simulation  Test your custom algorithm for solving MDPs or POMDPs against other state-of-the-art algorithms   If you are attempting to complete the first two goals, take a look at these Jupyer Notebook tutorials:   MDP Tutorial  for beginners gives an overview of using Value Iteration and Monte-Carlo Tree Search with the classic grid world problem  POMDP Tutorial  gives an overview of using SARSOP and QMDP to solve the tiger problem   If you are trying to write your own algorithm for solving MDPs or POMDPs with this interface take a look at the API section of this guide.", 
            "title": "Usage"
        }, 
        {
            "location": "/guide/#example-simulation-implementation", 
            "text": "This reference simulation implementation shows how the various functions will be used. Please note that this example is written for clarity and not efficiency.  type  ReferenceSimulator \n     rng :: AbstractRNG \n     max_steps  end  function  simulate ( simulator :: ReferenceSimulator ,   pomdp :: POMDP ,   policy :: Policy ,   updater :: BeliefUpdater ,   initial_belief :: Belief ) \n\n     s   =   create_state ( pomdp ) \n     o   =   create_observation ( pomdp ) \n     rand ( sim . rng ,   initial_belief ,   s ) \n\n     b   =   convert_belief ( updater ,   initial_belief ) \n\n     step   =   1 \n     disc   =   1.0 \n     r   =   0.0 \n\n     while   step   =   sim . max_steps     ! isterminal ( pomdp ,   s ) \n         a   =   action ( policy ,   b ) \n\n         sp   =   create_state ( pomdp ) \n         trans_dist   =   transition ( pomdp ,   s ,   a ) \n         rand ( sim . rng ,   trans_dist ,   sp ) \n\n         r   +=   disc * reward ( pomdp ,   s ,   a ,   sp ) \n\n         obs_dist   =   observation ( pomdp ,   s ,   a ,   sp ) \n         rand ( sim . rng ,   obs_dist ,   o ) \n\n         b   =   update ( updater ,   b ,   a ,   o ) \n\n         s   =   sp \n         disc   *=   discount ( pomdp ) \n         step   +=   1 \n     end  end", 
            "title": "Example Simulation Implementation"
        }, 
        {
            "location": "/api/", 
            "text": "Solver Documentation\n\n\nDocumentation for the \nPOMDPs.jl\n user interface. You can get help for any type or  function in the module by typing \n?\n in the Julia REPL followed by the name of  type or function. For example:\n\n\njulia\nusing\n \nPOMDPs\n\n\njulia\n?\n\n\nhelp\n?\nreward\n\n\nsearch\n:\n \nreward\n\n\n  \nreward\n{\nS\n,\nA\n,\nO\n}(\npomdp\n::\nPOMDP\n{\nS\n,\nA\n,\nO\n},\n \nstate\n::\nS\n,\n \naction\n::\nA\n,\n \nstatep\n::\nS\n)\n\n\n  \nReturns\n \nthe\n \nimmediate\n \nreward\n \nfor\n \nthe\n \ns\n-\na\n-\ns\n \ntriple\n\n\n  \nreward\n{\nS\n,\nA\n,\nO\n}(\npomdp\n::\nPOMDP\n{\nS\n,\nA\n,\nO\n},\n \nstate\n::\nS\n,\n \naction\n::\nA\n)\n\n\n  \nReturns\n \nthe\n \nimmediate\n \nreward\n \nfor\n \nthe\n \ns\n-\na\n \npair\n\n\n\n\n\n\n\n\nContents\n\n\n\n\nSolver Documentation\n\n\nContents\n\n\nIndex\n\n\nTypes\n\n\nModel Functions\n\n\nDistribution/Space Functions\n\n\nBelief Functions\n\n\nPolicy and Solver Functions\n\n\nSimulator\n\n\nUtility Tools\n\n\nConstants\n\n\n\n\n\n\n\n\n\n\nIndex\n\n\n\n\nBase.Random.rand\n\n\nPOMDPs.@pomdp_func\n\n\nPOMDPs.AbstractDistribution\n\n\nPOMDPs.AbstractSpace\n\n\nPOMDPs.Belief\n\n\nPOMDPs.BeliefUpdater\n\n\nPOMDPs.MDP\n\n\nPOMDPs.POMDP\n\n\nPOMDPs.Policy\n\n\nPOMDPs.REMOTE_URL\n\n\nPOMDPs.SUPPORTED_SOLVERS\n\n\nPOMDPs.Simulator\n\n\nPOMDPs.Solver\n\n\nPOMDPs.action\n\n\nPOMDPs.action_index\n\n\nPOMDPs.actions\n\n\nPOMDPs.add\n\n\nPOMDPs.convert_belief\n\n\nPOMDPs.create_belief\n\n\nPOMDPs.create_observation_distribution\n\n\nPOMDPs.create_policy\n\n\nPOMDPs.create_transition_distribution\n\n\nPOMDPs.dimensions\n\n\nPOMDPs.discount\n\n\nPOMDPs.initial_belief\n\n\nPOMDPs.isterminal\n\n\nPOMDPs.isterminal_obs\n\n\nPOMDPs.iterator\n\n\nPOMDPs.n_actions\n\n\nPOMDPs.n_observations\n\n\nPOMDPs.n_states\n\n\nPOMDPs.obs_index\n\n\nPOMDPs.observation\n\n\nPOMDPs.observations\n\n\nPOMDPs.pdf\n\n\nPOMDPs.reward\n\n\nPOMDPs.simulate\n\n\nPOMDPs.solve\n\n\nPOMDPs.state_index\n\n\nPOMDPs.states\n\n\nPOMDPs.strip_arg\n\n\nPOMDPs.transition\n\n\nPOMDPs.update\n\n\nPOMDPs.updater\n\n\nPOMDPs.value\n\n\n\n\n\n\nTypes\n\n\n#\n\n\nPOMDPs.POMDP\n \n \nType\n.\n\n\n\n\nAbstract base type for a partially observable Markov decision process.\n\n\nS\n:\n \nstate\n \ntype\n\n\nA\n:\n \naction\n \ntype\n\n\nO\n:\n \nobservation\n \ntype\n\n\n\n\n\n\n#\n\n\nPOMDPs.MDP\n \n \nType\n.\n\n\n\n\nAbstract base type for a fully observable Markov decision process.\n\n\nS\n:\n \nstate\n \ntype\n\n\nA\n:\n \naction\n \ntype\n\n\n\n\n\n\n#\n\n\nPOMDPs.AbstractSpace\n \n \nType\n.\n\n\n\n\nBase type for state, action and observation spaces.\n\n\nT\n:\n \ntype\n \nthat\n \nparametarizes\n \nthe\n \nspace\n \n(\nstate\n,\n \naction\n,\n \nor\n \nobservation\n)\n\n\n\n\n\n\n#\n\n\nPOMDPs.AbstractDistribution\n \n \nType\n.\n\n\n\n\nAbstract type for a probability distribution.\n\n\nT\n:\n \ntype\n \nover\n \nwhich\n \ndistribution\n \nis\n \nover\n \n(\nstate\n,\n \naction\n,\n \nor\n \nobservation\n)\n\n\n\n\n\n\n#\n\n\nPOMDPs.Solver\n \n \nType\n.\n\n\n\n\nBase type for an MDP/POMDP solver\n\n\n#\n\n\nPOMDPs.Policy\n \n \nType\n.\n\n\n\n\nBase type for a policy (a map from every possible belief, or more abstract policy state, to an optimal or suboptimal action)\n\n\n#\n\n\nPOMDPs.Belief\n \n \nType\n.\n\n\n\n\nAbstract type for an object representing some knowledge about the state (often a probability distribution)\n\n\n#\n\n\nPOMDPs.BeliefUpdater\n \n \nType\n.\n\n\n\n\nAbstract type for an object that defines how a belief should be updated\n\n\n\n\nModel Functions\n\n\n#\n\n\nPOMDPs.states\n \n \nFunction\n.\n\n\n\n\nstates{S,A,O}(problem::POMDP{S,A,O}, state::S)\nstates{S,A}(problem::MDP{S,A}, state::S)\n\n\n\n\n\nReturns a subset of the state space reachable from \nstate\n. \n\n\nstates(problem::POMDP)\nstates(problem::MDP)\n\n\n\n\n\nReturns the complete state space of a POMDP. \n\n\n#\n\n\nPOMDPs.actions\n \n \nFunction\n.\n\n\n\n\nactions{S,A,O}(problem::POMDP{S,A,O}, state::S, aspace::AbstractSpace{A})\nactions{S,A}(problem::MDP{S,A}, state::S, aspace::AbstractSpace{A})\n\n\n\n\n\nModifies aspace to the action space accessible from the given state and returns it.\n\n\nactions(problem::POMDP)\nactions(problem::MDP)\n\n\n\n\n\nReturns the entire action space of a POMDP.\n\n\nactions{S,A,O}(problem::POMDP{S,A,O}, belief::Belief, aspace::AbstractSpace{A})\n\n\n\n\n\nModifies aspace to the action space accessible from the states with nonzero belief and returns it.\n\n\n#\n\n\nPOMDPs.observations\n \n \nFunction\n.\n\n\n\n\nobservations{S,A,O}(problem::POMDP{S,A,O}, state::S, obs::AbstractSpace{O}=observations(problem))\n\n\n\n\n\nModifies ospace to the observation space accessible from the given state and returns it.\n\n\nobservations(problem::POMDP)\n\n\n\n\n\nReturns the entire observation space.\n\n\n#\n\n\nPOMDPs.reward\n \n \nFunction\n.\n\n\n\n\nreward{S,A,O}(problem::POMDP{S,A,O}, state::S, action::A, statep::S)\nreward{S,A}(problem::MDP{S,A}, state::S, action::A, statep::S)\n\n\n\n\n\nReturns the immediate reward for the s-a-s' triple\n\n\nreward{S,A,O}(problem::POMDP{S,A,O}, state::S, action::A)\nreward{S,A}(problem::MDP{S,A}, state::S, action::A)\n\n\n\n\n\nReturns the immediate reward for the s-a pair\n\n\n#\n\n\nPOMDPs.transition\n \n \nFunction\n.\n\n\n\n\ntransition{S,A,O}(problem::POMDP{S,A,O}, state::S, action::A,\n\n\n\n\n\ndistribution::AbstractDistribution{S}=create_transition_distribution(problem))     transition{S,A}(problem::MDP{S,A}, state::S, action::A, distribution::AbstractDistribution{S}=create_transition_distribution(problem))\n\n\nReturns the transition distribution from the current state-action pair\n\n\n#\n\n\nPOMDPs.observation\n \n \nFunction\n.\n\n\n\n\nobservation{S,A,O}(problem::POMDP{S,A,O}, state::S, action::A, statep::S, distribution::AbstractDistribution{O}=create_observation_distribution(problem))\n\n\n\n\n\nReturns the observation distribution for the s-a-s' tuple (state, action, and next state)\n\n\nobservation{S,A,O}(problem::POMDP{S,A,O}, action::A, statep::S, distribution::AbstractDistribution{O}=create_observation_distribution(problem))\n\n\n\n\n\nModifies distribution to the observation distribution for the a-s' tuple (action and next state) and returns it\n\n\n#\n\n\nPOMDPs.isterminal\n \n \nFunction\n.\n\n\n\n\nisterminal{S,A,O}(problem::POMDP{S,A,O}, state::S)\nisterminal{S,A}(problem::MDP{S,A}, state::S)\n\n\n\n\n\nChecks if state s is terminal\n\n\n#\n\n\nPOMDPs.isterminal_obs\n \n \nFunction\n.\n\n\n\n\nisterminal_obs{S,A,O}(problem::POMDP{S,A,O}, observation::O)\n\n\n\n\n\nChecks if an observation is terminal.\n\n\n#\n\n\nPOMDPs.n_states\n \n \nFunction\n.\n\n\n\n\nn_states(problem::POMDP)\nn_states(problem::MDP)\n\n\n\n\n\nReturns the number of states in \nproblem\n. Used for discrete models only.\n\n\n#\n\n\nPOMDPs.n_actions\n \n \nFunction\n.\n\n\n\n\nn_actions(problem::POMDP)\nn_actions(problem::MDP)\n\n\n\n\n\nReturns the number of actions in \nproblem\n. Used for discrete models only.\n\n\n#\n\n\nPOMDPs.n_observations\n \n \nFunction\n.\n\n\n\n\nn_observations(problem::POMDP)\n\n\n\n\n\nReturns the number of actions in \nproblem\n. Used for discrete models only.\n\n\n#\n\n\nPOMDPs.state_index\n \n \nFunction\n.\n\n\n\n\nstate_index{S,A,O}(problem::POMDP{S,A,O}, s::S)\nstate_index{S,A}(problem::MDP{S,A}, s::S)\n\n\n\n\n\nReturns the integer index of state \ns\n. Used for discrete models only.\n\n\n#\n\n\nPOMDPs.action_index\n \n \nFunction\n.\n\n\n\n\naction_index{S,A,O}(problem::POMDP{S,A,O}, a::A)\naction_index{S,A}(problem::MDP{S,A}, a::A)\n\n\n\n\n\nReturns the integer index of action \na\n. Used for discrete models only.\n\n\n#\n\n\nPOMDPs.obs_index\n \n \nFunction\n.\n\n\n\n\nobs_index{S,A,O}(problem::POMDP{S,A,O}, o::O)\n\n\n\n\n\nReturns the integer index of observation \no\n. Used for discrete models only.\n\n\n#\n\n\nPOMDPs.discount\n \n \nFunction\n.\n\n\n\n\ndiscount(problem::POMDP)\ndiscount(problem::MDP)\n\n\n\n\n\nReturn the discount factor for the problem.\n\n\n\n\nDistribution/Space Functions\n\n\n#\n\n\nBase.Random.rand\n \n \nFunction\n.\n\n\n\n\nrand{T}(rng::AbstractRNG, d::AbstractSpace{T}, sample::T)\n\n\n\n\n\nReturns a random \nsample\n from space \ns\n.\n\n\nrand{T}(rng::AbstractRNG, d::AbstractDistribution{T}, sample::T)\n\n\n\n\n\nFill \nsample\n with a random element from distribution \nd\n. The sample can be a state, action or observation.\n\n\n#\n\n\nPOMDPs.pdf\n \n \nFunction\n.\n\n\n\n\npdf{T}(d::AbstractDistribution{T}, x::T)\n\n\n\n\n\nValue of probability distribution \nd\n function at sample \nx\n.\n\n\n#\n\n\nPOMDPs.dimensions\n \n \nFunction\n.\n\n\n\n\ndimensions{T}(s::AbstractSpace{T})\n\n\n\n\n\nReturns the number of dimensions in space \ns\n.\n\n\n#\n\n\nPOMDPs.iterator\n \n \nFunction\n.\n\n\n\n\niterator{T}(s::AbstractSpace{T})\n\n\n\n\n\nReturns an iterable type (array or custom iterator) corresponding to space \ns\n. \n\n\n#\n\n\nPOMDPs.create_transition_distribution\n \n \nFunction\n.\n\n\n\n\ncreate_transition_distribution(problem::POMDP)\ncreate_transition_distribution(problem::MDP)\n\n\n\n\n\nReturns a transition distribution (for memory preallocation).\n\n\n#\n\n\nPOMDPs.create_observation_distribution\n \n \nFunction\n.\n\n\n\n\ncreate_observation_distribution(problem::POMDP)\ncreate_observation_distribution(problem::MDP)\n\n\n\n\n\nReturns an observation distribution (for memory preallocation).\n\n\n\n\nBelief Functions\n\n\n#\n\n\nPOMDPs.initial_belief\n \n \nFunction\n.\n\n\n\n\ninitial_belief(pomdp::POMDP, belief::Belief = create_belief(pomdp))\n\n\n\n\n\nReturns an initial belief for the pomdp.\n\n\n#\n\n\nPOMDPs.create_belief\n \n \nFunction\n.\n\n\n\n\ncreate_belief(updater::BeliefUpdater)\n\n\n\n\n\nCreates a belief object of the type used by \nupdater\n (preallocates memory)\n\n\ncreate_belief(pomdp::POMDP)\n\n\n\n\n\nCreates a belief either to be used by updater or pomdp\n\n\n#\n\n\nPOMDPs.update\n \n \nFunction\n.\n\n\n\n\nupdate(updater::BeliefUpdater, belief_old::Belief, action, obs,\nbelief_new::Belief=create_belief(updater))\n\n\n\n\n\nReturns a new instance of an updated belief given \nbelief_old\n and the latest action and observation.\n\n\n#\n\n\nPOMDPs.convert_belief\n \n \nFunction\n.\n\n\n\n\nconvert_belief(updater::BeliefUpdater, belief::Belief,\nnew_belief::Belief=create_belief(updater)) = belief\n\n\n\n\n\nReturns a belief that can be updated using \nupdater\n that has a similar distribution to \nbelief\n.\n\n\n\n\nPolicy and Solver Functions\n\n\n#\n\n\nPOMDPs.create_policy\n \n \nFunction\n.\n\n\n\n\ncreate_policy(solver::Solver, problem::POMDP)\ncreate_policy(solver::Solver, problem::MDP)\n\n\n\n\n\nCreates a policy object (for preallocation purposes)\n\n\n#\n\n\nPOMDPs.solve\n \n \nFunction\n.\n\n\n\n\nsolve(solver::Solver, problem::POMDP, policy=create_policy(solver, problem))\n\n\n\n\n\nSolves the POMDP using method associated with solver, and returns a policy. \n\n\n#\n\n\nPOMDPs.updater\n \n \nFunction\n.\n\n\n\n\nupdater(policy::Policy)\n\n\n\n\n\nReturns a default BeliefUpdater appropriate for a belief type that policy \np\n can use\n\n\n#\n\n\nPOMDPs.action\n \n \nFunction\n.\n\n\n\n\naction(p::Policy, x::Any, action)\naction(p::Policy, x::Belief, action)\n\n\n\n\n\nFills and returns action based on the current state or belief, given the policy.\n\n\nIf an MDP is being simulated, x will be a state; if a POMDP is being simulated, x will be a Belief\n\n\naction(policy::Policy, x::Any)\naction(policy::Policy, x::Belief)\n\n\n\n\n\nReturns an action for the current state or belief, given the policy\n\n\nIf an MDP is being simulated, x will be a state; if a POMDP is being simulated, x will be a Belief\n\n\n#\n\n\nPOMDPs.value\n \n \nFunction\n.\n\n\n\n\nvalue{S}(p::Policy, x::Any)\nvalue{S}(p::Policy, x::Belief)\n\n\n\n\n\nReturns the utility value from policy p given the state\n\n\n\n\nSimulator\n\n\n#\n\n\nPOMDPs.Simulator\n \n \nType\n.\n\n\n\n\nBase type for an object defining how a simulation should be carried out\n\n\n#\n\n\nPOMDPs.simulate\n \n \nFunction\n.\n\n\n\n\nsimulate{S,A,O}(simulator::Simulator, problem::POMDP{S,A,O}, policy::Policy, updater::BeliefUpdater, initial_belief::Belief) \nsimulate{S,A}(simulator::Simulator, problem::MDP{S,A}, policy::Policy, updater::BeliefUpdater, initial_belief::Belief)\n\n\n\n\n\nRuns a simulation using the specified policy and returns the accumulated reward\n\n\n\n\nUtility Tools\n\n\n#\n\n\nPOMDPs.add\n \n \nFunction\n.\n\n\n\n\nadd(solver_name::AbstractString)\n\n\n\n\n\nDownloads and installs a registered solver with name \nsolver_name\n.  This function is not exported, and must be called:\n\n\njulia\n \nusing\n \nPOMDPs\n\n\njulia\n \nPOMDPs\n.\nadd\n(\nMCTS\n)\n\n\n\n\n\n\n#\n\n\nPOMDPs.@pomdp_func\n \n \nMacro\n.\n\n\n\n\nProvide a default function implementation that throws an error when called.\n\n\n#\n\n\nPOMDPs.strip_arg\n \n \nFunction\n.\n\n\n\n\nStrip anything extra (type annotations, default values, etc) from an argument.\n\n\nFor now this cannot handle keyword arguments (it will throw an error).\n\n\n\n\nConstants\n\n\n#\n\n\nPOMDPs.REMOTE_URL\n \n \nConstant\n.\n\n\n\n\nurl to remote JuliaPOMDP organization repo\n\n\n#\n\n\nPOMDPs.SUPPORTED_SOLVERS\n \n \nConstant\n.\n\n\n\n\nSet containing string names of officially supported solvers  (e.g. \nMCTS\n, \nSARSOP\n, etc).  If you have a validated solver that supports the POMDPs.jl API, contact the developers to add your solver to this list.", 
            "title": "API"
        }, 
        {
            "location": "/api/#solver-documentation", 
            "text": "Documentation for the  POMDPs.jl  user interface. You can get help for any type or  function in the module by typing  ?  in the Julia REPL followed by the name of  type or function. For example:  julia using   POMDPs  julia ?  help ? reward  search :   reward \n\n   reward { S , A , O }( pomdp :: POMDP { S , A , O },   state :: S ,   action :: A ,   statep :: S ) \n\n   Returns   the   immediate   reward   for   the   s - a - s   triple \n\n   reward { S , A , O }( pomdp :: POMDP { S , A , O },   state :: S ,   action :: A ) \n\n   Returns   the   immediate   reward   for   the   s - a   pair", 
            "title": "Solver Documentation"
        }, 
        {
            "location": "/api/#contents", 
            "text": "Solver Documentation  Contents  Index  Types  Model Functions  Distribution/Space Functions  Belief Functions  Policy and Solver Functions  Simulator  Utility Tools  Constants", 
            "title": "Contents"
        }, 
        {
            "location": "/api/#index", 
            "text": "Base.Random.rand  POMDPs.@pomdp_func  POMDPs.AbstractDistribution  POMDPs.AbstractSpace  POMDPs.Belief  POMDPs.BeliefUpdater  POMDPs.MDP  POMDPs.POMDP  POMDPs.Policy  POMDPs.REMOTE_URL  POMDPs.SUPPORTED_SOLVERS  POMDPs.Simulator  POMDPs.Solver  POMDPs.action  POMDPs.action_index  POMDPs.actions  POMDPs.add  POMDPs.convert_belief  POMDPs.create_belief  POMDPs.create_observation_distribution  POMDPs.create_policy  POMDPs.create_transition_distribution  POMDPs.dimensions  POMDPs.discount  POMDPs.initial_belief  POMDPs.isterminal  POMDPs.isterminal_obs  POMDPs.iterator  POMDPs.n_actions  POMDPs.n_observations  POMDPs.n_states  POMDPs.obs_index  POMDPs.observation  POMDPs.observations  POMDPs.pdf  POMDPs.reward  POMDPs.simulate  POMDPs.solve  POMDPs.state_index  POMDPs.states  POMDPs.strip_arg  POMDPs.transition  POMDPs.update  POMDPs.updater  POMDPs.value", 
            "title": "Index"
        }, 
        {
            "location": "/api/#types", 
            "text": "#  POMDPs.POMDP     Type .   Abstract base type for a partially observable Markov decision process.  S :   state   type  A :   action   type  O :   observation   type   #  POMDPs.MDP     Type .   Abstract base type for a fully observable Markov decision process.  S :   state   type  A :   action   type   #  POMDPs.AbstractSpace     Type .   Base type for state, action and observation spaces.  T :   type   that   parametarizes   the   space   ( state ,   action ,   or   observation )   #  POMDPs.AbstractDistribution     Type .   Abstract type for a probability distribution.  T :   type   over   which   distribution   is   over   ( state ,   action ,   or   observation )   #  POMDPs.Solver     Type .   Base type for an MDP/POMDP solver  #  POMDPs.Policy     Type .   Base type for a policy (a map from every possible belief, or more abstract policy state, to an optimal or suboptimal action)  #  POMDPs.Belief     Type .   Abstract type for an object representing some knowledge about the state (often a probability distribution)  #  POMDPs.BeliefUpdater     Type .   Abstract type for an object that defines how a belief should be updated", 
            "title": "Types"
        }, 
        {
            "location": "/api/#model-functions", 
            "text": "#  POMDPs.states     Function .   states{S,A,O}(problem::POMDP{S,A,O}, state::S)\nstates{S,A}(problem::MDP{S,A}, state::S)  Returns a subset of the state space reachable from  state .   states(problem::POMDP)\nstates(problem::MDP)  Returns the complete state space of a POMDP.   #  POMDPs.actions     Function .   actions{S,A,O}(problem::POMDP{S,A,O}, state::S, aspace::AbstractSpace{A})\nactions{S,A}(problem::MDP{S,A}, state::S, aspace::AbstractSpace{A})  Modifies aspace to the action space accessible from the given state and returns it.  actions(problem::POMDP)\nactions(problem::MDP)  Returns the entire action space of a POMDP.  actions{S,A,O}(problem::POMDP{S,A,O}, belief::Belief, aspace::AbstractSpace{A})  Modifies aspace to the action space accessible from the states with nonzero belief and returns it.  #  POMDPs.observations     Function .   observations{S,A,O}(problem::POMDP{S,A,O}, state::S, obs::AbstractSpace{O}=observations(problem))  Modifies ospace to the observation space accessible from the given state and returns it.  observations(problem::POMDP)  Returns the entire observation space.  #  POMDPs.reward     Function .   reward{S,A,O}(problem::POMDP{S,A,O}, state::S, action::A, statep::S)\nreward{S,A}(problem::MDP{S,A}, state::S, action::A, statep::S)  Returns the immediate reward for the s-a-s' triple  reward{S,A,O}(problem::POMDP{S,A,O}, state::S, action::A)\nreward{S,A}(problem::MDP{S,A}, state::S, action::A)  Returns the immediate reward for the s-a pair  #  POMDPs.transition     Function .   transition{S,A,O}(problem::POMDP{S,A,O}, state::S, action::A,  distribution::AbstractDistribution{S}=create_transition_distribution(problem))     transition{S,A}(problem::MDP{S,A}, state::S, action::A, distribution::AbstractDistribution{S}=create_transition_distribution(problem))  Returns the transition distribution from the current state-action pair  #  POMDPs.observation     Function .   observation{S,A,O}(problem::POMDP{S,A,O}, state::S, action::A, statep::S, distribution::AbstractDistribution{O}=create_observation_distribution(problem))  Returns the observation distribution for the s-a-s' tuple (state, action, and next state)  observation{S,A,O}(problem::POMDP{S,A,O}, action::A, statep::S, distribution::AbstractDistribution{O}=create_observation_distribution(problem))  Modifies distribution to the observation distribution for the a-s' tuple (action and next state) and returns it  #  POMDPs.isterminal     Function .   isterminal{S,A,O}(problem::POMDP{S,A,O}, state::S)\nisterminal{S,A}(problem::MDP{S,A}, state::S)  Checks if state s is terminal  #  POMDPs.isterminal_obs     Function .   isterminal_obs{S,A,O}(problem::POMDP{S,A,O}, observation::O)  Checks if an observation is terminal.  #  POMDPs.n_states     Function .   n_states(problem::POMDP)\nn_states(problem::MDP)  Returns the number of states in  problem . Used for discrete models only.  #  POMDPs.n_actions     Function .   n_actions(problem::POMDP)\nn_actions(problem::MDP)  Returns the number of actions in  problem . Used for discrete models only.  #  POMDPs.n_observations     Function .   n_observations(problem::POMDP)  Returns the number of actions in  problem . Used for discrete models only.  #  POMDPs.state_index     Function .   state_index{S,A,O}(problem::POMDP{S,A,O}, s::S)\nstate_index{S,A}(problem::MDP{S,A}, s::S)  Returns the integer index of state  s . Used for discrete models only.  #  POMDPs.action_index     Function .   action_index{S,A,O}(problem::POMDP{S,A,O}, a::A)\naction_index{S,A}(problem::MDP{S,A}, a::A)  Returns the integer index of action  a . Used for discrete models only.  #  POMDPs.obs_index     Function .   obs_index{S,A,O}(problem::POMDP{S,A,O}, o::O)  Returns the integer index of observation  o . Used for discrete models only.  #  POMDPs.discount     Function .   discount(problem::POMDP)\ndiscount(problem::MDP)  Return the discount factor for the problem.", 
            "title": "Model Functions"
        }, 
        {
            "location": "/api/#distributionspace-functions", 
            "text": "#  Base.Random.rand     Function .   rand{T}(rng::AbstractRNG, d::AbstractSpace{T}, sample::T)  Returns a random  sample  from space  s .  rand{T}(rng::AbstractRNG, d::AbstractDistribution{T}, sample::T)  Fill  sample  with a random element from distribution  d . The sample can be a state, action or observation.  #  POMDPs.pdf     Function .   pdf{T}(d::AbstractDistribution{T}, x::T)  Value of probability distribution  d  function at sample  x .  #  POMDPs.dimensions     Function .   dimensions{T}(s::AbstractSpace{T})  Returns the number of dimensions in space  s .  #  POMDPs.iterator     Function .   iterator{T}(s::AbstractSpace{T})  Returns an iterable type (array or custom iterator) corresponding to space  s .   #  POMDPs.create_transition_distribution     Function .   create_transition_distribution(problem::POMDP)\ncreate_transition_distribution(problem::MDP)  Returns a transition distribution (for memory preallocation).  #  POMDPs.create_observation_distribution     Function .   create_observation_distribution(problem::POMDP)\ncreate_observation_distribution(problem::MDP)  Returns an observation distribution (for memory preallocation).", 
            "title": "Distribution/Space Functions"
        }, 
        {
            "location": "/api/#belief-functions", 
            "text": "#  POMDPs.initial_belief     Function .   initial_belief(pomdp::POMDP, belief::Belief = create_belief(pomdp))  Returns an initial belief for the pomdp.  #  POMDPs.create_belief     Function .   create_belief(updater::BeliefUpdater)  Creates a belief object of the type used by  updater  (preallocates memory)  create_belief(pomdp::POMDP)  Creates a belief either to be used by updater or pomdp  #  POMDPs.update     Function .   update(updater::BeliefUpdater, belief_old::Belief, action, obs,\nbelief_new::Belief=create_belief(updater))  Returns a new instance of an updated belief given  belief_old  and the latest action and observation.  #  POMDPs.convert_belief     Function .   convert_belief(updater::BeliefUpdater, belief::Belief,\nnew_belief::Belief=create_belief(updater)) = belief  Returns a belief that can be updated using  updater  that has a similar distribution to  belief .", 
            "title": "Belief Functions"
        }, 
        {
            "location": "/api/#policy-and-solver-functions", 
            "text": "#  POMDPs.create_policy     Function .   create_policy(solver::Solver, problem::POMDP)\ncreate_policy(solver::Solver, problem::MDP)  Creates a policy object (for preallocation purposes)  #  POMDPs.solve     Function .   solve(solver::Solver, problem::POMDP, policy=create_policy(solver, problem))  Solves the POMDP using method associated with solver, and returns a policy.   #  POMDPs.updater     Function .   updater(policy::Policy)  Returns a default BeliefUpdater appropriate for a belief type that policy  p  can use  #  POMDPs.action     Function .   action(p::Policy, x::Any, action)\naction(p::Policy, x::Belief, action)  Fills and returns action based on the current state or belief, given the policy.  If an MDP is being simulated, x will be a state; if a POMDP is being simulated, x will be a Belief  action(policy::Policy, x::Any)\naction(policy::Policy, x::Belief)  Returns an action for the current state or belief, given the policy  If an MDP is being simulated, x will be a state; if a POMDP is being simulated, x will be a Belief  #  POMDPs.value     Function .   value{S}(p::Policy, x::Any)\nvalue{S}(p::Policy, x::Belief)  Returns the utility value from policy p given the state", 
            "title": "Policy and Solver Functions"
        }, 
        {
            "location": "/api/#simulator", 
            "text": "#  POMDPs.Simulator     Type .   Base type for an object defining how a simulation should be carried out  #  POMDPs.simulate     Function .   simulate{S,A,O}(simulator::Simulator, problem::POMDP{S,A,O}, policy::Policy, updater::BeliefUpdater, initial_belief::Belief) \nsimulate{S,A}(simulator::Simulator, problem::MDP{S,A}, policy::Policy, updater::BeliefUpdater, initial_belief::Belief)  Runs a simulation using the specified policy and returns the accumulated reward", 
            "title": "Simulator"
        }, 
        {
            "location": "/api/#utility-tools", 
            "text": "#  POMDPs.add     Function .   add(solver_name::AbstractString)  Downloads and installs a registered solver with name  solver_name .  This function is not exported, and must be called:  julia   using   POMDPs  julia   POMDPs . add ( MCTS )   #  POMDPs.@pomdp_func     Macro .   Provide a default function implementation that throws an error when called.  #  POMDPs.strip_arg     Function .   Strip anything extra (type annotations, default values, etc) from an argument.  For now this cannot handle keyword arguments (it will throw an error).", 
            "title": "Utility Tools"
        }, 
        {
            "location": "/api/#constants", 
            "text": "#  POMDPs.REMOTE_URL     Constant .   url to remote JuliaPOMDP organization repo  #  POMDPs.SUPPORTED_SOLVERS     Constant .   Set containing string names of officially supported solvers  (e.g.  MCTS ,  SARSOP , etc).  If you have a validated solver that supports the POMDPs.jl API, contact the developers to add your solver to this list.", 
            "title": "Constants"
        }
    ]
}